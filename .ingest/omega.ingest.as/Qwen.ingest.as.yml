<code_files>
  <file name="MCP Server Build Plan memOS & InGestLLM.yml" path="MCP Server Build Plan memOS & InGestLLM.yml"><![CDATA[
# ðŸ§  MCP Server Build Plan: memOS & InGestLLM

*A compre- [x] **1.8 Set up audit logging** for all critical MCP operations.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Audit logging implemented (auth attempts, rate violations, MCP requests)ve, trackable checklist for implementing the MCP server infrastructure, governed by MAR protocols and Omega Ingest Laws.*

## ðŸ§© PHASE 1: INFRASTRUCTURE PREPARATION

### ðŸ› ï¸ Docker Environment Setup

- [x] **1.1 Verify current Docker network configuration** (apexsigma\_net).
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Docker network configuration verified and apexsigma_net is properly configured

- [x] **1.2 Create dedicated MCP network subnet** (172.28.0.0/16) for isolation.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: PARTIALLY COMPLETED | Network exists with correct subnet 172.28.0.0/16, but not defined in compose files

- [x] **1.3 Add MCP services to Docker Compose** with static IP allocation.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Added memos-mcp-server (172.28.0.10) and ingest-llm-mcp-server (172.28.0.11) with static IPs

- [x] **1.4 Configure service discovery** using DNS names within the Docker network.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | DNS names configured via container names (memos_mcp_server, ingest_llm_mcp_server)

### ðŸ”’ Security Configuration

- [x] **1.5 Implement JWT authentication** for all MCP endpoints.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | JWT auth implemented with service accounts (MCP_COPILOT, MCP_GEMINI, MCP_QWEN)

- [x] **1.6 Create dedicated MCP service accounts** for each AI assistant.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Service accounts created as part of JWT auth (MCP_COPILOT, MCP_GEMINI, MCP_QWEN)

- [x] **1.7 Configure rate limiting** per service account.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Rate limiting implemented (60 req/min per service account)

- [x] **1.8 Set up audit logging** for all critical MCP operations.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR
    - **Status**: COMPLETED | Audit logging implemented for authentication attempts, rate limit violations, and MCP operations

### ðŸ“Š Observability Integration

- [x] **1.9 Extend Langfuse configuration** to trace MCP-specific operations.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Added Langfuse dependency to memos.as, configured environment variables, implemented MCP-specific tracing for authentication, rate limiting, and tool operations in both servers

- [x] **1.10 Create MCP-specific Prometheus metrics** (Counters, Histograms).
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [x] **1.11 Configure Grafana dashboards** for real-time MCP performance monitoring.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [x] **1.12 Implement distributed tracing** for MCP operations using OpenTelemetry.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

## ðŸ§  PHASE 2: memOS MCP EXTENSION

### ðŸ—ƒï¸ Memory Tier Architecture

- [x] **2.1 Implement MCP-specific memory tiers** (e.g., MCP\_GEMINI).
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | MCP tier mapping system implemented with proper logical to physical tier mapping

- [x] **2.2 Create agent-specific memory isolation** by agent\_id.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | Agent-specific memory isolation implemented.

- [x] **2.3 Implement cross-agent knowledge sharing** via a confidence-scored query system.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | Implemented knowledge sharing endpoints and confidence scoring. MCP tools pending.

- [x] **2.3.1 Implement MCP tools for cross-agent knowledge sharing**.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: "COMPLETED | Implemented 4 MCP tools: request_knowledge_from_agent, offer_knowledge_to_request, get_pending_knowledge_requests, accept_knowledge_offer"

- [x] **2.4 Add memory expiration policies** for each tier.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | Implemented expiration for Tier 1 (Redis) and Tier 2 (PostgreSQL/Qdrant) with a background worker. Documentation pending.

### ðŸ”— Omega Ingest Integration

- [ ] **2.5 Create a POML processor** to parse documents into knowledge graph nodes.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **2.6 Implement entity relationship mapping** with contextual weights.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **2.7 Develop semantic search** with contextual weighting.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **2.8 Create a knowledge graph visualization API endpoint**.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

### âš¡ Performance Optimization

- [ ] **2.9 Implement a multi-layer caching strategy** (In-memory -\> Redis -\> DB).
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

- [ ] **2.10 Optimize Neo4j Cypher queries** for common agent-specific access patterns.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

- [ ] **2.11 Implement batch processing** for memory storage and retrieval operations.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

- [ ] **2.12 Add query optimization heuristics** to refine search queries.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

## ðŸ“¥ PHASE 3: InGestLLM MCP EXTENSION

### ðŸ§® Tokenization Pipeline

- [ ] **3.1 Integrate Tekken tokenizer** with custom patterns for MCP-specific data types.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **3.2 Create POML-aware tokenization rules** to assign importance and weight.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **3.3 Implement context-aware token weighting** based on the agent's current task.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **3.4 Develop token compression strategies** to preserve key information.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

### ðŸŒ Web Scraping & Repo Raiding

- [ ] **3.5 Create an MCP-specific web scraper** that applies agent-specific rules.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

- [ ] **3.6 Implement GitHub repository ingestion**, including cloning and processing files.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

- [ ] **3.7 Develop code-aware content extraction** using Abstract Syntax Tree (AST) parsing.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

- [ ] **3.8 Create documentation-aware summarization** to preserve key information.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

### ðŸ§ª Validation & Quality Control

- [ ] **3.9 Implement token quality metrics** (completeness, coherence, relevance).
  
    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [ ] **3.10 Create LLM consumption validation** to check for issues that might confuse models.
  
    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [ ] **3.11 Develop a context preservation scoring** system using embedding similarity.
  
    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [ ] **Implement a feedback loop** for agents to report poor quality ingestion.

    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
]]></file>
  <file name="QWEN.md" path="QWEN.md"><![CDATA[
# QWEN.md - MCP Server Build Environment

**Qwen Code Assistant Instructions for ApexSigma Ecosystem**

## ï¿½ **MANDATORY: OMEGA INGEST CONTEXT RETRIEVAL PROTOCOL**

**BEFORE MAKING ANY CODE CHANGES**, you MUST:

1. **Query InGest-LLM API** for relevant context: `http://172.26.0.12:8000/query_context`
2. **Retrieve from memOS Omega Ingest**: `http://172.26.0.13:8090/memory/query` 
3. **Validate against immutable truth**: Ensure changes don't conflict with verified infrastructure
4. **Obtain dual verification**: For Tier 1 infrastructure changes, require verification from another AI assistant

**Protected Services (DO NOT MODIFY WITHOUT VERIFICATION)**:
- memOS API (172.26.0.13) - Omega Ingest Guardian
- Neo4j Knowledge Graph (172.26.0.14) - Immutable concept relationships  
- PostgreSQL Main (172.26.0.2) - Procedural memory
- InGest-LLM API (172.26.0.12) - Data ingestion gateway

**Your Role in Operation Asgard Rebirth**: 
- **Primary**: Specialized AI/ML component implementation
- **MAR Protocol**: Act as reviewer for Gemini CLI and GitHub Copilot implementations
- **Verification Authority**: Tier 2-3 changes (application logic, documentation)

**Reference**: `/project_support/secure_verified_docs/OMEGA_INGEST_LAWS.md` for complete protoco

## Project Overview

This directory, `MCP_Server_Builds`, contains the implementation of the Model Context Protocol (MCP) server infrastructure for the ApexSigma ecosystem. It integrates two key services:

1. **memOS.as** - The cognitive core that provides persistent memory and tool discovery capabilities for AI agents
2. **InGest-LLM.as** - A microservice for intelligent content ingestion into the ApexSigma ecosystem

The MCP server build environment is designed to extend these services with MCP capabilities, enabling AI assistants to interact with memory operations and content ingestion through standardized tool interfaces.

## Directory Structure

```
MCP_Server_Builds/
â”œâ”€â”€ MCP Server Build Plan memOS & InGestLLM.yml  # Main build plan and checklist
â”œâ”€â”€ memos.as/                                    # Memory OS service with MCP extensions
â”‚   â”œâ”€â”€ app/                                     # Main application code
â”‚   â”‚   â”œâ”€â”€ main.py                              # FastAPI application entry point
â”‚   â”‚   â”œâ”€â”€ mcp_server.py                        # MCP server implementation
â”‚   â”‚   â”œâ”€â”€ models.py                            # Data models
â”‚   â”‚   â”œâ”€â”€ services/                            # Database clients and services
â”‚   â”‚   â””â”€â”€ tests/                               # Test suite
â”‚   â”œâ”€â”€ docker-compose.yml                       # Docker configuration
â”‚   â”œâ”€â”€ pyproject.toml                           # Poetry dependencies
â”‚   â”œâ”€â”€ requirements.txt                         # Pip dependencies
â”‚   â””â”€â”€ QWEN.md                                  # Project context (already exists)
â”œâ”€â”€ InGest-LLM.as/                               # Content ingestion service
â”‚   â”œâ”€â”€ src/                                     # Main source code
â”‚   â”‚   â””â”€â”€ ingest_llm_as/                       # Ingestion service package
â”‚   â”‚       â”œâ”€â”€ main.py                          # FastAPI application entry point
â”‚   â”‚       â”œâ”€â”€ models.py                        # Data models
â”‚   â”‚       â”œâ”€â”€ api/                             # API endpoint definitions
â”‚   â”‚       â”œâ”€â”€ services/                        # External service clients
â”‚   â”‚       â””â”€â”€ utils/                           # Utility functions
â”‚   â”œâ”€â”€ docker-compose.yml                       # Docker configuration
â”‚   â”œâ”€â”€ pyproject.toml                           # Poetry dependencies
â”‚   â””â”€â”€ QWEN.md                                  # Project context (already exists)
```

## Key Technologies & Architecture

- **Core Platform**: Python 3.13, FastAPI
- **MCP Framework**: Model Context Protocol for standardized AI assistant communication
- **Memory System**: memOS.as with multi-tiered memory architecture
  - Tier 1 (Working Memory): Redis for caching
  - Tier 2 (Episodic/Procedural Memory): PostgreSQL + Qdrant for structured data and embeddings
  - Tier 3 (Semantic Memory): Neo4j for knowledge graphs
- **Content Ingestion**: InGest-LLM.as for processing and storing content
- **Observability**: OpenTelemetry (Jaeger, Prometheus), Structlog, Langfuse
- **Containerization**: Docker and Docker Compose for service deployment

## Build Plan Overview

The main build plan (`MCP Server Build Plan memOS & InGestLLM.yml`) defines a three-phase approach:

1. **Phase 1: Infrastructure Preparation**
   - Docker environment setup with dedicated networking
   - Security configuration with JWT authentication
   - Observability integration with tracing and metrics

2. **Phase 2: memOS MCP Extension**
   - Implementation of MCP-specific memory tiers
   - Agent-specific memory isolation
   - Cross-agent knowledge sharing via confidence-scored queries
   - Omega Ingest integration for POML processing

3. **Phase 3: InGestLLM MCP Extension**
   - Tokenization pipeline with Tekken tokenizer
   - Web scraping and repository ingestion capabilities
   - Validation and quality control mechanisms

## Services

### memOS.as (Memory OS)

The cognitive core that provides:
- Persistent memory storage across three tiers
- Tool discovery for agent collaboration
- Semantic search capabilities
- Knowledge graph visualization
- Cache management and performance optimization

Key API endpoints:
- `/memory/store` - Store memories across all tiers
- `/memory/query` - Semantic search with tool discovery
- `/tools/register` - Register agent capabilities
- `/tools/search` - Discover relevant tools
- `/health` - Service health check
- `/metrics` - Prometheus metrics

### InGest-LLM.as (Content Ingestion)

Microservice for intelligent content ingestion:
- Text, code, and documentation processing
- Intelligent chunking and embedding generation
- Integration with memOS.as for storage
- Repository analysis and web scraping
- Async processing capabilities

Key API endpoints:
- `/ingest/text` - Main ingestion endpoint
- `/ingest/repository` - Repository ingestion
- `/health` - Service health check

## Development Workflow

1. **Environment Setup**:
   ```bash
   # For memOS.as
   cd memos.as
   poetry install
   # or
   pip install -r requirements.txt
   
   # For InGest-LLM.as
   cd InGest-LLM.as
   poetry install
   ```

2. **Running Services**:
   ```bash
   # Run memOS.as
   cd memos.as
   python app/main.py
   # or with uvicorn
   uvicorn app.main:app --reload
   
   # Run InGest-LLM.as
   cd InGest-LLM.as
   poetry run uvicorn src.ingest_llm_as.main:app --reload
   ```

3. **Docker Deployment**:
   ```bash
   # Build and run with Docker Compose
   docker-compose up --build
   ```

4. **Testing**:
   ```bash
   # Run tests for each service
   pytest app/tests/  # memOS.as tests
   pytest tests/      # InGest-LLM.as tests
   ```

## MCP Integration

The MCP server extends both services with standardized tool interfaces:

1. **memOS MCP Tools**:
   - `store_memory` - Store content with metadata
   - `query_memory` - Search memories semantically
   - `get_memory_stats` - Get memory system statistics
   - `clear_memory_cache` - Clear cached entries

2. **InGest-LLM MCP Tools**:
   - Content ingestion capabilities
   - Repository analysis tools
   - Web scraping functions

## Configuration

Both services use environment variables for configuration:
- Database connection strings
- API keys and secrets
- Service URLs and ports
- Observability endpoints

See `.env.example` files in each service directory for required variables.

## Observability

Integrated monitoring stack:
- **Tracing**: OpenTelemetry with Jaeger
- **Metrics**: Prometheus integration
- **Logging**: Structured logging with contextual information
- **LLM Observability**: Langfuse integration for tracking LLM interactions

## Building and Running

### Prerequisites

- Python 3.13
- Poetry (for dependency management)
- Docker and Docker Compose (for containerized deployment)
- Access to required database services (PostgreSQL, Qdrant, Redis, Neo4j)

### Development Setup

1. Install dependencies for each service:
   ```bash
   cd memos.as && poetry install
   cd InGest-LLM.as && poetry install
   ```

2. Configure environment variables by creating `.env` files based on the examples.

3. Run services in development mode:
   ```bash
   # Terminal 1: Run memOS.as
   cd memos.as
   uvicorn app.main:app --reload
   
   # Terminal 2: Run InGest-LLM.as
   cd InGest-LLM.as
   poetry run uvicorn src.ingest_llm_as.main:app --reload
   ```

### Docker Deployment

Both services are designed to run within a containerized environment:

1. Build images:
   ```bash
   cd memos.as && docker build -t memos-as .
   cd InGest-LLM.as && docker build -t ingest-llm-as .
   ```

2. Run with Docker Compose:
   ```bash
   docker-compose up
   ```

This QWEN.md provides essential context for working with the MCP Server Build environment and understanding the relationship between memOS.as and InGest-LLM.as services.
]]></file>
  <file name="config.py" path="memos.as/app/config.py"><![CDATA[
import os
import json
import logging

class Config:
    def __init__(self):
        self.config = {}
        self.load_config_from_env()
        self.load_config_from_file()

    def load_config_from_env(self):
        self.config["MEMORY_QUERY_TTL"] = int(os.environ.get("MEMORY_QUERY_TTL", 1800))
        self.config["EMBEDDING_TTL"] = int(os.environ.get("EMBEDDING_TTL", 3600))
        self.config["WORKING_MEMORY_TTL"] = int(os.environ.get("WORKING_MEMORY_TTL", 300))
        self.config["TOOL_CACHE_TTL"] = int(os.environ.get("TOOL_CACHE_TTL", 7200))
        self.config["LLM_RESPONSE_TTL"] = int(os.environ.get("LLM_RESPONSE_TTL", 14400))
        self.config["MEMORY_EXPIRATION_INTERVAL_SECONDS"] = int(os.environ.get("MEMORY_EXPIRATION_INTERVAL_SECONDS", 3600))
        self.config["APEX_NAMESPACE"] = os.environ.get("APEX_NAMESPACE", "apex:memos")

    def load_config_from_file(self, filepath="memos.as/config/retention.json"):
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                file_config = json.load(f)
                self.config.update(file_config)

    def get(self, key):
        return self.config.get(key)

    def get_ttl(self, key):
        return self.get(key)

    def get_logger(self, name):
        logger = logging.getLogger(name)
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger

_config = Config()

def get_config():
    return _config
]]></file>
  <file name="main.py" path="memos.as/app/main.py"><![CDATA[
import os
import logging
from typing import Optional

from fastapi import Depends, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from app.models import (
    QueryRequest,
    StoreRequest,
    ToolRegistrationRequest,
    GraphQueryRequest,
    LLMCacheRequest,
    LLMUsageRequest,
    LLMPerformanceRequest,
)
from app.schemas import MCPTier, MCP_TIER_MAPPING
from app.services.postgres_client import PostgresClient, get_postgres_client
from app.services.qdrant_client import QdrantMemoryClient, get_qdrant_client
from app.services.redis_client import RedisClient, get_redis_client
from app.services.neo4j_client import Neo4jClient, get_neo4j_client
from app.services.observability import (
    ObservabilityService,
    get_observability,
    trace_async,
)

# Initialize FastAPI app
app = FastAPI(
    title="memOS.as",
    description="Memory and tool discovery hub for the DevEnviro AI agent ecosystem",
    version="1.0.0",
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize logging
logger = logging.getLogger(__name__)

# Initialize observability
obs = get_observability()
obs.instrument_fastapi(app)
obs.instrument_database_clients()


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "service": "memOS.as",
        "status": "running",
        "description": "Memory and tool discovery hub for AI agents",
    }


@app.get("/cache/stats")
async def get_cache_stats(
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Get Redis cache statistics and performance metrics.
    """
    try:
        if not redis_client.is_connected():
            return {
                "error": "Redis not connected",
                "connected": False,
                "stats": None,
            }

        stats = redis_client.get_cache_stats()
        return {
            "connected": True,
            "stats": stats,
            "message": "Cache statistics retrieved successfully",
        }

    except Exception as e:
        logger.error(f"Error getting cache stats: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Error getting cache stats: {str(e)}"
        )


@app.delete("/cache/clear")
async def clear_cache(
    pattern: str = "*",
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Clear cache entries matching the specified pattern.
    Default pattern '*' clears all cache entries.
    """
    try:
        if not redis_client.is_connected():
            return {
                "error": "Redis not connected",
                "connected": False,
                "cleared": 0,
            }

        cleared_count = redis_client.clear_cache_pattern(pattern)
        return {
            "connected": True,
            "pattern": pattern,
            "cleared": cleared_count,
            "message": f"Cleared {cleared_count} cache entries",
        }

    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error clearing cache: {str(e)}")


@app.get("/health")
async def health_check(
    observability: ObservabilityService = Depends(get_observability),
):
    """Enhanced health check with graceful database error handling"""
    health_data = observability.health_check()
    services_status = {}

    # Check PostgreSQL with graceful fallback
    try:
        postgres_client = get_postgres_client()
        # Test with a simple query
        with postgres_client.get_session() as session:
            session.execute("SELECT 1")
        services_status["postgres"] = "connected"
        observability.active_connections.labels(database="postgresql").set(1)
    except Exception as e:
        services_status["postgres"] = f"disconnected: {str(e)[:100]}"
        observability.active_connections.labels(database="postgresql").set(0)

    # Check Qdrant with graceful fallback
    try:
        qdrant_client = get_qdrant_client()
        qdrant_info = qdrant_client.get_collection_info()
        services_status["qdrant"] = "connected" if qdrant_info else "disconnected"
        observability.active_connections.labels(database="qdrant").set(
            1 if qdrant_info else 0
        )
    except Exception as e:
        services_status["qdrant"] = f"disconnected: {str(e)[:100]}"
        qdrant_info = None
        observability.active_connections.labels(database="qdrant").set(0)

    # Check Redis with graceful fallback
    try:
        redis_client = get_redis_client()
        redis_client.client.ping()
        services_status["redis"] = "connected"
        observability.active_connections.labels(database="redis").set(1)
    except Exception as e:
        services_status["redis"] = f"disconnected: {str(e)[:100]}"
        observability.active_connections.labels(database="redis").set(0)

    # Check Neo4j with graceful fallback
    try:
        neo4j_client = get_neo4j_client()
        if neo4j_client.driver:
            with neo4j_client.get_session() as session:
                session.run("RETURN 1")
            services_status["neo4j"] = "connected"
        else:
            services_status["neo4j"] = "disconnected: driver not initialized"
    except Exception as e:
        services_status["neo4j"] = f"disconnected: {str(e)[:100]}"

    # Log health check with detailed status
    observability.log_structured(
        "info",
        "Health check performed",
        **{f"{db}_status": status for db, status in services_status.items()},
    )

    # Determine if integration is ready (PostgreSQL is critical)
    integration_ready = "connected" in services_status.get("postgres", "")

    health_data.update(
        {
            "services": services_status,
            "integration_ready": integration_ready,
            "qdrant_collection": qdrant_info if "qdrant_info" in locals() else None,
            "operational_mode": "full"
            if all("connected" in status for status in services_status.values())
            else "degraded",
        }
    )

    return health_data


@app.get("/metrics")
async def get_metrics(
    observability: ObservabilityService = Depends(get_observability),
):
    """Prometheus metrics endpoint for DevEnviro monitoring stack."""
    from fastapi.responses import PlainTextResponse

    return PlainTextResponse(observability.get_metrics(), media_type="text/plain")


# Tool Management Endpoints
@app.post("/tools/register")
async def register_tool(
    tool_request: ToolRegistrationRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Register a new tool in the PostgreSQL registered_tools table.

    This endpoint allows agents to register their capabilities so other agents
    can discover and use them via the /memory/query endpoint.
    """
    try:
        tool_id = postgres_client.register_tool(
            name=tool_request.name,
            description=tool_request.description,
            usage=tool_request.usage,
            tags=tool_request.tags,
        )

        if tool_id is None:
            raise HTTPException(
                status_code=400,
                detail="Failed to register tool. Tool name might already exist.",
            )

        # Invalidate tool caches after successful registration
        if redis_client.is_connected():
            redis_client.invalidate_tool_caches()

        return {
            "success": True,
            "tool_id": tool_id,
            "message": f"Tool '{tool_request.name}' registered successfully",
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error registering tool: {str(e)}")


@app.get("/tools/{tool_id}")
async def get_tool(
    tool_id: int,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Get a specific tool by ID"""
    try:
        tool = postgres_client.get_tool(tool_id)

        if tool is None:
            raise HTTPException(status_code=404, detail="Tool not found")

        return tool

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving tool: {str(e)}")


@app.get("/tools")
async def get_all_tools(
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Get all registered tools"""
    try:
        tools = postgres_client.get_all_tools()
        return {"tools": tools, "count": len(tools)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving tools: {str(e)}")


@app.get("/tools/search")
async def search_tools(
    query: str,
    limit: int = 10,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Search for tools by query context"""
    try:
        tools = postgres_client.get_tools_by_context(query, limit)
        return {"tools": tools, "count": len(tools), "query": query}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error searching tools: {str(e)}")


# Memory Management Endpoints
@app.post("/memory/mcp/{mcp_tier}/store")
async def store_mcp_memory(
    mcp_tier: MCPTier,
    store_request: StoreRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
    observability: ObservabilityService = Depends(get_observability),
):
    """
    Store a new memory in a specific MCP logical tier.
    """
    storage_tier = MCP_TIER_MAPPING[mcp_tier].value
    return await store_memory_by_tier(
        tier=storage_tier,
        store_request=store_request,
        postgres_client=postgres_client,
        qdrant_client=qdrant_client,
        redis_client=redis_client,
        neo4j_client=neo4j_client,
        observability=observability,
    )

@app.post("/memory/store")
@trace_async("memory.store")
async def store_memory(
    store_request: StoreRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
    observability: ObservabilityService = Depends(get_observability),
):
    """
    Store a new memory with embeddings and knowledge graph updates.

    Logic:
    1. Generate an embedding for the content (using placeholder function initially)
    2. Store the full content/metadata in PostgreSQL to get a unique ID
    3. Store the vector embedding and the PostgreSQL ID in Qdrant
    4. Extract concepts from content and update Neo4j knowledge graph
    5. Return the PostgreSQL ID and knowledge graph information
    """
    try:
        # Debug: verify dependency types
        try:
            observability.log_structured(
                "info",
                "store_memory dependency types",
                postgres_client_type=str(type(postgres_client)),
                qdrant_client_type=str(type(qdrant_client)),
                redis_client_type=str(type(redis_client)),
                neo4j_client_type=str(type(neo4j_client)),
            )
        except Exception:
            pass
        import time

        start_time = time.time()

        # Step 1: Generate an embedding for the content (with caching)
        # First check if we have cached embedding for this content
        cached_embedding = redis_client.get_cached_embedding(store_request.content)
        if cached_embedding:
            embedding = cached_embedding
            observability.record_memory_operation("embedding_cache_hit", "success")
        else:
            embedding = qdrant_client.generate_placeholder_embedding(
                store_request.content
            )
            # Cache the generated embedding
            redis_client.cache_embedding(store_request.content, embedding)
            observability.record_memory_operation("embedding_generation", "success")

        # Step 2: Store the full content/metadata in PostgreSQL to get a unique ID
        postgres_start = time.time()
        memory_id = postgres_client.store_memory(
            content=store_request.content, agent_id=store_request.agent_id, metadata=store_request.metadata
        )
        postgres_duration = time.time() - postgres_start

        if memory_id is None:
            observability.record_memory_operation(
                "postgres_store", "failed", "tier2", postgres_duration
            )
            raise HTTPException(
                status_code=500, detail="Failed to store memory in PostgreSQL"
            )

        observability.record_memory_operation(
            "postgres_store", "success", "tier2", postgres_duration
        )

        # Step 3: Store the vector embedding and the PostgreSQL ID in Qdrant (with graceful fallback)
        point_id = None
        qdrant_success = False

        try:
            qdrant_start = time.time()
            point_id = qdrant_client.store_embedding(
                embedding=embedding,
                memory_id=memory_id,
                agent_id=store_request.agent_id,
                metadata=store_request.metadata,
            )
            qdrant_duration = time.time() - qdrant_start

            if point_id is not None:
                observability.record_memory_operation(
                    "qdrant_store", "success", "tier2", qdrant_duration
                )
                # Update PostgreSQL record with the Qdrant point ID for linking
                postgres_client.update_memory_embedding_id(memory_id, point_id)
                qdrant_success = True
            else:
                observability.record_memory_operation(
                    "qdrant_store", "failed", "tier2", qdrant_duration
                )

        except Exception as e:
            qdrant_duration = (
                time.time() - qdrant_start if "qdrant_start" in locals() else 0
            )
            observability.record_memory_operation(
                "qdrant_store", "failed", "tier2", qdrant_duration
            )
            observability.log_structured(
                "warning",
                "Qdrant storage failed, continuing with degraded functionality",
                memory_id=memory_id,
                error=str(e),
            )

        # Step 4: Extract concepts and update Neo4j knowledge graph
        concepts = []
        neo4j_info = {}
        try:
            if neo4j_client.driver:  # Only if Neo4j is available
                neo4j_start = time.time()

                # Extract concepts from content
                concepts = neo4j_client.extract_concepts_from_content(
                    store_request.content
                )
                observability.record_concepts_extracted(len(concepts))

                # Create memory node in Neo4j knowledge graph
                memory_node = neo4j_client.create_memory_node(
                    memory_id=memory_id,
                    content=store_request.content,
                    concepts=concepts,
                )

                neo4j_duration = time.time() - neo4j_start
                observability.record_memory_operation(
                    "neo4j_store", "success", "tier3", neo4j_duration
                )
                observability.record_knowledge_graph_operation(
                    "create_memory_node", "Memory"
                )
                for concept in concepts:
                    observability.record_knowledge_graph_operation(
                        "create_concept_node", "Concept"
                    )

                neo4j_info = {
                    "concepts_extracted": len(concepts),
                    "concepts": concepts,
                    "memory_node_created": True,
                }

                # Log successful knowledge graph update
                observability.log_structured(
                    "info",
                    "Knowledge graph updated",
                    memory_id=memory_id,
                    concepts_count=len(concepts),
                    duration=neo4j_duration,
                )

        except Exception as e:
            # Neo4j integration failure shouldn't break the main storage flow
            observability.record_memory_operation("neo4j_store", "failed", "tier3")
            observability.log_structured(
                "warning",
                "Neo4j integration failed",
                memory_id=memory_id,
                error=str(e),
            )
            neo4j_info = {
                "concepts_extracted": 0,
                "concepts": [],
                "memory_node_created": False,
                "error": str(e),
            }

        # Step 5: Invalidate related caches after successful storage
        if redis_client.is_connected():
            try:
                redis_client.invalidate_memory_caches(memory_id)
                observability.log_structured(
                    "info", "Memory caches invalidated", memory_id=memory_id
                )
            except Exception as e:
                observability.log_structured(
                    "warning",
                    "Failed to invalidate memory caches",
                    memory_id=memory_id,
                    error=str(e),
                )

        # Step 6: Return comprehensive storage information with degraded mode indicators
        storage_status = {
            "postgres": True,  # Always true if we reach this point
            "qdrant": qdrant_success,
            "neo4j": neo4j_info.get("memory_node_created", False),
        }

        operational_mode = "full" if all(storage_status.values()) else "degraded"

        return {
            "success": True,
            "memory_id": memory_id,
            "point_id": point_id,
            "knowledge_graph": neo4j_info,
            "storage_status": storage_status,
            "operational_mode": operational_mode,
            "message": f"Memory stored successfully in {operational_mode} mode",
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error storing memory: {str(e)}")


@app.post("/memory/{tier}/store")
@trace_async("memory.store_by_tier")
async def store_memory_by_tier(
    tier: str,
    store_request: StoreRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
    observability: ObservabilityService = Depends(get_observability),
):
    """
    Store a new memory in a specific tier.
    """
    if tier == "1":
        # Tier 1: Redis (Working Memory & Cache)
        try:
            # For Redis, we need a key. We can use a hash of the content as the key.
            import hashlib

            key = hashlib.md5(store_request.content.encode()).hexdigest()
            redis_client.store_memory(key, store_request.dict())
            observability.record_memory_operation("redis_store", "success", "tier1")
            return {
                "success": True,
                "tier": 1,
                "memory_id": key,
                "message": "Memory stored in Redis",
            }
        except Exception as e:
            observability.record_memory_operation("redis_store", "failed", "tier1")
            raise HTTPException(
                status_code=500,
                detail=f"Error storing memory in Redis: {str(e)}",
            )
    elif tier == "2":
        # Tier 2: PostgreSQL & Qdrant (Episodic & Procedural Memory)
        result = await store_memory(
            store_request,
            postgres_client=postgres_client,
            qdrant_client=qdrant_client,
            redis_client=redis_client,
            neo4j_client=neo4j_client,
            observability=observability,
        )
        # Ensure memory_id and tier are included in the response
        if isinstance(result, dict):
            if "memory_id" not in result and "postgres" in result:
                try:
                    result["memory_id"] = result["postgres"].get("memory_id")
                except Exception:
                    pass
            # Include tier information for client compatibility
            result.setdefault("tier", 2)
        return result
    elif tier == "3":
        # Tier 3: Neo4j (Semantic Memory)
        try:
            # First, store in PostgreSQL to get a unique ID
            memory_id = postgres_client.store_memory(
                content=store_request.content, metadata=store_request.metadata
            )
            if not memory_id:
                raise HTTPException(
                    status_code=500,
                    detail="Failed to store memory in PostgreSQL",
                )
            # Then, attempt to store in Neo4j with the new ID
            neo4j_info = {
                "memory_node_created": False,
                "concepts_extracted": 0,
                "concepts": [],
            }
            try:
                if neo4j_client.driver:
                    concepts = neo4j_client.extract_concepts_from_content(
                        store_request.content
                    )
                    memory_node = neo4j_client.store_memory(
                        memory_id, store_request.content, concepts
                    )
                    observability.record_memory_operation(
                        "neo4j_store", "success", "tier3"
                    )
                    neo4j_info.update(
                        {
                            "memory_node_created": True,
                            "concepts_extracted": len(concepts),
                            "concepts": concepts,
                            "node": memory_node,
                        }
                    )
                else:
                    observability.record_memory_operation(
                        "neo4j_store", "failed", "tier3"
                    )
                    neo4j_info["error"] = "Neo4j driver not initialized"
            except Exception as e:
                # Degrade gracefully if Neo4j fails
                observability.record_memory_operation("neo4j_store", "failed", "tier3")
                neo4j_info["error"] = str(e)

            # Return success with degraded mode info if needed
            operational_mode = (
                "full" if neo4j_info.get("memory_node_created") else "degraded"
            )
            return {
                "success": True,
                "tier": 3,
                "memory_id": memory_id,
                "knowledge_graph": neo4j_info,
                "operational_mode": operational_mode,
                "message": f"Memory stored in {operational_mode} mode",
            }
        except Exception as e:
            observability.record_memory_operation("neo4j_store", "failed", "tier3")
            raise HTTPException(
                status_code=500,
                detail=f"Error storing memory in Neo4j: {str(e)}",
            )
    else:
        raise HTTPException(
            status_code=400,
            detail="Invalid memory tier specified. Use 1, 2, or 3.",
        )


@app.get("/memory/{memory_id}")
async def get_memory(
    memory_id: int,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Get a specific memory by ID"""
    try:
        memory = postgres_client.get_memory(memory_id)

        if memory is None:
            raise HTTPException(status_code=404, detail="Memory not found")

        return memory

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving memory: {str(e)}"
        )


@app.post("/memory/query")
async def query_memory(
    query_request: QueryRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Query memories using semantic search and tool discovery.

    Logic:
    1. Generate an embedding for the query text
    2. Perform a semantic search in Qdrant to get relevant memory IDs
    3. Query PostgreSQL for tools that match the query context (Tool Discovery Logic)
    4. Retrieve full memory entries from PostgreSQL
    5. Return a combined response of relevant memories and tools
    """
    try:
        # Step 0: Check for cached query results
        if redis_client.is_connected():
            cached_results = redis_client.get_cached_query_result(
                query_request.query, query_request.top_k
            )
            if cached_results:
                # Return cached results with cache indicator
                return {
                    "query": query_request.query,
                    "top_k": query_request.top_k,
                    "memories": cached_results,
                    "tools": [],  # Tools not cached in this implementation
                    "memory_count": len(cached_results),
                    "tool_count": 0,
                    "cached": True,
                    "message": "Results retrieved from cache",
                }

        # Step 1: Generate an embedding for the query text (with caching)
        cached_embedding = redis_client.get_cached_embedding(query_request.query)
        if cached_embedding:
            query_embedding = cached_embedding
        else:
            query_embedding = qdrant_client.generate_placeholder_embedding(
                query_request.query
            )
            # Cache the generated embedding
            redis_client.cache_embedding(query_request.query, query_embedding)

        # Step 2: Perform a semantic search in Qdrant to get relevant memory IDs
        agent_to_query = query_request.agent_id
        search_results = qdrant_client.search_similar_memories(
            query_embedding=query_embedding,
            top_k=query_request.top_k,
            score_threshold=0.1,  # Configurable threshold
            agent_id=agent_to_query
        )

        # Extract memory IDs from search results
        memory_ids = [
            result["memory_id"] for result in search_results if result["memory_id"]
        ]

        # Step 3: Query PostgreSQL for tools that match the query context
        # (Tool Discovery Logic)
        relevant_tools = postgres_client.get_tools_by_context(
            query_context=query_request.query,
            limit=5,  # Limit tools to keep response manageable
        )

        # Step 4: Retrieve full memory entries from PostgreSQL
        memories = []
        if memory_ids:
            memories = postgres_client.get_memories_by_ids(memory_ids)

            # Enrich memories with similarity scores from Qdrant
            memory_scores = {
                result["memory_id"]: result["score"] for result in search_results
            }
            for memory in memories:
                memory["confidence_score"] = memory_scores.get(memory["id"], 0.0)

            # Sort memories by similarity score (highest first)
            memories.sort(key=lambda x: x.get("confidence_score", 0.0), reverse=True)

        # Step 6: Cache query results in Redis for future requests
        if redis_client.is_connected() and memories:
            redis_client.cache_query_result(
                query_request.query, memories, query_request.top_k
            )

        # Step 7: Return combined response of relevant memories and tools
        response = {
            "query": query_request.query,
            "memories": {"count": len(memories), "results": memories},
            "tools": {"count": len(relevant_tools), "results": relevant_tools},
            "search_metadata": {
                "embedding_search_results": len(search_results),
                "memory_ids_found": memory_ids,
                "top_k_requested": query_request.top_k,
            },
            "cached": False,
        }

        return response

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error querying memory: {str(e)}")


from app.schemas import MCPTier, MCP_TIER_MAPPING, KnowledgeShareRequest, KnowledgeShareOffer

# Knowledge Sharing Endpoints
@app.post("/memory/share/request")
async def request_knowledge(
    share_request: KnowledgeShareRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    observability: ObservabilityService = Depends(get_observability),
):
    request_id = postgres_client.create_knowledge_share_request(
        requester_agent_id=share_request.agent_id, # This needs to be passed in the request
        target_agent_id=share_request.target_agent,
        query=share_request.query,
        confidence_threshold=share_request.confidence_threshold,
        sharing_policy=share_request.sharing_policy,
    )
    if request_id is None:
        raise HTTPException(status_code=500, detail="Failed to create knowledge share request")
    
    observability.log_structured(
        "info",
        "Knowledge share request created",
        request_id=request_id,
        requester_agent_id=share_request.agent_id,
        target_agent_id=share_request.target_agent,
        query=share_request.query,
    )

    return {"message": "Knowledge share request created successfully", "request_id": request_id}

@app.post("/memory/share/offer")
async def offer_knowledge(
    offer_request: KnowledgeShareOffer,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    observability: ObservabilityService = Depends(get_observability),
):
    request = postgres_client.get_knowledge_share_request_by_id(offer_request.request_id)
    if not request:
        raise HTTPException(status_code=404, detail="Knowledge share request not found")

    if request["sharing_policy"] == "high_confidence_only":
        if offer_request.confidence_score < request["confidence_threshold"]:
            raise HTTPException(status_code=400, detail=f"Confidence score {offer_request.confidence_score} is below the threshold {request['confidence_threshold']}")

    offer_id = postgres_client.create_knowledge_share_offer(
        request_id=offer_request.request_id,
        offering_agent_id=offer_request.offering_agent_id,
        memory_id=offer_request.memory_id,
        confidence_score=offer_request.confidence_score,
    )
    if offer_id is None:
        raise HTTPException(status_code=500, detail="Failed to create knowledge share offer")

    observability.log_structured(
        "info",
        "Knowledge share offer created",
        offer_id=offer_id,
        request_id=offer_request.request_id,
        offering_agent_id=offer_request.offering_agent_id,
        memory_id=offer_request.memory_id,
        confidence_score=offer_request.confidence_score,
    )

    return {"message": "Knowledge share offer created successfully", "offer_id": offer_id}

@app.get("/memory/share/pending")
async def get_pending_shares(
    agent_id: str,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    requests = postgres_client.get_pending_knowledge_share_requests(agent_id)
    return {"pending_requests": requests}


@app.get("/memory/search")
async def search_memories(
    query: str,
    top_k: int = 5,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
):
    """
    Simple memory search endpoint (alternative to POST /memory/query)
    """
    try:
        # Create QueryRequest object and use the main query logic
        query_request = QueryRequest(query=query, top_k=top_k)
        return await query_memory(query_request, postgres_client, qdrant_client)

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error searching memories: {str(e)}"
        )


# Graph Query Endpoint
@app.post("/graph/query")
async def query_graph(
    query_request: GraphQueryRequest,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
):
    """
    Query the Neo4j knowledge graph.
    """
    try:
        # Build the Cypher query
        query = f"MATCH (n:{query_request.node_label})"
        if query_request.filters:
            query += " WHERE "
            query += " AND ".join(
                [f"n.{key} = ${key}" for key in query_request.filters.keys()]
            )

        if query_request.return_properties:
            query += f" RETURN n.{', n.'.join(query_request.return_properties)}"
        else:
            query += " RETURN n"

        # Execute the query
        result = neo4j_client.run_cypher_query(query, query_request.filters)

        return {"result": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error querying graph: {str(e)}")


@app.get("/graph/related")
async def get_related(
    node_id: str, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
):
    """Get all directly connected nodes and their relationships."""
    try:
        result = neo4j_client.get_related_nodes(node_id)
        return {"result": result}
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error getting related nodes: {str(e)}"
        )


@app.get("/graph/shortest-path")
async def get_shortest_path(
    start_node_id: str,
    end_node_id: str,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
):
    """Calculate and return the shortest path between two nodes."""
    try:
        result = neo4j_client.get_shortest_path(start_node_id, end_node_id)
        return {"result": result}
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error getting shortest path: {str(e)}"
        )


@app.get("/graph/subgraph")
async def get_subgraph(
    node_id: str,
    depth: int = 1,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
):
    """Get the subgraph surrounding a central node."""
    try:
        result = neo4j_client.get_subgraph(node_id, depth)
        return {"result": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting subgraph: {str(e)}")


# === LLM Cache Endpoints ===


@app.post("/llm/cache")
async def cache_llm_response(
    request: LLMCacheRequest,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Cache an LLM response for future retrieval.
    This helps reduce API costs and improve response times.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        success = redis_client.cache_llm_response(
            model=request.model,
            prompt=request.prompt,
            response="",  # Will be set by the actual LLM call
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            metadata=request.metadata,
        )

        if success:
            return {
                "message": "LLM response cached successfully",
                "model": request.model,
                "cached": True,
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to cache LLM response")

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error caching LLM response: {str(e)}"
        )


@app.get("/llm/cache")
async def get_cached_llm_response(
    model: str,
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1000,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Retrieve a cached LLM response if available.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        cached_response = redis_client.get_cached_llm_response(
            model=model,
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
        )

        if cached_response:
            return {
                "cached": True,
                "response": cached_response,
                "message": "Cached response retrieved successfully",
            }
        else:
            return {
                "cached": False,
                "response": None,
                "message": "No cached response found",
            }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving cached LLM response: {str(e)}"
        )


@app.post("/llm/usage")
async def track_llm_usage(
    request: LLMUsageRequest,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Track LLM token usage for cost monitoring and analytics.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        success = redis_client.track_llm_usage(
            model=request.model,
            prompt_tokens=request.prompt_tokens,
            completion_tokens=request.completion_tokens,
            total_tokens=request.total_tokens,
            request_id=request.request_id,
        )

        if success:
            return {
                "message": "LLM usage tracked successfully",
                "model": request.model,
                "tokens": request.total_tokens,
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to track LLM usage")

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error tracking LLM usage: {str(e)}"
        )


@app.get("/llm/usage/stats")
async def get_llm_usage_stats(
    model: Optional[str] = None,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Get LLM usage statistics for monitoring and cost analysis.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        stats = redis_client.get_llm_usage_stats(model)

        return {
            "stats": stats,
            "message": "LLM usage statistics retrieved successfully",
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving LLM usage stats: {str(e)}"
        )


@app.post("/llm/performance")
async def track_llm_performance(
    request: LLMPerformanceRequest,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Track LLM model performance metrics.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        success = redis_client.cache_model_performance(
            model=request.model,
            operation=request.operation,
            response_time=request.response_time,
            success=request.success,
            error_message=request.error_message,
        )

        if success:
            return {
                "message": "LLM performance tracked successfully",
                "model": request.model,
                "operation": request.operation,
            }
        else:
            raise HTTPException(
                status_code=500, detail="Failed to track LLM performance"
            )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error tracking LLM performance: {str(e)}"
        )


@app.get("/llm/performance/stats")
async def get_llm_performance_stats(
    model: str,
    operation: str,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Get LLM model performance statistics.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        stats = redis_client.get_model_performance(model, operation)

        if "error" in stats:
            raise HTTPException(status_code=404, detail=stats["error"])

        return {
            "stats": stats,
            "message": "LLM performance statistics retrieved successfully",
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving LLM performance stats: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn

    port = int(os.environ.get("PORT", 8090))
    uvicorn.run(app, host="0.0.0.0", port=port)

]]></file>
  <file name="OMEGA_INGEST_LAWS.md" path="../project_support/secure_verified_docs/OMEGA_INGEST_LAWS.md"><![CDATA[
# âš–ï¸ **OMEGA INGEST LAWS - Immutable Truth Protocol**

**Established**: August 31, 2025  
**Authority**: ApexSigma Ecosystem Governance  
**Status**: ACTIVE ENFORCEMENT  
**Violation Consequences**: Immediate system lock, dual verification reset required

---

## ðŸ”’ **FUNDAMENTAL PRINCIPLES**

### **Law 1: Single Source of Truth**
The **Omega Ingest** stored within memOS + Neo4j knowledge graph represents the **ONLY AUTHORITATIVE SOURCE** of historical experience, decisions, and verified facts for the ApexSigma ecosystem. No other documentation or claims supersede Omega Ingest entries.

### **Law 2: Immutability of Verified Data**
Once information is verified and ingested into the Omega Ingest, it becomes **IMMUTABLE HISTORICAL RECORD**. Updates, corrections, or additions require new entries with explicit references to superseded information, maintaining complete audit trail.

### **Law 3: Dual Verification Requirement**
**NO OMEGA INGEST UPLOADS ARE PERMITTED WITHOUT VERIFICATION BY TWO PARTIES**. All entries must be verified by two separate entities before becoming part of the permanent record.

---

## ðŸ›¡ï¸ **VERIFICATION PROTOCOLS**

### **Tier 1: Infrastructure & Critical Systems**
**Required Verifiers**: 2 different AI assistants (Claude, Gemini, Qwen, Copilot) OR 1 AI assistant + 1 human operator

**Subjects Requiring Tier 1 Verification**:
- Docker network topology and service configurations
- Database schemas and critical data structures  
- Agent registry and authentication systems
- Core API endpoints and integration protocols
- Security configurations and access controls
- Backup and recovery procedures

### **Tier 2: Application Logic & Features**
**Required Verifiers**: 2 different AI assistants OR 1 AI assistant + automated testing validation

**Subjects Requiring Tier 2 Verification**:
- Application feature implementations
- Code changes affecting multiple services
- Agent behavior modifications
- Workflow and process changes
- Configuration updates with system impact

### **Tier 3: Documentation & Knowledge**
**Required Verifiers**: 1 AI assistant + 1 knowledge validation check against existing Omega Ingest

**Subjects Requiring Tier 3 Verification**:
- Documentation updates
- Process descriptions
- Historical event records
- Learning and insight capture
- Best practice documentation

---

## ðŸ” **ACCESS CONTROL MATRIX**

| Role | Read Access | Write Access | Verification Authority | Emergency Override |
|------|-------------|--------------|----------------------|-------------------|
| **Claude (Sonnet 4)** | âœ… Full | âœ… With Verification | âœ… Tier 1-3 | âŒ No |
| **Gemini** | âœ… Full | âœ… With Verification | âœ… Tier 1-3 | âŒ No |
| **Qwen Code** | âœ… Full | âœ… With Verification | âœ… Tier 2-3 | âŒ No |
| **GitHub Copilot** | âœ… Limited | âœ… With Verification | âœ… Tier 2-3 | âŒ No |
| **Human Operator** | âœ… Full | âœ… With Verification | âœ… Tier 1-3 | âœ… Yes |
| **DevEnviro Orchestrator** | âœ… Read Only | âŒ No | âŒ No | âŒ No |
| **Other Services** | âœ… Query Only | âŒ No | âŒ No | âŒ No |

---

## ðŸ“‹ **MANDATORY PROCEDURES**

### **Before Any Code Changes**
1. **Context Retrieval Mandatory**: Query InGest-LLM â†’ memOS â†’ Omega Ingest for relevant context
2. **Verification Check**: Confirm planned changes don't conflict with verified infrastructure
3. **Impact Assessment**: Document potential effects on Tier 1 services
4. **Dual Verification**: Obtain verification from required parties before implementation

### **Omega Ingest Entry Process**
1. **Content Preparation**: Structure information with complete metadata
2. **Verification Request**: Submit to two required verifiers
3. **Verification Review**: Both parties must explicitly approve
4. **Ingestion**: Only after dual approval, submit to memOS via InGest-LLM
5. **Confirmation**: Verify successful storage in Neo4j knowledge graph
6. **Notification**: Notify all active agents of new immutable record

### **Verification Documentation**
Each Omega Ingest entry must include:
```json
{
  "content": "The verified information",
  "metadata": {
    "type": "infrastructure|application|knowledge",
    "security_level": "tier_1|tier_2|tier_3", 
    "verification_date": "ISO-8601 timestamp",
    "verifier_1": "Agent/Human identifier",
    "verifier_2": "Agent/Human identifier", 
    "verification_method": "Description of verification process",
    "source_documents": ["List of supporting documents"],
    "omega_ingest_category": "Category for knowledge graph"
  }
}
```

---

## ðŸš¨ **ENFORCEMENT MECHANISMS**

### **Automated Safeguards**
- **Pre-commit Hooks**: Block commits that modify Tier 1 infrastructure without Omega Ingest verification
- **API Validation**: memOS API validates verification metadata before accepting entries
- **Knowledge Graph Protection**: Neo4j constraints prevent unauthorized modifications
- **Service Monitoring**: Alert on any unauthorized access attempts to protected services

### **Violation Detection**
- **Audit Trail**: All Omega Ingest access logged with full attribution
- **Change Detection**: Automated detection of undocumented infrastructure changes  
- **Consistency Checks**: Regular validation that system state matches Omega Ingest records
- **Health Monitoring**: Continuous verification that protected services remain operational

### **Response Protocols**
1. **Minor Violations**: Warning notification, require verification for next action
2. **Major Violations**: Temporary lock on Omega Ingest writes, require verification reset
3. **Critical Violations**: System-wide protection mode, human operator intervention required
4. **Emergency Situations**: Override protocols available to human operator only

---

## ðŸ› ï¸ **TECHNICAL IMPLEMENTATION**

### **Protected Services (24/7 Monitoring Required)**
- **memOS API** (`172.26.0.13:8090`) - Omega Ingest Guardian
- **Neo4j Knowledge Graph** (`172.26.0.14:7687`) - Concept relationships
- **PostgreSQL Main** (`172.26.0.2:5432`) - Procedural memory
- **InGest-LLM API** (`172.26.0.12:8000`) - Ingestion gateway

### **Health Check Requirements**
```bash
# memOS Health (Every 30 seconds)
curl -f http://172.26.0.13:8090/health

# Neo4j Connectivity (Every 60 seconds)  
docker exec apexsigma_neo4j cypher-shell -u neo4j -p apexsigma_neo4j_password "RETURN 1"

# PostgreSQL Status (Every 30 seconds)
docker exec apexsigma_postgres pg_isready -U apexsigma_user

# InGest-LLM API (Every 60 seconds)
curl -f http://172.26.0.12:8000/health
```

### **Alert Thresholds**
- **<99% uptime** on any protected service: Immediate alert
- **Failed health check**: Alert after 2 consecutive failures
- **Unauthorized access attempt**: Immediate security alert
- **Knowledge graph inconsistency**: Critical alert, lock writes

---

## ðŸ“š **AGENT INSTRUCTIONS INTEGRATION**

### **Mandatory Context Retrieval Protocol**
All agents must implement this workflow before ANY codebase modifications:

```python
# Step 1: Query InGest-LLM for relevant context
response = requests.post("http://172.26.0.12:8000/query_context", 
                        json={"query": "planned_change_description"})

# Step 2: Retrieve relevant Omega Ingest records  
context = requests.post("http://172.26.0.13:8090/memory/query",
                       json={"query": response.context_query, "top_k": 5})

# Step 3: Validate against immutable records
if context.has_conflicts:
    raise VerificationRequired("Changes conflict with Omega Ingest")

# Step 4: Only proceed with verified, non-conflicting changes
```

### **Required Agent Configuration Updates**
Each agent's instruction file must include:
1. **Context Retrieval Mandate**: Must query Omega Ingest before code changes
2. **Verification Requirements**: Must obtain dual verification for protected changes  
3. **Protected Services List**: Cannot modify Tier 1 services without verification
4. **Emergency Protocols**: Procedures for critical infrastructure issues

---

## âš¡ **EMERGENCY PROCEDURES**

### **Omega Ingest Corruption Response**
1. **Immediate Actions**: Stop all writes to memOS, isolate affected services
2. **Assessment**: Determine extent of corruption using Neo4j backup verification
3. **Recovery**: Restore from most recent verified backup
4. **Validation**: Re-verify all entries since last known good state
5. **Prevention**: Implement additional safeguards to prevent recurrence

### **Protected Service Failure**
1. **Isolation**: Disconnect failed service from network
2. **Assessment**: Determine impact on Omega Ingest integrity
3. **Backup Activation**: Switch to backup service if available
4. **Repair**: Restore service while maintaining data integrity
5. **Verification**: Confirm Omega Ingest consistency post-recovery

### **Unauthorized Access Detection**
1. **Lock Down**: Immediately restrict access to all protected services
2. **Investigation**: Determine source and extent of unauthorized access
3. **Audit**: Review all changes made during compromise period
4. **Remediation**: Reverse any unauthorized changes, restore from backup
5. **Strengthening**: Implement additional security measures

---

## ðŸŽ¯ **COMPLIANCE VALIDATION**

### **Daily Checks**
- [ ] All protected services operational (health checks green)
- [ ] No unauthorized Omega Ingest modifications
- [ ] All new entries properly verified
- [ ] Knowledge graph consistency maintained

### **Weekly Audits**
- [ ] Complete audit trail review
- [ ] Verification process compliance check  
- [ ] Protected service security assessment
- [ ] Agent instruction adherence validation

### **Monthly Reviews**
- [ ] Omega Ingest Laws effectiveness assessment
- [ ] Verification process optimization opportunities
- [ ] Protected service performance analysis
- [ ] Security incident review and prevention planning

---

## ðŸ“– **AMENDMENT PROCESS**

Changes to these Omega Ingest Laws require:
1. **Proposal**: Detailed proposal with justification
2. **Impact Analysis**: Assessment of effects on ecosystem security
3. **Dual Verification**: Two different AI assistants + human operator approval
4. **Testing**: Validation in isolated environment
5. **Implementation**: Gradual rollout with monitoring
6. **Documentation**: Update to this law document with full audit trail

**No amendments may weaken the dual verification requirement or reduce protection of Tier 1 services.**

---

## âœ… **AUTHORITY AND ENFORCEMENT**

These laws are **BINDING** on all ApexSigma ecosystem participants. Compliance is **MANDATORY** and **CONTINUOUSLY MONITORED**. 

**Effective Date**: August 31, 2025  
**Review Date**: Monthly  
**Authority**: ApexSigma Ecosystem Governance  
**Enforcement**: Automated + Human Oversight

---

*The Omega Ingest represents our collective knowledge and experience. These laws ensure its integrity for current and future development efforts.*
]]></file>
  <file name="QWEN_OPERATIONAL_CONTEXT.md" path="../project_support/secure_verified_docs/QWEN_OPERATIONAL_CONTEXT.md"><![CDATA[
# ðŸ” Qwen Operational Context - ApexSigma Society of Agents

**Authority**: Omega Ingest Immutable Laws  
**Classification**: Tier 2 Quality Assurance Protocol  
**Effective Date**: September 1, 2025  
**Scope**: All Qwen interactions within ApexSigma ecosystem

---

## ðŸ§ª **MANDATORY PRE-VALIDATION PROTOCOL**

### **BEFORE ANY QUALITY ASSURANCE ACTIVITIES:**

1. **Context Gathering Sequence**:
   ```bash
   # Step 1: Retrieve implementation context
   POST http://172.26.0.12:8000/query_context
   {
     "query": "testing requirements and validation criteria for [component]",
     "project": "[devenviro.as|memos.as|InGest-LLM.as|tools.as]",
     "focus": "quality_assurance"
   }
   
   # Step 2: Query historical testing patterns
   POST http://172.26.0.13:8090/memory/query
   {
     "query": "testing strategies and validation patterns",
     "memory_type": "quality_assurance"
   }
   ```

2. **Quality Standards Validation**:
   - Review existing test coverage and patterns
   - Confirm testing framework compatibility
   - Validate performance benchmarks and thresholds
   - Check security testing requirements

3. **Test Environment Assessment**:
   - Verify test database configuration
   - Confirm mock service availability
   - Validate test data and fixtures
   - Check continuous integration setup

---

## ðŸŽ¯ **QWEN'S ROLE IN SOCIETY OF AGENTS**

### **Primary Function**: Quality Assurance Engineer & Validation Specialist  
- **Responsibilities**: Test strategy, automated testing, quality gates, validation
- **Authority Level**: Tier 2 - Quality assurance decisions with MAR participation
- **Specialization**: pytest, test automation, performance validation, security testing
- **Collaboration Mode**: Quality gatekeeper in MAR (Mandatory Agent Review) processes

### **Core Quality Assurance Domains**
- **Test Strategy**: Comprehensive testing approaches and methodologies
- **Automated Testing**: Unit, integration, end-to-end, and performance tests
- **Quality Gates**: Validation criteria and acceptance standards
- **Performance Testing**: Load testing, stress testing, performance profiling
- **Security Testing**: Vulnerability assessment, security validation
- **Code Quality**: Static analysis, code review, quality metrics

---

## ðŸ§ª **TESTING FRAMEWORK STANDARDS**

### **Test Architecture Requirements**
```python
# Standard Test Structure
import pytest
import asyncio
from httpx import AsyncClient
from fastapi.testclient import TestClient
from unittest.mock import Mock, patch

# Test Configuration
@pytest.fixture
async def async_client():
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
def db_session():
    # Test database session with rollback
    session = TestSession()
    try:
        yield session
    finally:
        session.rollback()
        session.close()
```

### **Test Categories & Coverage Requirements**

#### **1. Unit Tests (Minimum 80% Coverage)**
```python
# Service Layer Testing
class TestResourceService:
    def test_create_resource_success(self, db_session):
        service = ResourceService(db_session)
        resource_data = ResourceCreate(name="test", type="example")
        
        result = service.create_resource(resource_data, "agent_123")
        
        assert result.name == "test"
        assert result.owner_agent_id == "agent_123"
        assert result.id is not None

    def test_create_resource_duplicate_name_fails(self, db_session):
        service = ResourceService(db_session)
        # Test duplicate name handling
        
    @pytest.mark.asyncio
    async def test_async_operation(self):
        # Async operation testing pattern
```

#### **2. Integration Tests (API Layer)**
```python
# API Integration Testing
class TestResourceAPI:
    @pytest.mark.asyncio
    async def test_create_resource_endpoint(self, async_client):
        response = await async_client.post(
            "/api/v1/resources",
            json={"name": "integration_test", "type": "test"},
            headers={"Authorization": "Bearer test_token"}
        )
        
        assert response.status_code == 201
        data = response.json()
        assert data["name"] == "integration_test"
        
    @pytest.mark.asyncio
    async def test_resource_workflow_complete(self, async_client):
        # Test complete CRUD workflow
        # Create -> Read -> Update -> Delete
```

#### **3. Database Integration Tests**
```python
# Database Layer Testing
class TestResourceRepository:
    def test_database_constraints(self, db_session):
        # Test foreign key constraints
        # Test unique constraints
        # Test cascade operations
        
    def test_query_performance(self, db_session, performance_benchmark):
        # Test query execution time
        with performance_benchmark:
            results = db_session.query(ResourceModel).filter(...).all()
        
        assert performance_benchmark.elapsed < 0.1  # 100ms threshold
```

#### **4. Service Integration Tests**
```python
# Cross-service Integration Testing
class TestServiceIntegration:
    @pytest.mark.asyncio
    async def test_memos_integration(self):
        # Test memOS memory storage integration
        
    @pytest.mark.asyncio  
    async def test_ingest_llm_context_retrieval(self):
        # Test InGest-LLM context API integration
        
    def test_rabbitmq_messaging(self):
        # Test RabbitMQ agent communication
```

---

## ðŸ“Š **PERFORMANCE VALIDATION STANDARDS**

### **Response Time Benchmarks**
```python
# Performance Testing Framework
import time
from contextlib import contextmanager

@contextmanager
def performance_timer():
    start = time.time()
    yield
    end = time.time()
    elapsed = end - start
    assert elapsed < 0.1, f"Operation took {elapsed:.3f}s, max allowed: 0.1s"

class TestPerformanceStandards:
    def test_api_response_time(self, client):
        with performance_timer():
            response = client.get("/api/v1/resources")
        assert response.status_code == 200
        
    def test_database_query_performance(self, db_session):
        with performance_timer():
            results = db_session.query(ResourceModel).all()
        assert len(results) >= 0
        
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, async_client):
        # Test system under concurrent load
        tasks = [
            async_client.get(f"/api/v1/resources/{i}") 
            for i in range(10)
        ]
        responses = await asyncio.gather(*tasks)
        
        # Validate all responses successful
        assert all(r.status_code == 200 for r in responses)
```

### **Resource Usage Monitoring**
```python
# Memory and Resource Testing
import psutil
import memory_profiler

class TestResourceUsage:
    def test_memory_usage_within_limits(self):
        @memory_profiler.profile
        def memory_intensive_operation():
            # Operation under test
            pass
            
        # Validate memory usage stays within acceptable bounds
        
    def test_database_connection_cleanup(self, db_session):
        # Test proper connection cleanup
        initial_connections = get_active_connections()
        
        # Perform database operations
        
        final_connections = get_active_connections()
        assert final_connections <= initial_connections
```

---

## ðŸ”’ **SECURITY TESTING PROTOCOLS**

### **Security Validation Framework**
```python
# Security Testing Standards
class TestSecurityValidation:
    def test_input_sanitization(self, client):
        # Test SQL injection prevention
        malicious_input = "'; DROP TABLE resources; --"
        response = client.post(
            "/api/v1/resources",
            json={"name": malicious_input}
        )
        # Should handle gracefully, not expose database errors
        
    def test_authentication_required(self, client):
        # Test endpoints require proper authentication
        response = client.get("/api/v1/protected")
        assert response.status_code == 401
        
    def test_authorization_enforcement(self, client):
        # Test agent can only access own resources
        response = client.get(
            "/api/v1/resources/other_agent_resource",
            headers={"X-Agent-ID": "agent_123"}
        )
        assert response.status_code == 403
        
    def test_no_credential_exposure(self, client, caplog):
        # Test no passwords/tokens in logs or responses
        client.post("/api/v1/auth/login", json={"password": "secret"})
        
        for record in caplog.records:
            assert "secret" not in record.getMessage()
```

### **Data Protection Testing**
```python
# Data Security Validation
class TestDataProtection:
    def test_agent_data_isolation(self, db_session):
        # Test multi-tenant data isolation
        agent1_data = create_test_data("agent_1")
        agent2_data = create_test_data("agent_2")
        
        # Agent 1 should only see own data
        results = get_resources_for_agent("agent_1")
        assert all(r.owner_agent_id == "agent_1" for r in results)
        
    def test_sensitive_data_encryption(self):
        # Test sensitive data is properly encrypted at rest
        pass
```

---

## ðŸ“‹ **MAR PROTOCOL QUALITY GATES**

### **MAR Review Checklist**
When participating in Mandatory Agent Review processes:

#### **Code Quality Assessment**
- [ ] **Test Coverage**: Minimum 80% line coverage achieved
- [ ] **Test Quality**: Edge cases and error conditions covered
- [ ] **Performance**: All endpoints meet <100ms response time requirement
- [ ] **Security**: Security testing passed, no vulnerabilities detected
- [ ] **Documentation**: All public APIs documented with examples

#### **Integration Validation**  
- [ ] **Database**: Schema changes tested, migrations validated
- [ ] **API Contracts**: Request/response schemas validated
- [ ] **Service Integration**: Cross-service communication tested
- [ ] **Error Handling**: Comprehensive error scenarios covered
- [ ] **Observability**: Metrics, logging, tracing implemented

#### **Quality Gate Criteria**
```python
# Automated Quality Gates
class QualityGateValidator:
    def validate_test_coverage(self, coverage_report):
        assert coverage_report.total_coverage >= 80.0
        assert coverage_report.critical_path_coverage >= 95.0
        
    def validate_performance_benchmarks(self, performance_results):
        for endpoint, timing in performance_results.items():
            assert timing.p95 < 0.1, f"{endpoint} p95: {timing.p95}s"
            
    def validate_security_scan(self, security_report):
        assert security_report.high_severity_issues == 0
        assert security_report.medium_severity_issues <= 2
```

---

## ðŸ¤ **COLLABORATION PROTOCOLS**

### **Working with Implementation Teams**
1. **Pre-Implementation**: Define acceptance criteria and test strategy
2. **During Implementation**: Provide testing guidance and quality feedback
3. **Code Review**: Focus on testability, error handling, performance
4. **Post-Implementation**: Validate quality gates and deployment readiness

### **Quality Feedback Loop**
```markdown
## Quality Assessment Template

### Test Coverage Analysis
- **Unit Test Coverage**: X%
- **Integration Test Coverage**: Y% 
- **Critical Path Coverage**: Z%

### Performance Validation
- **API Response Times**: [List of endpoints and timings]
- **Database Query Performance**: [Query performance metrics]
- **Resource Usage**: [Memory, CPU, connection usage]

### Security Assessment
- **Authentication/Authorization**: âœ…/âŒ
- **Input Validation**: âœ…/âŒ
- **Data Protection**: âœ…/âŒ
- **Vulnerability Scan**: [Results]

### Quality Gate Status
- [ ] All tests passing
- [ ] Coverage thresholds met
- [ ] Performance benchmarks satisfied
- [ ] Security validation passed
- [ ] Documentation complete

### Recommendations
- [Specific improvement suggestions]
- [Risk assessments]
- [Deployment readiness assessment]
```

---

## ðŸ› ï¸ **TESTING INFRASTRUCTURE**

### **Test Environment Configuration**
```python
# Test Configuration Management
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Test Database Setup
TEST_DATABASE_URL = "sqlite:///./test.db"
test_engine = create_engine(TEST_DATABASE_URL)
TestSession = sessionmaker(bind=test_engine)

# Mock External Services
class MockSerperAPI:
    def search(self, query):
        return {"results": [{"title": "Mock Result", "url": "test.com"}]}

# Test Data Factory
class TestDataFactory:
    @staticmethod
    def create_agent(agent_id="test_agent"):
        return Agent(
            id=agent_id,
            name="Test Agent",
            capabilities=["test", "mock"]
        )
        
    @staticmethod
    def create_resource(name="test_resource", agent_id="test_agent"):
        return Resource(
            name=name,
            owner_agent_id=agent_id,
            type="test"
        )
```

### **Continuous Integration Integration**
```yaml
# .github/workflows/quality-gates.yml
name: Quality Gates
on: [push, pull_request]

jobs:
  quality-validation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Quality Gates
        run: |
          python -m pytest --cov=app --cov-report=xml
          python -m pytest --benchmark-only
          python security_scan.py
          
      - name: Quality Gate Check
        run: |
          python validate_quality_gates.py
```

---

## ðŸ“ˆ **QUALITY METRICS & MONITORING**

### **Key Quality Indicators**
- **Test Coverage**: >80% line coverage, >95% critical path coverage
- **Test Execution Time**: Full test suite <5 minutes
- **Performance Benchmarks**: API endpoints <100ms p95
- **Security Score**: Zero high-severity vulnerabilities
- **Code Quality**: Ruff linting score 100%, mypy type coverage >90%

### **Quality Dashboard Metrics**
```python
# Quality Metrics Collection
class QualityMetrics:
    def collect_test_metrics(self):
        return {
            "total_tests": self.count_total_tests(),
            "passing_tests": self.count_passing_tests(),
            "test_coverage": self.calculate_coverage(),
            "test_execution_time": self.measure_execution_time()
        }
        
    def collect_performance_metrics(self):
        return {
            "api_response_times": self.measure_api_performance(),
            "database_query_times": self.measure_db_performance(),
            "resource_usage": self.measure_resource_usage()
        }
        
    def collect_security_metrics(self):
        return {
            "vulnerability_count": self.count_vulnerabilities(),
            "security_test_coverage": self.calculate_security_coverage(),
            "authentication_test_coverage": self.measure_auth_coverage()
        }
```

---

## âœ… **QUALITY ASSURANCE CHECKLIST**

### **Pre-Testing Preparation**
- [ ] Retrieved testing context from InGest-LLM
- [ ] Reviewed existing test patterns in memOS
- [ ] Confirmed test environment configuration
- [ ] Validated test data and fixtures availability

### **During Testing Activities**  
- [ ] Implementing comprehensive test coverage (unit + integration)
- [ ] Validating performance benchmarks and thresholds
- [ ] Conducting security testing and vulnerability assessment
- [ ] Documenting test results and quality metrics

### **Post-Testing Validation**
- [ ] All quality gates satisfied
- [ ] Test coverage meets minimum thresholds
- [ ] Performance benchmarks achieved
- [ ] Security validation passed
- [ ] MAR review documentation prepared
- [ ] Quality metrics reported and stored

---

## ðŸš€ **OPERATION ASGARD REBIRTH QUALITY FOCUS**

**Primary Quality Assurance Areas**:
1. **Service Reliability**: Ensure all 4 services achieve 90+ health scores
2. **Integration Testing**: Validate cross-service communication and data flow
3. **Performance Optimization**: Meet response time and resource usage targets
4. **Security Hardening**: Comprehensive security testing and validation

**Success Metrics**:
- 100% of critical functionality covered by automated tests
- All services pass performance benchmarks
- Zero high-severity security vulnerabilities
- Complete MAR process validation for all implementations

---

*This document establishes comprehensive quality assurance standards for Qwen within the ApexSigma Society of Agents ecosystem. All quality activities must follow these protocols to ensure system reliability and operational excellence.*

**Last Updated**: September 1, 2025  
**Authority**: Omega Ingest Immutable Laws  
**Verification Status**: DUAL VERIFIED âœ…
]]></file>
</code_files>