<code_files>
  <file name="MCP Server Build Plan memOS & InGestLLM.yml" path="MCP Server Build Plan memOS & InGestLLM.yml"><![CDATA[
# 🧠 MCP Server Build Plan: memOS & InGestLLM

*A compre- [x] **1.8 Set up audit logging** for all critical MCP operations.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Audit logging implemented (auth attempts, rate violations, MCP requests)ve, trackable checklist for implementing the MCP server infrastructure, governed by MAR protocols and Omega Ingest Laws.*

## 🧩 PHASE 1: INFRASTRUCTURE PREPARATION

### 🛠️ Docker Environment Setup

- [x] **1.1 Verify current Docker network configuration** (apexsigma\_net).
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Docker network configuration verified and apexsigma_net is properly configured

- [x] **1.2 Create dedicated MCP network subnet** (172.28.0.0/16) for isolation.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: PARTIALLY COMPLETED | Network exists with correct subnet 172.28.0.0/16, but not defined in compose files

- [x] **1.3 Add MCP services to Docker Compose** with static IP allocation.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Added memos-mcp-server (172.28.0.10) and ingest-llm-mcp-server (172.28.0.11) with static IPs

- [x] **1.4 Configure service discovery** using DNS names within the Docker network.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | DNS names configured via container names (memos_mcp_server, ingest_llm_mcp_server)

### 🔒 Security Configuration

- [x] **1.5 Implement JWT authentication** for all MCP endpoints.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | JWT auth implemented with service accounts (MCP_COPILOT, MCP_GEMINI, MCP_QWEN)

- [x] **1.6 Create dedicated MCP service accounts** for each AI assistant.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Service accounts created as part of JWT auth (MCP_COPILOT, MCP_GEMINI, MCP_QWEN)

- [x] **1.7 Configure rate limiting** per service account.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Rate limiting implemented (60 req/min per service account)

- [x] **1.8 Set up audit logging** for all critical MCP operations.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR
    - **Status**: COMPLETED | Audit logging implemented for authentication attempts, rate limit violations, and MCP operations

### 📊 Observability Integration

- [x] **1.9 Extend Langfuse configuration** to trace MCP-specific operations.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
    - **Status**: COMPLETED | Added Langfuse dependency to memos.as, configured environment variables, implemented MCP-specific tracing for authentication, rate limiting, and tool operations in both servers

- [x] **1.10 Create MCP-specific Prometheus metrics** (Counters, Histograms).
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [x] **1.11 Configure Grafana dashboards** for real-time MCP performance monitoring.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [x] **1.12 Implement distributed tracing** for MCP operations using OpenTelemetry.
  
    - **Implementer**: agent:Copilot
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

## 🧠 PHASE 2: memOS MCP EXTENSION

### 🗃️ Memory Tier Architecture

- [x] **2.1 Implement MCP-specific memory tiers** (e.g., MCP\_GEMINI).
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | MCP tier mapping system implemented with proper logical to physical tier mapping

- [x] **2.2 Create agent-specific memory isolation** by agent\_id.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | Agent-specific memory isolation implemented.

- [x] **2.3 Implement cross-agent knowledge sharing** via a confidence-scored query system.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | Implemented knowledge sharing endpoints and confidence scoring. MCP tools pending.

- [x] **2.3.1 Implement MCP tools for cross-agent knowledge sharing**.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: "COMPLETED | Implemented 4 MCP tools: request_knowledge_from_agent, offer_knowledge_to_request, get_pending_knowledge_requests, accept_knowledge_offer"

- [x] **2.4 Add memory expiration policies** for each tier.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR, Omega Ingest Laws
    - **Status**: COMPLETED | Implemented expiration for Tier 1 (Redis) and Tier 2 (PostgreSQL/Qdrant) with a background worker. Documentation pending.

### 🔗 Omega Ingest Integration

- [ ] **2.5 Create a POML processor** to parse documents into knowledge graph nodes.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **2.6 Implement entity relationship mapping** with contextual weights.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **2.7 Develop semantic search** with contextual weighting.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **2.8 Create a knowledge graph visualization API endpoint**.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

### ⚡ Performance Optimization

- [ ] **2.9 Implement a multi-layer caching strategy** (In-memory -\> Redis -\> DB).
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

- [ ] **2.10 Optimize Neo4j Cypher queries** for common agent-specific access patterns.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

- [ ] **2.11 Implement batch processing** for memory storage and retrieval operations.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

- [ ] **2.12 Add query optimization heuristics** to refine search queries.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Copilot
    - **Protocols**: MAR

## 📥 PHASE 3: InGestLLM MCP EXTENSION

### 🧮 Tokenization Pipeline

- [ ] **3.1 Integrate Tekken tokenizer** with custom patterns for MCP-specific data types.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **3.2 Create POML-aware tokenization rules** to assign importance and weight.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **3.3 Implement context-aware token weighting** based on the agent's current task.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

- [ ] **3.4 Develop token compression strategies** to preserve key information.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR, Omega Ingest Laws

### 🌐 Web Scraping & Repo Raiding

- [ ] **3.5 Create an MCP-specific web scraper** that applies agent-specific rules.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

- [ ] **3.6 Implement GitHub repository ingestion**, including cloning and processing files.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

- [ ] **3.7 Develop code-aware content extraction** using Abstract Syntax Tree (AST) parsing.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

- [ ] **3.8 Create documentation-aware summarization** to preserve key information.
  
    - **Implementer**: agent:Gemini
    - **Reviewer**: agent:Qwen
    - **Protocols**: MAR

### 🧪 Validation & Quality Control

- [ ] **3.9 Implement token quality metrics** (completeness, coherence, relevance).
  
    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [ ] **3.10 Create LLM consumption validation** to check for issues that might confuse models.
  
    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [ ] **3.11 Develop a context preservation scoring** system using embedding similarity.
  
    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR

- [ ] **Implement a feedback loop** for agents to report poor quality ingestion.

    - **Implementer**: agent:Qwen
    - **Reviewer**: agent:Gemini
    - **Protocols**: MAR
]]></file>
  <file name=".env" path="memos.as/.env"><![CDATA[
# Langfuse Configuration for memOS.as
LANGFUSE_SECRET_KEY=sk-lf-2d1442e7-96dd-44f0-a440-978b279196cd
LANGFUSE_PUBLIC_KEY=pk-lf-db478257-9f4c-4ee0-a895-b96d67f80892
LANGFUSE_HOST=https://cloud.langfuse.com

# MCP Server Configuration
MCP_SERVER_NAME=memos-as
MCP_SERVER_VERSION=1.0.0

# Database Configuration
DATABASE_URL=sqlite:///./context.db

# JWT Configuration for MCP Authentication
JWT_SECRET_KEY=your-jwt-secret-key-here
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# Service Account Tokens for MCP Authentication
MCP_COPILOT_TOKEN=your-copilot-token-here
MCP_GEMINI_TOKEN=your-gemini-token-here
MCP_QWEN_TOKEN=your-qwen-token-here

# Rate Limiting
MCP_RATE_LIMIT_REQUESTS_PER_MINUTE=60

# Monitoring Configuration
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
JAEGER_AGENT_HOST=localhost
JAEGER_AGENT_PORT=14268
]]></file>
  <file name="Dockerfile" path="memos.as/Dockerfile"><![CDATA[
# Use an official Python runtime as a parent image
FROM python:3.11-slim

# Set the working directory in the container
WORKDIR /code

# Add the app directory to the PYTHONPATH
ENV PYTHONPATH=/code

# Install curl
RUN apt-get update && apt-get install -y curl

# Copy the dependencies file to the working directory
COPY requirements.txt .

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy the application's code to the working directory
COPY ./app /code/app

# Command to run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8090"]

]]></file>
  <file name="QWEN.md" path="memos.as/QWEN.md"><![CDATA[
# QWEN.md - memOS.as Project

## ⚠️ **MANDATORY: OMEGA INGEST CONTEXT RETRIEVAL PROTOCOL**

**BEFORE MAKING ANY CODE CHANGES**, you MUST:

1. **Query InGest-LLM API** for relevant context: `http://172.26.0.12:8000/query_context`
2. **Retrieve from memOS Omega Ingest**: `http://172.26.0.13:8090/memory/query`
3. **Validate against immutable truth**: Ensure changes don't conflict with verified infrastructure
4. **Obtain dual verification**: For Tier 1 infrastructure changes, require verification from another AI assistant

**Protected Services (DO NOT MODIFY WITHOUT VERIFICATION)**:
- memOS API (172.26.0.13) - Omega Ingest Guardian
- Neo4j Knowledge Graph (172.26.0.14) - Immutable concept relationships
- PostgreSQL Main (172.26.0.2) - Procedural memory
- InGest-LLM API (172.26.0.12) - Data ingestion gateway

**Your Role in MCP Server Development**:
- **Primary**: Code review and quality assurance for MCP server implementations
- **MAR Protocol**: Act as reviewer for GitHub Copilot and Gemini MCP implementations
- **Verification Authority**: Quality validation and testing for MCP infrastructure
- **Current Focus**: Validation, quality control, and Phase 3 InGestLLM MCP features

## Project Overview

This project, `memos.as` (memOS), serves as the cognitive core for the DevEnviro ecosystem with a focus on **MCP server development**. It provides persistent memory and tool discovery capabilities for AI agents through standardized Model Context Protocol (MCP) interfaces, transforming them from simple executors into resourceful, learning problem-solvers.

**Current Development Focus**: MCP server implementation for memOS and InGestLLM services. Broader ecosystem development is on hold.

## Key Technologies & Architecture

- **Core Platform**: Python 3.13, FastAPI, MCP Server Framework
- **MCP Servers**:
  - `memos-mcp-server`: Memory operations MCP server (172.28.0.10)
  - `ingest-llm-mcp-server`: Data ingestion MCP server (172.28.0.11)
- **Memory Tiers**:
  - **Tier 1 (Working Memory)**: Redis for high-speed caching and temporary storage.
  - **Tier 2 (Episodic/Procedural Memory)**: PostgreSQL (structured data) and Qdrant (vector embeddings).
  - **Tier 3 (Semantic Memory)**: Neo4j (knowledge graph for concepts and relationships).
- **MCP Infrastructure**: JWT authentication, service accounts (MCP_COPILOT, MCP_GEMINI, MCP_QWEN), rate limiting (60 req/min), audit logging
- **Observability**: OpenTelemetry (Jaeger for tracing, Prometheus for metrics), Structlog (logging), Langfuse (LLM observability).
- **Database Clients**: Custom clients for PostgreSQL, Qdrant, Redis, and Neo4j.
- **Dependency Management**: Poetry (declared in `pyproject.toml`), with dependencies listed in `requirements.txt`.

## Directory Structure

```
memos.as/
├── app/                       # Main application package
│   ├── main.py                # FastAPI application entry point and core endpoints
│   ├── mcp_server.py         # MCP server implementation for memory operations
│   ├── models/                # Pydantic data models for requests/responses
│   ├── services/              # Database and external service clients
│   └── tests/                 # Application tests
├── config/                    # Configuration files
├── docs/                      # Documentation (MkDocs)
├── scripts/                   # Utility scripts
├── Dockerfile                 # Application Docker image definition
├── docker-compose.yml         # Service definition for unified DevEnviro stack
├── pyproject.toml             # Poetry project configuration
├── poetry.lock                # Locked dependencies
├── requirements.txt           # List of dependencies
├── .github/
│   └── copilot-instructions.md # AI agent development guidelines
├── GEMINI.md                  # Gemini-specific development instructions
├── QWEN.md                    # Qwen-specific development instructions
└── README.md                  # Project README
```

## MCP Development Phases

### Phase 1: Infrastructure Preparation (Current Focus)
- ✅ Docker network configuration (172.28.0.0/16)
- ✅ JWT authentication and service accounts
- ✅ Rate limiting per service account
- ✅ Langfuse tracing integration
- 🔄 **Audit logging setup** (in progress)
- 🔄 **Prometheus metrics** (pending)
- 🔄 **Grafana dashboards** (pending)
- 🔄 **Distributed tracing** (pending)

### Phase 2: memOS MCP Extension (Next)
- Agent-specific memory tiers (MCP_GEMINI, MCP_COPILOT, etc.)
- Omega Ingest integration with POML processing
- Cross-agent knowledge sharing via confidence-scored queries
- Multi-layer caching optimization

### Phase 3: InGestLLM MCP Extension (Future)
- Tokenization pipeline with Tekken tokenizer
- Web scraping and GitHub repository ingestion
- Code-aware content extraction using AST parsing
- Token quality validation and context preservation

## Core Concepts

- **Memory Tiers**:
  - **Tier 1 (`/memory/1/store`)**: Stores data in Redis. Used for working memory and caching.
  - **Tier 2 (`/memory/2/store`)**: Stores data in PostgreSQL and Qdrant. Used for episodic events and procedural knowledge (like code). This is the default storage path used by `InGest-LLM.as`.
  - **Tier 3 (`/memory/3/store`)**: Stores data in Neo4j. Used for semantic knowledge and relationships between concepts.
- **MCP Tools**: Standardized tools for memory operations accessible by AI assistants
- **Tool Discovery**: Agents can register tools they provide. Other agents can discover relevant tools by querying memory based on context.
- **Observability**: Integrated metrics and tracing to monitor service health and performance across all memory tiers.

## Building and Running

### Prerequisites

- Python 3.13
- Poetry
- Docker and Docker Compose (for containerized deployment)
- Access to PostgreSQL, Qdrant, Redis, and Neo4j instances (as configured in `docker-compose.unified.yml` or `.env`)

### Development Setup

1.  Install dependencies (managed by Poetry, but `requirements.txt` exists):
    ```bash
    # If using Poetry (preferred)
    poetry install

    # Or, if using pip with requirements.txt
    pip install -r requirements.txt
    ```
2.  Configure environment variables by creating a `.env` file (refer to `docker-compose.yml` or services for required variables like database URIs).
3.  Run the development server:
    ```bash
    # Main memOS API
    python app/main.py
    # Or with uvicorn
    uvicorn app.main:app --reload

    # MCP Server
    uvicorn app.mcp_server:app --reload --host 0.0.0.0 --port 8091
    ```

### Docker Deployment

The service is designed to run within the larger DevEnviro ecosystem using Docker Compose.

1.  Build the image:
    ```bash
    docker build -t memos-as .
    ```
2.  Run the container (typically as part of the unified stack):
    ```bash
    docker-compose up
    ```

## Key API Endpoints

### Core memOS Endpoints
- **GET /**: Health check.
- **GET /health**: Detailed health check for all connected services (PostgreSQL, Qdrant, Redis, Neo4j).
- **GET /metrics**: Prometheus metrics endpoint.
- **POST /memory/store**: Stores memory across all tiers (calls the Tier 2 logic by default).
- **POST /memory/{tier}/store**: Stores memory in a specific tier (1, 2, or 3).
- **GET /memory/{memory_id}**: Retrieves a specific memory by its ID from PostgreSQL.
- **POST /memory/query**: Performs a semantic search using Qdrant and discovers relevant tools from PostgreSQL.
- **GET /memory/search**: Simple query endpoint for searching memories.
- **POST /tools/register**: Registers a new tool capability.
- **GET /tools**: Retrieves all registered tools.
- **GET /tools/search**: Searches for tools based on a query context.
- **GET /cache/stats**: Gets Redis cache statistics.
- **DELETE /cache/clear**: Clears the Redis cache.

### MCP Server Endpoints
- **GET /mcp/health**: MCP server health check
- **POST /mcp/tools/store_memory**: MCP tool for storing memory
- **POST /mcp/tools/query_memory**: MCP tool for querying memory
- **POST /mcp/tools/register_tool**: MCP tool for registering tools
- **GET /mcp/tools/list**: MCP tool for listing available tools

## Development Workflow

1.  **Modify Code**: Edit files in `app/`, focusing on MCP server implementation.
2.  **Run Tests**: Execute tests using `pytest`.
3.  **Build & Deploy**: Use `poetry build` for distribution or `docker build`/`docker-compose` for containerized deployment within the DevEnviro stack.
4.  **Interact**: Use the FastAPI docs at `http://localhost:8090/docs` (main API) or `http://localhost:8091/docs` (MCP server) to interact with the APIs.
5.  **Quality Assurance**: Follow MAR protocol - get reviews from other AI assistants before integration.

## MCP Server Build Plan Reference

See `MCP Server Build Plan memOS & InGestLLM.yml` for the complete development roadmap and current task status.

This `QWEN.md` provides the essential context for working with the `memos.as` MCP server development project.

]]></file>
  <file name="main.py" path="memos.as/app/main.py"><![CDATA[
import os
import logging
from typing import Optional

from fastapi import Depends, FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware

from app.models import (
    QueryRequest,
    StoreRequest,
    ToolRegistrationRequest,
    GraphQueryRequest,
    LLMCacheRequest,
    LLMUsageRequest,
    LLMPerformanceRequest,
)
from app.schemas import MCPTier, MCP_TIER_MAPPING
from app.services.postgres_client import PostgresClient, get_postgres_client
from app.services.qdrant_client import QdrantMemoryClient, get_qdrant_client
from app.services.redis_client import RedisClient, get_redis_client
from app.services.neo4j_client import Neo4jClient, get_neo4j_client
from app.services.observability import (
    ObservabilityService,
    get_observability,
    trace_async,
)

# Initialize FastAPI app
app = FastAPI(
    title="memOS.as",
    description="Memory and tool discovery hub for the DevEnviro AI agent ecosystem",
    version="1.0.0",
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize logging
logger = logging.getLogger(__name__)

# Initialize observability
obs = get_observability()
obs.instrument_fastapi(app)
obs.instrument_database_clients()


@app.get("/")
async def root():
    """Health check endpoint"""
    return {
        "service": "memOS.as",
        "status": "running",
        "description": "Memory and tool discovery hub for AI agents",
    }


@app.get("/cache/stats")
async def get_cache_stats(
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Get Redis cache statistics and performance metrics.
    """
    try:
        if not redis_client.is_connected():
            return {
                "error": "Redis not connected",
                "connected": False,
                "stats": None,
            }

        stats = redis_client.get_cache_stats()
        return {
            "connected": True,
            "stats": stats,
            "message": "Cache statistics retrieved successfully",
        }

    except Exception as e:
        logger.error(f"Error getting cache stats: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Error getting cache stats: {str(e)}"
        )


@app.delete("/cache/clear")
async def clear_cache(
    pattern: str = "*",
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Clear cache entries matching the specified pattern.
    Default pattern '*' clears all cache entries.
    """
    try:
        if not redis_client.is_connected():
            return {
                "error": "Redis not connected",
                "connected": False,
                "cleared": 0,
            }

        cleared_count = redis_client.clear_cache_pattern(pattern)
        return {
            "connected": True,
            "pattern": pattern,
            "cleared": cleared_count,
            "message": f"Cleared {cleared_count} cache entries",
        }

    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error clearing cache: {str(e)}")


@app.get("/health")
async def health_check(
    observability: ObservabilityService = Depends(get_observability),
):
    """Enhanced health check with graceful database error handling"""
    health_data = observability.health_check()
    services_status = {}

    # Check PostgreSQL with graceful fallback
    try:
        postgres_client = get_postgres_client()
        # Test with a simple query
        with postgres_client.get_session() as session:
            session.execute("SELECT 1")
        services_status["postgres"] = "connected"
        observability.active_connections.labels(database="postgresql").set(1)
    except Exception as e:
        services_status["postgres"] = f"disconnected: {str(e)[:100]}"
        observability.active_connections.labels(database="postgresql").set(0)

    # Check Qdrant with graceful fallback
    try:
        qdrant_client = get_qdrant_client()
        qdrant_info = qdrant_client.get_collection_info()
        services_status["qdrant"] = "connected" if qdrant_info else "disconnected"
        observability.active_connections.labels(database="qdrant").set(
            1 if qdrant_info else 0
        )
    except Exception as e:
        services_status["qdrant"] = f"disconnected: {str(e)[:100]}"
        qdrant_info = None
        observability.active_connections.labels(database="qdrant").set(0)

    # Check Redis with graceful fallback
    try:
        redis_client = get_redis_client()
        redis_client.client.ping()
        services_status["redis"] = "connected"
        observability.active_connections.labels(database="redis").set(1)
    except Exception as e:
        services_status["redis"] = f"disconnected: {str(e)[:100]}"
        observability.active_connections.labels(database="redis").set(0)

    # Check Neo4j with graceful fallback
    try:
        neo4j_client = get_neo4j_client()
        if neo4j_client.driver:
            with neo4j_client.get_session() as session:
                session.run("RETURN 1")
            services_status["neo4j"] = "connected"
        else:
            services_status["neo4j"] = "disconnected: driver not initialized"
    except Exception as e:
        services_status["neo4j"] = f"disconnected: {str(e)[:100]}"

    # Log health check with detailed status
    observability.log_structured(
        "info",
        "Health check performed",
        **{f"{db}_status": status for db, status in services_status.items()},
    )

    # Determine if integration is ready (PostgreSQL is critical)
    integration_ready = "connected" in services_status.get("postgres", "")

    health_data.update(
        {
            "services": services_status,
            "integration_ready": integration_ready,
            "qdrant_collection": qdrant_info if "qdrant_info" in locals() else None,
            "operational_mode": "full"
            if all("connected" in status for status in services_status.values())
            else "degraded",
        }
    )

    return health_data


@app.get("/metrics")
async def get_metrics(
    observability: ObservabilityService = Depends(get_observability),
):
    """Prometheus metrics endpoint for DevEnviro monitoring stack."""
    from fastapi.responses import PlainTextResponse

    return PlainTextResponse(observability.get_metrics(), media_type="text/plain")


# Tool Management Endpoints
@app.post("/tools/register")
async def register_tool(
    tool_request: ToolRegistrationRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Register a new tool in the PostgreSQL registered_tools table.

    This endpoint allows agents to register their capabilities so other agents
    can discover and use them via the /memory/query endpoint.
    """
    try:
        tool_id = postgres_client.register_tool(
            name=tool_request.name,
            description=tool_request.description,
            usage=tool_request.usage,
            tags=tool_request.tags,
        )

        if tool_id is None:
            raise HTTPException(
                status_code=400,
                detail="Failed to register tool. Tool name might already exist.",
            )

        # Invalidate tool caches after successful registration
        if redis_client.is_connected():
            redis_client.invalidate_tool_caches()

        return {
            "success": True,
            "tool_id": tool_id,
            "message": f"Tool '{tool_request.name}' registered successfully",
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error registering tool: {str(e)}")


@app.get("/tools/{tool_id}")
async def get_tool(
    tool_id: int,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Get a specific tool by ID"""
    try:
        tool = postgres_client.get_tool(tool_id)

        if tool is None:
            raise HTTPException(status_code=404, detail="Tool not found")

        return tool

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving tool: {str(e)}")


@app.get("/tools")
async def get_all_tools(
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Get all registered tools"""
    try:
        tools = postgres_client.get_all_tools()
        return {"tools": tools, "count": len(tools)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving tools: {str(e)}")


@app.get("/tools/search")
async def search_tools(
    query: str,
    limit: int = 10,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Search for tools by query context"""
    try:
        tools = postgres_client.get_tools_by_context(query, limit)
        return {"tools": tools, "count": len(tools), "query": query}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error searching tools: {str(e)}")


# Memory Management Endpoints
@app.post("/memory/mcp/{mcp_tier}/store")
async def store_mcp_memory(
    mcp_tier: MCPTier,
    store_request: StoreRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
    observability: ObservabilityService = Depends(get_observability),
):
    """
    Store a new memory in a specific MCP logical tier.
    """
    storage_tier = MCP_TIER_MAPPING[mcp_tier].value
    return await store_memory_by_tier(
        tier=storage_tier,
        store_request=store_request,
        postgres_client=postgres_client,
        qdrant_client=qdrant_client,
        redis_client=redis_client,
        neo4j_client=neo4j_client,
        observability=observability,
    )

@app.post("/memory/store")
@trace_async("memory.store")
async def store_memory(
    store_request: StoreRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
    observability: ObservabilityService = Depends(get_observability),
):
    """
    Store a new memory with embeddings and knowledge graph updates.

    Logic:
    1. Generate an embedding for the content (using placeholder function initially)
    2. Store the full content/metadata in PostgreSQL to get a unique ID
    3. Store the vector embedding and the PostgreSQL ID in Qdrant
    4. Extract concepts from content and update Neo4j knowledge graph
    5. Return the PostgreSQL ID and knowledge graph information
    """
    try:
        # Debug: verify dependency types
        try:
            observability.log_structured(
                "info",
                "store_memory dependency types",
                postgres_client_type=str(type(postgres_client)),
                qdrant_client_type=str(type(qdrant_client)),
                redis_client_type=str(type(redis_client)),
                neo4j_client_type=str(type(neo4j_client)),
            )
        except Exception:
            pass
        import time

        start_time = time.time()

        # Step 1: Generate an embedding for the content (with caching)
        # First check if we have cached embedding for this content
        cached_embedding = redis_client.get_cached_embedding(store_request.content)
        if cached_embedding:
            embedding = cached_embedding
            observability.record_memory_operation("embedding_cache_hit", "success")
        else:
            embedding = qdrant_client.generate_placeholder_embedding(
                store_request.content
            )
            # Cache the generated embedding
            redis_client.cache_embedding(store_request.content, embedding)
            observability.record_memory_operation("embedding_generation", "success")

        # Step 2: Store the full content/metadata in PostgreSQL to get a unique ID
        postgres_start = time.time()
        memory_id = postgres_client.store_memory(
            content=store_request.content, agent_id=store_request.agent_id, metadata=store_request.metadata
        )
        postgres_duration = time.time() - postgres_start

        if memory_id is None:
            observability.record_memory_operation(
                "postgres_store", "failed", "tier2", postgres_duration
            )
            raise HTTPException(
                status_code=500, detail="Failed to store memory in PostgreSQL"
            )

        observability.record_memory_operation(
            "postgres_store", "success", "tier2", postgres_duration
        )

        # Step 3: Store the vector embedding and the PostgreSQL ID in Qdrant (with graceful fallback)
        point_id = None
        qdrant_success = False

        try:
            qdrant_start = time.time()
            point_id = qdrant_client.store_embedding(
                embedding=embedding,
                memory_id=memory_id,
                agent_id=store_request.agent_id,
                metadata=store_request.metadata,
            )
            qdrant_duration = time.time() - qdrant_start

            if point_id is not None:
                observability.record_memory_operation(
                    "qdrant_store", "success", "tier2", qdrant_duration
                )
                # Update PostgreSQL record with the Qdrant point ID for linking
                postgres_client.update_memory_embedding_id(memory_id, point_id)
                qdrant_success = True
            else:
                observability.record_memory_operation(
                    "qdrant_store", "failed", "tier2", qdrant_duration
                )

        except Exception as e:
            qdrant_duration = (
                time.time() - qdrant_start if "qdrant_start" in locals() else 0
            )
            observability.record_memory_operation(
                "qdrant_store", "failed", "tier2", qdrant_duration
            )
            observability.log_structured(
                "warning",
                "Qdrant storage failed, continuing with degraded functionality",
                memory_id=memory_id,
                error=str(e),
            )

        # Step 4: Extract concepts and update Neo4j knowledge graph
        concepts = []
        neo4j_info = {}
        try:
            if neo4j_client.driver:  # Only if Neo4j is available
                neo4j_start = time.time()

                # Extract concepts from content
                concepts = neo4j_client.extract_concepts_from_content(
                    store_request.content
                )
                observability.record_concepts_extracted(len(concepts))

                # Create memory node in Neo4j knowledge graph
                memory_node = neo4j_client.create_memory_node(
                    memory_id=memory_id,
                    content=store_request.content,
                    concepts=concepts,
                )

                neo4j_duration = time.time() - neo4j_start
                observability.record_memory_operation(
                    "neo4j_store", "success", "tier3", neo4j_duration
                )
                observability.record_knowledge_graph_operation(
                    "create_memory_node", "Memory"
                )
                for concept in concepts:
                    observability.record_knowledge_graph_operation(
                        "create_concept_node", "Concept"
                    )

                neo4j_info = {
                    "concepts_extracted": len(concepts),
                    "concepts": concepts,
                    "memory_node_created": True,
                }

                # Log successful knowledge graph update
                observability.log_structured(
                    "info",
                    "Knowledge graph updated",
                    memory_id=memory_id,
                    concepts_count=len(concepts),
                    duration=neo4j_duration,
                )

        except Exception as e:
            # Neo4j integration failure shouldn't break the main storage flow
            observability.record_memory_operation("neo4j_store", "failed", "tier3")
            observability.log_structured(
                "warning",
                "Neo4j integration failed",
                memory_id=memory_id,
                error=str(e),
            )
            neo4j_info = {
                "concepts_extracted": 0,
                "concepts": [],
                "memory_node_created": False,
                "error": str(e),
            }

        # Step 5: Invalidate related caches after successful storage
        if redis_client.is_connected():
            try:
                redis_client.invalidate_memory_caches(memory_id)
                observability.log_structured(
                    "info", "Memory caches invalidated", memory_id=memory_id
                )
            except Exception as e:
                observability.log_structured(
                    "warning",
                    "Failed to invalidate memory caches",
                    memory_id=memory_id,
                    error=str(e),
                )

        # Step 6: Return comprehensive storage information with degraded mode indicators
        storage_status = {
            "postgres": True,  # Always true if we reach this point
            "qdrant": qdrant_success,
            "neo4j": neo4j_info.get("memory_node_created", False),
        }

        operational_mode = "full" if all(storage_status.values()) else "degraded"

        return {
            "success": True,
            "memory_id": memory_id,
            "point_id": point_id,
            "knowledge_graph": neo4j_info,
            "storage_status": storage_status,
            "operational_mode": operational_mode,
            "message": f"Memory stored successfully in {operational_mode} mode",
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error storing memory: {str(e)}")


@app.post("/memory/{tier}/store")
@trace_async("memory.store_by_tier")
async def store_memory_by_tier(
    tier: str,
    store_request: StoreRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
    observability: ObservabilityService = Depends(get_observability),
):
    """
    Store a new memory in a specific tier.
    """
    if tier == "1":
        # Tier 1: Redis (Working Memory & Cache)
        try:
            # For Redis, we need a key. We can use a hash of the content as the key.
            import hashlib

            key = hashlib.md5(store_request.content.encode()).hexdigest()
            redis_client.store_memory(key, store_request.dict())
            observability.record_memory_operation("redis_store", "success", "tier1")
            return {
                "success": True,
                "tier": 1,
                "memory_id": key,
                "message": "Memory stored in Redis",
            }
        except Exception as e:
            observability.record_memory_operation("redis_store", "failed", "tier1")
            raise HTTPException(
                status_code=500,
                detail=f"Error storing memory in Redis: {str(e)}",
            )
    elif tier == "2":
        # Tier 2: PostgreSQL & Qdrant (Episodic & Procedural Memory)
        result = await store_memory(
            store_request,
            postgres_client=postgres_client,
            qdrant_client=qdrant_client,
            redis_client=redis_client,
            neo4j_client=neo4j_client,
            observability=observability,
        )
        # Ensure memory_id and tier are included in the response
        if isinstance(result, dict):
            if "memory_id" not in result and "postgres" in result:
                try:
                    result["memory_id"] = result["postgres"].get("memory_id")
                except Exception:
                    pass
            # Include tier information for client compatibility
            result.setdefault("tier", 2)
        return result
    elif tier == "3":
        # Tier 3: Neo4j (Semantic Memory)
        try:
            # First, store in PostgreSQL to get a unique ID
            memory_id = postgres_client.store_memory(
                content=store_request.content, metadata=store_request.metadata
            )
            if not memory_id:
                raise HTTPException(
                    status_code=500,
                    detail="Failed to store memory in PostgreSQL",
                )
            # Then, attempt to store in Neo4j with the new ID
            neo4j_info = {
                "memory_node_created": False,
                "concepts_extracted": 0,
                "concepts": [],
            }
            try:
                if neo4j_client.driver:
                    concepts = neo4j_client.extract_concepts_from_content(
                        store_request.content
                    )
                    memory_node = neo4j_client.store_memory(
                        memory_id, store_request.content, concepts
                    )
                    observability.record_memory_operation(
                        "neo4j_store", "success", "tier3"
                    )
                    neo4j_info.update(
                        {
                            "memory_node_created": True,
                            "concepts_extracted": len(concepts),
                            "concepts": concepts,
                            "node": memory_node,
                        }
                    )
                else:
                    observability.record_memory_operation(
                        "neo4j_store", "failed", "tier3"
                    )
                    neo4j_info["error"] = "Neo4j driver not initialized"
            except Exception as e:
                # Degrade gracefully if Neo4j fails
                observability.record_memory_operation("neo4j_store", "failed", "tier3")
                neo4j_info["error"] = str(e)

            # Return success with degraded mode info if needed
            operational_mode = (
                "full" if neo4j_info.get("memory_node_created") else "degraded"
            )
            return {
                "success": True,
                "tier": 3,
                "memory_id": memory_id,
                "knowledge_graph": neo4j_info,
                "operational_mode": operational_mode,
                "message": f"Memory stored in {operational_mode} mode",
            }
        except Exception as e:
            observability.record_memory_operation("neo4j_store", "failed", "tier3")
            raise HTTPException(
                status_code=500,
                detail=f"Error storing memory in Neo4j: {str(e)}",
            )
    else:
        raise HTTPException(
            status_code=400,
            detail="Invalid memory tier specified. Use 1, 2, or 3.",
        )


@app.get("/memory/{memory_id}")
async def get_memory(
    memory_id: int,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    """Get a specific memory by ID"""
    try:
        memory = postgres_client.get_memory(memory_id)

        if memory is None:
            raise HTTPException(status_code=404, detail="Memory not found")

        return memory

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving memory: {str(e)}"
        )


@app.post("/memory/query")
async def query_memory(
    query_request: QueryRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Query memories using semantic search and tool discovery.

    Logic:
    1. Generate an embedding for the query text
    2. Perform a semantic search in Qdrant to get relevant memory IDs
    3. Query PostgreSQL for tools that match the query context (Tool Discovery Logic)
    4. Retrieve full memory entries from PostgreSQL
    5. Return a combined response of relevant memories and tools
    """
    try:
        # Step 0: Check for cached query results
        if redis_client.is_connected():
            cached_results = redis_client.get_cached_query_result(
                query_request.query, query_request.top_k
            )
            if cached_results:
                # Return cached results with cache indicator
                return {
                    "query": query_request.query,
                    "top_k": query_request.top_k,
                    "memories": cached_results,
                    "tools": [],  # Tools not cached in this implementation
                    "memory_count": len(cached_results),
                    "tool_count": 0,
                    "cached": True,
                    "message": "Results retrieved from cache",
                }

        # Step 1: Generate an embedding for the query text (with caching)
        cached_embedding = redis_client.get_cached_embedding(query_request.query)
        if cached_embedding:
            query_embedding = cached_embedding
        else:
            query_embedding = qdrant_client.generate_placeholder_embedding(
                query_request.query
            )
            # Cache the generated embedding
            redis_client.cache_embedding(query_request.query, query_embedding)

        # Step 2: Perform a semantic search in Qdrant to get relevant memory IDs
        agent_to_query = query_request.agent_id
        search_results = qdrant_client.search_similar_memories(
            query_embedding=query_embedding,
            top_k=query_request.top_k,
            score_threshold=0.1,  # Configurable threshold
            agent_id=agent_to_query
        )

        # Extract memory IDs from search results
        memory_ids = [
            result["memory_id"] for result in search_results if result["memory_id"]
        ]

        # Step 3: Query PostgreSQL for tools that match the query context
        # (Tool Discovery Logic)
        relevant_tools = postgres_client.get_tools_by_context(
            query_context=query_request.query,
            limit=5,  # Limit tools to keep response manageable
        )

        # Step 4: Retrieve full memory entries from PostgreSQL
        memories = []
        if memory_ids:
            memories = postgres_client.get_memories_by_ids(memory_ids)

            # Enrich memories with similarity scores from Qdrant
            memory_scores = {
                result["memory_id"]: result["score"] for result in search_results
            }
            for memory in memories:
                memory["confidence_score"] = memory_scores.get(memory["id"], 0.0)

            # Sort memories by similarity score (highest first)
            memories.sort(key=lambda x: x.get("confidence_score", 0.0), reverse=True)

        # Step 6: Cache query results in Redis for future requests
        if redis_client.is_connected() and memories:
            redis_client.cache_query_result(
                query_request.query, memories, query_request.top_k
            )

        # Step 7: Return combined response of relevant memories and tools
        response = {
            "query": query_request.query,
            "memories": {"count": len(memories), "results": memories},
            "tools": {"count": len(relevant_tools), "results": relevant_tools},
            "search_metadata": {
                "embedding_search_results": len(search_results),
                "memory_ids_found": memory_ids,
                "top_k_requested": query_request.top_k,
            },
            "cached": False,
        }

        return response

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error querying memory: {str(e)}")


from app.schemas import MCPTier, MCP_TIER_MAPPING, KnowledgeShareRequest, KnowledgeShareOffer

# Knowledge Sharing Endpoints
@app.post("/memory/share/request")
async def request_knowledge(
    share_request: KnowledgeShareRequest,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    observability: ObservabilityService = Depends(get_observability),
):
    request_id = postgres_client.create_knowledge_share_request(
        requester_agent_id=share_request.agent_id, # This needs to be passed in the request
        target_agent_id=share_request.target_agent,
        query=share_request.query,
        confidence_threshold=share_request.confidence_threshold,
        sharing_policy=share_request.sharing_policy,
    )
    if request_id is None:
        raise HTTPException(status_code=500, detail="Failed to create knowledge share request")
    
    observability.log_structured(
        "info",
        "Knowledge share request created",
        request_id=request_id,
        requester_agent_id=share_request.agent_id,
        target_agent_id=share_request.target_agent,
        query=share_request.query,
    )

    return {"message": "Knowledge share request created successfully", "request_id": request_id}

@app.post("/memory/share/offer")
async def offer_knowledge(
    offer_request: KnowledgeShareOffer,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    observability: ObservabilityService = Depends(get_observability),
):
    request = postgres_client.get_knowledge_share_request_by_id(offer_request.request_id)
    if not request:
        raise HTTPException(status_code=404, detail="Knowledge share request not found")

    if request["sharing_policy"] == "high_confidence_only":
        if offer_request.confidence_score < request["confidence_threshold"]:
            raise HTTPException(status_code=400, detail=f"Confidence score {offer_request.confidence_score} is below the threshold {request['confidence_threshold']}")

    offer_id = postgres_client.create_knowledge_share_offer(
        request_id=offer_request.request_id,
        offering_agent_id=offer_request.offering_agent_id,
        memory_id=offer_request.memory_id,
        confidence_score=offer_request.confidence_score,
    )
    if offer_id is None:
        raise HTTPException(status_code=500, detail="Failed to create knowledge share offer")

    observability.log_structured(
        "info",
        "Knowledge share offer created",
        offer_id=offer_id,
        request_id=offer_request.request_id,
        offering_agent_id=offer_request.offering_agent_id,
        memory_id=offer_request.memory_id,
        confidence_score=offer_request.confidence_score,
    )

    return {"message": "Knowledge share offer created successfully", "offer_id": offer_id}

@app.get("/memory/share/pending")
async def get_pending_shares(
    agent_id: str,
    postgres_client: PostgresClient = Depends(get_postgres_client),
):
    requests = postgres_client.get_pending_knowledge_share_requests(agent_id)
    return {"pending_requests": requests}


@app.get("/memory/search")
async def search_memories(
    query: str,
    top_k: int = 5,
    postgres_client: PostgresClient = Depends(get_postgres_client),
    qdrant_client: QdrantMemoryClient = Depends(get_qdrant_client),
):
    """
    Simple memory search endpoint (alternative to POST /memory/query)
    """
    try:
        # Create QueryRequest object and use the main query logic
        query_request = QueryRequest(query=query, top_k=top_k)
        return await query_memory(query_request, postgres_client, qdrant_client)

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error searching memories: {str(e)}"
        )


# Graph Query Endpoint
@app.post("/graph/query")
async def query_graph(
    query_request: GraphQueryRequest,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
):
    """
    Query the Neo4j knowledge graph.
    """
    try:
        # Build the Cypher query
        query = f"MATCH (n:{query_request.node_label})"
        if query_request.filters:
            query += " WHERE "
            query += " AND ".join(
                [f"n.{key} = ${key}" for key in query_request.filters.keys()]
            )

        if query_request.return_properties:
            query += f" RETURN n.{', n.'.join(query_request.return_properties)}"
        else:
            query += " RETURN n"

        # Execute the query
        result = neo4j_client.run_cypher_query(query, query_request.filters)

        return {"result": result}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error querying graph: {str(e)}")


@app.get("/graph/related")
async def get_related(
    node_id: str, neo4j_client: Neo4jClient = Depends(get_neo4j_client)
):
    """Get all directly connected nodes and their relationships."""
    try:
        result = neo4j_client.get_related_nodes(node_id)
        return {"result": result}
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error getting related nodes: {str(e)}"
        )


@app.get("/graph/shortest-path")
async def get_shortest_path(
    start_node_id: str,
    end_node_id: str,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
):
    """Calculate and return the shortest path between two nodes."""
    try:
        result = neo4j_client.get_shortest_path(start_node_id, end_node_id)
        return {"result": result}
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error getting shortest path: {str(e)}"
        )


@app.get("/graph/subgraph")
async def get_subgraph(
    node_id: str,
    depth: int = 1,
    neo4j_client: Neo4jClient = Depends(get_neo4j_client),
):
    """Get the subgraph surrounding a central node."""
    try:
        result = neo4j_client.get_subgraph(node_id, depth)
        return {"result": result}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting subgraph: {str(e)}")


# === LLM Cache Endpoints ===


@app.post("/llm/cache")
async def cache_llm_response(
    request: LLMCacheRequest,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Cache an LLM response for future retrieval.
    This helps reduce API costs and improve response times.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        success = redis_client.cache_llm_response(
            model=request.model,
            prompt=request.prompt,
            response="",  # Will be set by the actual LLM call
            temperature=request.temperature,
            max_tokens=request.max_tokens,
            metadata=request.metadata,
        )

        if success:
            return {
                "message": "LLM response cached successfully",
                "model": request.model,
                "cached": True,
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to cache LLM response")

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error caching LLM response: {str(e)}"
        )


@app.get("/llm/cache")
async def get_cached_llm_response(
    model: str,
    prompt: str,
    temperature: float = 0.7,
    max_tokens: int = 1000,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Retrieve a cached LLM response if available.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        cached_response = redis_client.get_cached_llm_response(
            model=model,
            prompt=prompt,
            temperature=temperature,
            max_tokens=max_tokens,
        )

        if cached_response:
            return {
                "cached": True,
                "response": cached_response,
                "message": "Cached response retrieved successfully",
            }
        else:
            return {
                "cached": False,
                "response": None,
                "message": "No cached response found",
            }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving cached LLM response: {str(e)}"
        )


@app.post("/llm/usage")
async def track_llm_usage(
    request: LLMUsageRequest,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Track LLM token usage for cost monitoring and analytics.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        success = redis_client.track_llm_usage(
            model=request.model,
            prompt_tokens=request.prompt_tokens,
            completion_tokens=request.completion_tokens,
            total_tokens=request.total_tokens,
            request_id=request.request_id,
        )

        if success:
            return {
                "message": "LLM usage tracked successfully",
                "model": request.model,
                "tokens": request.total_tokens,
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to track LLM usage")

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error tracking LLM usage: {str(e)}"
        )


@app.get("/llm/usage/stats")
async def get_llm_usage_stats(
    model: Optional[str] = None,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Get LLM usage statistics for monitoring and cost analysis.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        stats = redis_client.get_llm_usage_stats(model)

        return {
            "stats": stats,
            "message": "LLM usage statistics retrieved successfully",
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving LLM usage stats: {str(e)}"
        )


@app.post("/llm/performance")
async def track_llm_performance(
    request: LLMPerformanceRequest,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Track LLM model performance metrics.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        success = redis_client.cache_model_performance(
            model=request.model,
            operation=request.operation,
            response_time=request.response_time,
            success=request.success,
            error_message=request.error_message,
        )

        if success:
            return {
                "message": "LLM performance tracked successfully",
                "model": request.model,
                "operation": request.operation,
            }
        else:
            raise HTTPException(
                status_code=500, detail="Failed to track LLM performance"
            )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error tracking LLM performance: {str(e)}"
        )


@app.get("/llm/performance/stats")
async def get_llm_performance_stats(
    model: str,
    operation: str,
    redis_client: RedisClient = Depends(get_redis_client),
):
    """
    Get LLM model performance statistics.
    """
    try:
        if not redis_client.is_connected():
            raise HTTPException(
                status_code=503, detail="Redis cache service unavailable"
            )

        stats = redis_client.get_model_performance(model, operation)

        if "error" in stats:
            raise HTTPException(status_code=404, detail=stats["error"])

        return {
            "stats": stats,
            "message": "LLM performance statistics retrieved successfully",
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error retrieving LLM performance stats: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn

    port = int(os.environ.get("PORT", 8090))
    uvicorn.run(app, host="0.0.0.0", port=port)

]]></file>
  <file name="mcp_server.py" path="memos.as/app/mcp_server.py"><![CDATA[
#!/usr/bin/env python3
"""
MCP Server for memOS.as - Memory Operations
Provides MCP tools for storing, retrieving, and managing
memories in the memOS system.
"""

import logging
import os
import json
from datetime import datetime, timedelta
from typing import Any, Dict, Optional
from collections import defaultdict
import jwt
import httpx
from mcp.server import Server
import uvicorn
from fastapi import FastAPI, Depends, HTTPException, status, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel

from app.services.observability import get_observability

# Try to import Langfuse
try:
    from langfuse import Langfuse
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    Langfuse = None

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configure audit logging
audit_logger = logging.getLogger("mcp_audit")
audit_logger.setLevel(logging.INFO)
audit_handler = logging.StreamHandler()
audit_formatter = logging.Formatter(
    json.dumps({
        "timestamp": "%(asctime)s",
        "level": "%(levelname)s",
        "service": "memOS-MCP",
        "event": "%(message)s"
    })
)
audit_handler.setFormatter(audit_formatter)
audit_logger.addHandler(audit_handler)

# JWT Configuration
JWT_SECRET_KEY = os.getenv("JWT_SECRET_KEY", "apexsigma-mcp-secret-key-2025")
JWT_ALGORITHM = "HS256"
JWT_ACCESS_TOKEN_EXPIRE_MINUTES = 30

# Service accounts for AI assistants
SERVICE_ACCOUNTS = {
    "MCP_COPILOT": "copilot-secret-token",
    "MCP_GEMINI": "gemini-secret-token",
    "MCP_QWEN": "qwen-secret-token"
}

# MCP-specific memory tier mapping
MCP_MEMORY_TIERS = {
    "MCP_COPILOT": "MCP_COPILOT",
    "MCP_GEMINI": "MCP_GEMINI",
    "MCP_QWEN": "MCP_QWEN",
    "MCP_SYSTEM": "MCP_SYSTEM"
}


def get_mcp_memory_tier(service_account: str) -> str:
    """
    Map service account to MCP-specific memory tier.

    Args:
        service_account: The service account name

    Returns:
        MCP-specific memory tier name
    """
    return MCP_MEMORY_TIERS.get(service_account, "MCP_SYSTEM")

# Initialize Langfuse client for MCP-specific tracing
langfuse_client = None
if LANGFUSE_AVAILABLE:
    try:
        public_key = os.environ.get("LANGFUSE_PUBLIC_KEY")
        secret_key = os.environ.get("LANGFUSE_SECRET_KEY")
        host = os.environ.get("LANGFUSE_HOST", "https://cloud.langfuse.com")

        if public_key and secret_key:
            langfuse_client = Langfuse(
                public_key=public_key,
                secret_key=secret_key,
                host=host
            )
            logger.info("Langfuse client initialized for MCP tracing")
        else:
            logger.warning("Langfuse API keys not found, MCP tracing disabled")
    except Exception as e:
        logger.error(f"Failed to initialize Langfuse client: {e}")
        langfuse_client = None

# Rate limiting configuration (requests per minute)
RATE_LIMITS = {
    "MCP_COPILOT": 60,  # 60 requests per minute
    "MCP_GEMINI": 60,
    "MCP_QWEN": 60
}

# In-memory rate limiting storage
rate_limit_store: Dict[str, Dict[str, Any]] = defaultdict(lambda: {"count": 0, "reset_time": datetime.utcnow()})# MCP Server setup
server = Server("memos-mcp-server")

# FastAPI app for MCP over HTTP
app = FastAPI(
    title="memOS MCP Server",
    description="MCP server for memory operations"
)

# Initialize observability service
observability = get_observability()

# Security scheme for JWT authentication
security = HTTPBearer()

# memOS API base URL (should be configurable)
MEMOS_BASE_URL = os.getenv("MEMOS_BASE_URL", "http://memos-api:8090")


def create_mcp_trace(name: str, service_account: str, metadata: Optional[Dict[str, Any]] = None):
    """Create a Langfuse trace for MCP operations."""
    if not langfuse_client:
        return None

    try:
        trace = langfuse_client.trace(
            name=name,
            user_id=service_account,
            metadata={
                "service": "memOS-MCP",
                "service_account": service_account,
                "timestamp": datetime.utcnow().isoformat(),
                **(metadata or {})
            }
        )
        return trace
    except Exception as e:
        logger.error(f"Failed to create Langfuse trace: {e}")
        return None


def create_mcp_span(trace, name: str, input_data: Optional[Dict[str, Any]] = None):
    """Create a span within an MCP trace."""
    if not trace:
        return None

    try:
        span = trace.span(
            name=name,
            input=input_data
        )
        return span
    except Exception as e:
        logger.error(f"Failed to create Langfuse span: {e}")
        return None


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """Create JWT access token"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=JWT_ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, JWT_SECRET_KEY, algorithm=JWT_ALGORITHM)
    return encoded_jwt


def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verify JWT token"""
    try:
        payload = jwt.decode(credentials.credentials, JWT_SECRET_KEY, algorithms=[JWT_ALGORITHM])
        service_account: str = payload.get("sub")
        if service_account not in SERVICE_ACCOUNTS:
            log_auth_attempt(service_account or "unknown", False, {"reason": "invalid_service_account"})
            observability.record_mcp_auth_attempt(service_account or "unknown", False)
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid service account",
                headers={"WWW-Authenticate": "Bearer"},
            )
        log_auth_attempt(service_account, True)
        observability.record_mcp_auth_attempt(service_account, True)
        return service_account
    except jwt.PyJWTError as e:
        log_auth_attempt("unknown", False, {"reason": "invalid_token", "error": str(e)})
        observability.record_mcp_auth_attempt("unknown", False)
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid authentication credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )


def check_rate_limit(service_account: str):
    """Check and enforce rate limiting for service account"""
    now = datetime.utcnow()
    user_data = rate_limit_store[service_account]
    
    # Reset counter if time window has passed
    if now >= user_data["reset_time"]:
        user_data["count"] = 0
        user_data["reset_time"] = now + timedelta(minutes=1)
    
    # Check if limit exceeded
    if user_data["count"] >= RATE_LIMITS.get(service_account, 60):
        reset_time = user_data["reset_time"]
        log_rate_limit_violation(service_account, {
            "current_count": user_data["count"],
            "limit": RATE_LIMITS.get(service_account, 60),
            "reset_time": reset_time.isoformat()
        })
        observability.record_mcp_rate_limit_hit(service_account)
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail=f"Rate limit exceeded. Try again after {reset_time.isoformat()}",
            headers={"Retry-After": str(int((reset_time - now).total_seconds()))}
        )
    
    # Increment counter
    user_data["count"] += 1


def verify_token_and_rate_limit(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verify JWT token and check rate limits with Langfuse tracing"""
    trace = create_mcp_trace("mcp_authentication", "system", {"operation": "token_verification"})
    span = create_mcp_span(trace, "verify_token_and_rate_limit")

    try:
        service_account = verify_token(credentials)
        check_rate_limit(service_account)

        if span:
            span.end(output={"service_account": service_account, "status": "success"})

        return service_account
    except Exception as e:
        if span:
            span.end(output={"error": str(e), "status": "failed"})
        raise


def log_audit_event(event_type: str, service_account: Optional[str] = None,
                   details: Optional[Dict[str, Any]] = None, success: bool = True):
    """Log audit events for security monitoring"""
    audit_data = {
        "event_type": event_type,
        "service_account": service_account,
        "timestamp": datetime.utcnow().isoformat(),
        "success": success,
        "details": details or {}
    }
    audit_logger.info(json.dumps(audit_data))

    # Record audit event metrics
    severity = "error" if not success else "info"
    observability.record_mcp_audit_event(event_type, service_account or "unknown", severity)


def log_auth_attempt(service_account: str, success: bool, details: Optional[Dict[str, Any]] = None):
    """Log authentication attempts"""
    log_audit_event("authentication", service_account, details, success)


def log_rate_limit_violation(service_account: str, details: Optional[Dict[str, Any]] = None):
    """Log rate limit violations"""
    log_audit_event("rate_limit_violation", service_account, details, False)


def log_mcp_request(service_account: str, method: str, details: Optional[Dict[str, Any]] = None):
    """Log MCP requests"""
    log_audit_event("mcp_request", service_account, details, True)


@app.post("/auth/token")
async def get_access_token(service_account: str, secret: str):
    """Get access token for service account"""
    if service_account not in SERVICE_ACCOUNTS:
        log_auth_attempt(service_account, False, {"reason": "invalid_service_account"})
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid service account"
        )
    
    if secret != SERVICE_ACCOUNTS[service_account]:
        log_auth_attempt(service_account, False, {"reason": "invalid_secret"})
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid secret"
        )
    
    log_auth_attempt(service_account, True, {"action": "token_generated"})
    access_token = create_access_token(data={"sub": service_account})
    return {"access_token": access_token, "token_type": "bearer"}


class StoreMemoryRequest(BaseModel):
    content: str
    metadata: Optional[Dict[str, Any]] = None


class QueryMemoryRequest(BaseModel):
    query: str
    top_k: Optional[int] = 5


# MCP Tools - Define functions that will be registered as tools
@server.tool()
async def store_memory_tool(content: str, metadata: Optional[str] = None) -> str:
    """
    Store a memory in the memOS system using MCP-specific tiers.

    Args:
        content: The memory content to store
        metadata: Optional JSON metadata as string

    Returns:
        Success message with stored memory details
    """
    # Get service account from request context
    service_account = request_context.get("service_account", "MCP_SYSTEM")

    # Map service account to MCP-specific memory tier
    mcp_tier = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("store_memory", service_account, {
        "operation": "memory_storage",
        "mcp_tier": mcp_tier
    })
    span = create_mcp_span(trace, "store_memory_operation", {
        "content_length": len(content),
        "tier": mcp_tier
    })

    try:
        parsed_metadata = None
        if metadata:
            parsed_metadata = json.loads(metadata)

        # Add MCP-specific metadata
        if parsed_metadata is None:
            parsed_metadata = {}
        parsed_metadata.update({
            "mcp_service_account": service_account,
            "mcp_tier": mcp_tier,
            "stored_by": "mcp_server"
        })

        request_data = StoreMemoryRequest(
            content=content,
            metadata=parsed_metadata,
            tier=mcp_tier  # Use MCP-specific tier
        )

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/store",
                json=request_data.dict(),
                timeout=30.0
            )
            response.raise_for_status()
            result = response.json()

        if span:
            span.end(output={"status": "success", "result": result, "tier": mcp_tier})

        log_mcp_request(service_account, "store_memory", {
            "content_length": len(content),
            "tier": mcp_tier
        })
        observability.record_mcp_tool_usage("store_memory", service_account, True)
        observability.record_mcp_memory_operation("store", mcp_tier, service_account, True)

        return f"Memory stored successfully in MCP tier '{mcp_tier}': {result}"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "tier": mcp_tier})
        logger.error("Error storing memory: %s", e)
        observability.record_mcp_tool_usage("store_memory", service_account, False)
        observability.record_mcp_memory_operation("store", mcp_tier, service_account, False)
        return f"Error storing memory in MCP tier '{mcp_tier}': {str(e)}"


@server.tool()
async def query_memory_by_mcp_tier_tool(query: str, top_k: int = 5) -> str:
    """
    Query memories from the current MCP service account's tier.

    This tool searches only memories stored by the same service account,
    providing agent-specific memory isolation.

    Args:
        query: The search query
        top_k: Number of top results to return

    Returns:
        Search results from the current service account's memory tier
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    mcp_tier = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("query_memory_by_mcp_tier", service_account, {
        "operation": "memory_query",
        "mcp_tier": mcp_tier,
        "query_length": len(query)
    })
    span = create_mcp_span(trace, "mcp_tier_query_operation", {
        "query": query,
        "top_k": top_k,
        "tier": mcp_tier
    })

    try:
        # Create query request with MCP tier filter
        query_request = {
            "query": query,
            "top_k": top_k,
            "filters": {
                "tier": mcp_tier  # Filter by MCP-specific tier
            }
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/query",
                json=query_request,
                timeout=30.0
            )
            response.raise_for_status()
            results = response.json()

        # Format results with MCP tier information
        formatted_results = []
        memories = results.get("memories", {}).get("results", [])

        if memories:
            formatted_results.append(f"Found {len(memories)} memories in MCP tier '{mcp_tier}':")
            formatted_results.append("")

            for i, result in enumerate(memories):
                formatted_results.append(f"{i+1}. {result.get('content', 'No content')}")
                if result.get('metadata'):
                    metadata = result['metadata']
                    if 'mcp_service_account' in metadata:
                        formatted_results.append(f"   Service Account: {metadata['mcp_service_account']}")
                    if 'stored_by' in metadata:
                        formatted_results.append(f"   Stored by: {metadata['stored_by']}")
                if result.get('similarity_score'):
                    formatted_results.append(f"   Similarity: {result['similarity_score']:.3f}")
                formatted_results.append("")

            if span:
                span.end(output={"status": "success", "results_count": len(memories), "tier": mcp_tier})

            log_mcp_request(service_account, "query_memory_by_mcp_tier", {
                "query_length": len(query),
                "results_count": len(memories),
                "tier": mcp_tier
            })
            observability.record_mcp_tool_usage("query_memory_by_mcp_tier", service_account, True)

            return "\n".join(formatted_results)
        else:
            if span:
                span.end(output={"status": "success", "results_count": 0, "tier": mcp_tier})

            observability.record_mcp_tool_usage("query_memory_by_mcp_tier", service_account, True)
            return f"No memories found in MCP tier '{mcp_tier}' for query: {query}"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "tier": mcp_tier})
        logger.error(f"Error querying memory by MCP tier: {e}")
        observability.record_mcp_tool_usage("query_memory_by_mcp_tier", service_account, False)
        return f"Error querying memories in MCP tier '{mcp_tier}': {str(e)}"


@server.tool()
async def get_mcp_memory_stats_tool() -> str:
    """
    Get memory statistics for the current MCP service account's tier.

    Returns:
        Memory statistics specific to the current service account
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    mcp_tier = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("get_mcp_memory_stats", service_account, {
        "operation": "memory_stats",
        "mcp_tier": mcp_tier
    })

    try:
        # Query memories by MCP tier
        query_request = {
            "query": "*",  # Match all memories
            "top_k": 1000,  # Get a large sample for stats
            "filters": {
                "tier": mcp_tier
            }
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/query",
                json=query_request,
                timeout=30.0
            )
            response.raise_for_status()
            results = response.json()

        memories = results.get("memories", {}).get("results", [])
        memory_count = len(memories)

        # Calculate statistics
        total_content_length = sum(len(m.get("content", "")) for m in memories)
        avg_content_length = total_content_length / memory_count if memory_count > 0 else 0

        # Count memories by service account
        service_account_counts = {}
        for memory in memories:
            metadata = memory.get("metadata", {})
            sa = metadata.get("mcp_service_account", "unknown")
            service_account_counts[sa] = service_account_counts.get(sa, 0) + 1

        stats = {
            "mcp_tier": mcp_tier,
            "total_memories": memory_count,
            "average_content_length": avg_content_length,
            "service_account_breakdown": service_account_counts,
            "query_timestamp": datetime.utcnow().isoformat()
        }

        if trace:
            trace.update(metadata={"stats": stats})

        log_mcp_request(service_account, "get_mcp_memory_stats", {
            "tier": mcp_tier,
            "memory_count": memory_count
        })
        observability.record_mcp_tool_usage("get_mcp_memory_stats", service_account, True)

        return f"""MCP Memory Statistics for tier '{mcp_tier}':

Total Memories: {stats['total_memories']}
Average Content Length: {stats['average_content_length']:.1f} characters

Service Account Breakdown:
{chr(10).join(f"  {sa}: {count} memories" for sa, count in stats['service_account_breakdown'].items())}

Query Time: {stats['query_timestamp']}"""

    except Exception as e:
        if trace:
            trace.update(metadata={"error": str(e)})
        logger.error(f"Error getting MCP memory stats: {e}")
        observability.record_mcp_tool_usage("get_mcp_memory_stats", service_account, False)
        return f"Error getting memory statistics for MCP tier '{mcp_tier}': {str(e)}"


# Remove the old tool definitions that use decorators
# @server.tool()
# async def store_memory(content: str, metadata: Optional[str] = None) -> str:
    """
    Store a memory in the memOS system using MCP-specific tiers.

    Args:
        content: The memory content to store
        metadata: Optional JSON metadata as string

    Returns:
        Success message with stored memory details
    """
    # Get service account from request context
    service_account = request_context.get("service_account", "MCP_SYSTEM")

    # Map service account to MCP-specific memory tier
    mcp_tier = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("store_memory", service_account, {
        "operation": "memory_storage",
        "mcp_tier": mcp_tier
    })
    span = create_mcp_span(trace, "store_memory_operation", {
        "content_length": len(content),
        "tier": mcp_tier
    })

    try:
        parsed_metadata = None
        if metadata:
            parsed_metadata = json.loads(metadata)

        # Add MCP-specific metadata
        if parsed_metadata is None:
            parsed_metadata = {}
        parsed_metadata.update({
            "mcp_service_account": service_account,
            "mcp_tier": mcp_tier,
            "stored_by": "mcp_server"
        })

        request_data = StoreMemoryRequest(
            content=content,
            metadata=parsed_metadata,
            tier=mcp_tier  # Use MCP-specific tier
        )

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/store",
                json=request_data.dict(),
                timeout=30.0
            )
            response.raise_for_status()
            result = response.json()

        if span:
            span.end(output={"status": "success", "result": result, "tier": mcp_tier})

        log_mcp_request(service_account, "store_memory", {
            "content_length": len(content),
            "tier": mcp_tier
        })
        observability.record_mcp_tool_usage("store_memory", service_account, True)
        observability.record_mcp_memory_operation("store", mcp_tier, service_account, True)

        return f"Memory stored successfully in MCP tier '{mcp_tier}': {result}"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "tier": mcp_tier})
        logger.error("Error storing memory: %s", e)
        observability.record_mcp_tool_usage("store_memory", service_account, False)
        observability.record_mcp_memory_operation("store", mcp_tier, service_account, False)
        return f"Error storing memory in MCP tier '{mcp_tier}': {str(e)}"


# Remove the old tool definitions that use decorators
# @server.tool()
# async def query_memory_by_mcp_tier(query: str, top_k: int = 5) -> str:
    """
    Query memories from the current MCP service account's tier.

    This tool searches only memories stored by the same service account,
    providing agent-specific memory isolation.

    Args:
        query: The search query
        top_k: Number of top results to return

    Returns:
        Search results from the current service account's memory tier
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    mcp_tier = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("query_memory_by_mcp_tier", service_account, {
        "operation": "memory_query",
        "mcp_tier": mcp_tier,
        "query_length": len(query)
    })
    span = create_mcp_span(trace, "mcp_tier_query_operation", {
        "query": query,
        "top_k": top_k,
        "tier": mcp_tier
    })

    try:
        # Create query request with MCP tier filter
        query_request = {
            "query": query,
            "top_k": top_k,
            "filters": {
                "tier": mcp_tier  # Filter by MCP-specific tier
            }
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/query",
                json=query_request,
                timeout=30.0
            )
            response.raise_for_status()
            results = response.json()

        # Format results with MCP tier information
        formatted_results = []
        memories = results.get("memories", {}).get("results", [])

        if memories:
            formatted_results.append(f"Found {len(memories)} memories in MCP tier '{mcp_tier}':")
            formatted_results.append("")

            for i, result in enumerate(memories):
                formatted_results.append(f"{i+1}. {result.get('content', 'No content')}")
                if result.get('metadata'):
                    metadata = result['metadata']
                    if 'mcp_service_account' in metadata:
                        formatted_results.append(f"   Service Account: {metadata['mcp_service_account']}")
                    if 'stored_by' in metadata:
                        formatted_results.append(f"   Stored by: {metadata['stored_by']}")
                if result.get('similarity_score'):
                    formatted_results.append(f"   Similarity: {result['similarity_score']:.3f}")
                formatted_results.append("")

            if span:
                span.end(output={"status": "success", "results_count": len(memories), "tier": mcp_tier})

            log_mcp_request(service_account, "query_memory_by_mcp_tier", {
                "query_length": len(query),
                "results_count": len(memories),
                "tier": mcp_tier
            })
            observability.record_mcp_tool_usage("query_memory_by_mcp_tier", service_account, True)

            return "\n".join(formatted_results)
        else:
            if span:
                span.end(output={"status": "success", "results_count": 0, "tier": mcp_tier})

            observability.record_mcp_tool_usage("query_memory_by_mcp_tier", service_account, True)
            return f"No memories found in MCP tier '{mcp_tier}' for query: {query}"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "tier": mcp_tier})
        logger.error(f"Error querying memory by MCP tier: {e}")
        observability.record_mcp_tool_usage("query_memory_by_mcp_tier", service_account, False)
        return f"Error querying memories in MCP tier '{mcp_tier}': {str(e)}"


# Remove the old tool definitions that use decorators
# @server.tool()
# async def get_mcp_memory_stats() -> str:
    """
    Get memory statistics for the current MCP service account's tier.

    Returns:
        Memory statistics specific to the current service account
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    mcp_tier = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("get_mcp_memory_stats", service_account, {
        "operation": "memory_stats",
        "mcp_tier": mcp_tier
    })

    try:
        # Query memories by MCP tier
        query_request = {
            "query": "*",  # Match all memories
            "top_k": 1000,  # Get a large sample for stats
            "filters": {
                "tier": mcp_tier
            }
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/query",
                json=query_request,
                timeout=30.0
            )
            response.raise_for_status()
            results = response.json()

        memories = results.get("memories", {}).get("results", [])
        memory_count = len(memories)

        # Calculate statistics
        total_content_length = sum(len(m.get("content", "")) for m in memories)
        avg_content_length = total_content_length / memory_count if memory_count > 0 else 0

        # Count memories by service account
        service_account_counts = {}
        for memory in memories:
            metadata = memory.get("metadata", {})
            sa = metadata.get("mcp_service_account", "unknown")
            service_account_counts[sa] = service_account_counts.get(sa, 0) + 1

        stats = {
            "mcp_tier": mcp_tier,
            "total_memories": memory_count,
            "average_content_length": avg_content_length,
            "service_account_breakdown": service_account_counts,
            "query_timestamp": datetime.utcnow().isoformat()
        }

        if trace:
            trace.update(metadata={"stats": stats})

        log_mcp_request(service_account, "get_mcp_memory_stats", {
            "tier": mcp_tier,
            "memory_count": memory_count
        })
        observability.record_mcp_tool_usage("get_mcp_memory_stats", service_account, True)

        return f"""MCP Memory Statistics for tier '{mcp_tier}':

Total Memories: {stats['total_memories']}
Average Content Length: {stats['average_content_length']:.1f} characters

Service Account Breakdown:
{chr(10).join(f"  {sa}: {count} memories" for sa, count in stats['service_account_breakdown'].items())}

Query Time: {stats['query_timestamp']}"""

    except Exception as e:
        if trace:
            trace.update(metadata={"error": str(e)})
        logger.error(f"Error getting MCP memory stats: {e}")
        observability.record_mcp_tool_usage("get_mcp_memory_stats", service_account, False)
        return f"Error getting memory statistics for MCP tier '{mcp_tier}': {str(e)}"


# Remove the old tool definitions that use decorators
# @server.tool()
# async def clear_memory_cache(pattern: str = "*") -> str:
    """
    Clear memory cache entries.

    Args:
        pattern: Pattern to match for cache clearing (default: all)

    Returns:
        Cache clearing result
    """
    try:
        async with httpx.AsyncClient() as client:
            response = await client.delete(
                f"{MEMOS_BASE_URL}/cache/clear",
                params={"pattern": pattern},
                timeout=30.0
            )
            response.raise_for_status()
            result = response.json()

        return f"Cache cleared: {result}"

    except Exception as e:
        logger.error(f"Error clearing cache: {e}")
        return f"Error clearing cache: {str(e)}"


@app.get("/metrics")
async def get_mcp_metrics():
    """Prometheus metrics endpoint for MCP server"""
    from fastapi.responses import PlainTextResponse
    return PlainTextResponse(observability.get_metrics(), media_type="text/plain")


# Context variable to store service account during request processing
request_context = {"service_account": "MCP_SYSTEM"}


@app.post("/mcp")
async def handle_mcp_request(request: Dict[str, Any], service_account: str = Depends(verify_token_and_rate_limit)):
    """Handle MCP requests - Protected by JWT authentication and rate limiting"""
    # Set service account in context for MCP tools to access
    request_context["service_account"] = service_account

    # Record active connection
    observability.update_mcp_active_connections(service_account, 1)

    # Create MCP-specific trace for the entire request
    trace = create_mcp_trace("mcp_request", service_account, {
        "request_type": request.get("type", "unknown"),
        "method": request.get("method", "unknown")
    })

    start_time = datetime.utcnow()

    try:
        # Log the MCP request
        log_mcp_request(service_account, "mcp_request", {
            "request_type": request.get("type", "unknown"),
            "method": request.get("method", "unknown")
        })

        logger.info("MCP request from service account: %s", service_account)
        result = await server.handle_request(request)

        # Record successful request metrics
        duration = (datetime.utcnow() - start_time).total_seconds()
        observability.record_mcp_request(
            method=request.get("method", "unknown"),
            endpoint="/mcp",
            service_account=service_account,
            status_code=200,
            duration=duration
        )

        if trace:
            trace.update(metadata={"status": "success"})

        return result
    except Exception as e:
        # Record failed request metrics
        duration = (datetime.utcnow() - start_time).total_seconds()
        observability.record_mcp_request(
            method=request.get("method", "unknown"),
            endpoint="/mcp",
            service_account=service_account,
            status_code=500,
            duration=duration
        )

        if trace:
            trace.update(metadata={"status": "error", "error": str(e)})
        raise
    finally:
        # Reset active connection
        observability.update_mcp_active_connections(service_account, 0)


@server.tool()
async def request_knowledge_from_agent(target_agent_id: str, query: str, confidence_threshold: float = 0.8, sharing_policy: str = "high_confidence_only") -> str:
    """
    Request knowledge from another agent via cross-agent knowledge sharing.

    This tool allows the current agent to request specific knowledge or information
    from another agent in the ecosystem, with confidence-based filtering.

    Args:
        target_agent_id: The ID of the agent to request knowledge from
        query: The specific knowledge or information being requested
        confidence_threshold: Minimum confidence score required (0.0-1.0)
        sharing_policy: Knowledge sharing policy ("high_confidence_only", "all_confidence", "manual_review")

    Returns:
        Success message with request details or error message
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    requester_agent_id = get_mcp_memory_tier(service_account)  # Map service account to agent ID

    trace = create_mcp_trace("request_knowledge_from_agent", service_account, {
        "operation": "knowledge_request",
        "target_agent": target_agent_id,
        "query_length": len(query),
        "confidence_threshold": confidence_threshold
    })
    span = create_mcp_span(trace, "knowledge_request_operation", {
        "requester": requester_agent_id,
        "target": target_agent_id,
        "query": query,
        "threshold": confidence_threshold
    })

    try:
        # Create knowledge share request
        request_data = {
            "agent_id": requester_agent_id,
            "target_agent": target_agent_id,
            "query": query,
            "confidence_threshold": confidence_threshold,
            "sharing_policy": sharing_policy
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/share/request",
                json=request_data,
                timeout=30.0
            )
            response.raise_for_status()
            result = response.json()

        if span:
            span.end(output={"status": "success", "request_id": result.get("request_id"), "target_agent": target_agent_id})

        log_mcp_request(service_account, "request_knowledge_from_agent", {
            "target_agent": target_agent_id,
            "query_length": len(query),
            "confidence_threshold": confidence_threshold
        })
        observability.record_mcp_tool_usage("request_knowledge_from_agent", service_account, True)

        return f"Knowledge request sent successfully to agent '{target_agent_id}': Request ID {result.get('request_id', 'unknown')}"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "target_agent": target_agent_id})
        logger.error(f"Error requesting knowledge from agent {target_agent_id}: {e}")
        observability.record_mcp_tool_usage("request_knowledge_from_agent", service_account, False)
        return f"Error requesting knowledge from agent '{target_agent_id}': {str(e)}"


@server.tool()
async def offer_knowledge_to_request(request_id: int, memory_id: int, confidence_score: float) -> str:
    """
    Offer knowledge in response to a knowledge sharing request.

    This tool allows an agent to offer specific knowledge/memory in response
    to a pending knowledge sharing request, with an associated confidence score.

    Args:
        request_id: The ID of the knowledge sharing request
        memory_id: The ID of the memory/knowledge being offered
        confidence_score: Confidence score for this knowledge offer (0.0-1.0)

    Returns:
        Success message with offer details or error message
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    offering_agent_id = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("offer_knowledge_to_request", service_account, {
        "operation": "knowledge_offer",
        "request_id": request_id,
        "memory_id": memory_id,
        "confidence_score": confidence_score
    })
    span = create_mcp_span(trace, "knowledge_offer_operation", {
        "offering_agent": offering_agent_id,
        "request_id": request_id,
        "memory_id": memory_id,
        "confidence": confidence_score
    })

    try:
        # Create knowledge share offer
        offer_data = {
            "request_id": request_id,
            "offering_agent_id": offering_agent_id,
            "memory_id": memory_id,
            "confidence_score": confidence_score
        }

        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{MEMOS_BASE_URL}/memory/share/offer",
                json=offer_data,
                timeout=30.0
            )
            response.raise_for_status()
            result = response.json()

        if span:
            span.end(output={"status": "success", "offer_id": result.get("offer_id"), "request_id": request_id})

        log_mcp_request(service_account, "offer_knowledge_to_request", {
            "request_id": request_id,
            "memory_id": memory_id,
            "confidence_score": confidence_score
        })
        observability.record_mcp_tool_usage("offer_knowledge_to_request", service_account, True)

        return f"Knowledge offer submitted successfully: Offer ID {result.get('offer_id', 'unknown')} for request {request_id}"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "request_id": request_id})
        logger.error(f"Error offering knowledge for request {request_id}: {e}")
        observability.record_mcp_tool_usage("offer_knowledge_to_request", service_account, False)
        return f"Error offering knowledge for request {request_id}: {str(e)}"


@server.tool()
async def get_pending_knowledge_requests() -> str:
    """
    Get all pending knowledge sharing requests for the current agent.

    This tool retrieves all knowledge sharing requests that have been made
    to the current agent and are still pending response.

    Returns:
        List of pending knowledge requests with details
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    agent_id = get_mcp_memory_tier(service_account)

    trace = create_mcp_trace("get_pending_knowledge_requests", service_account, {
        "operation": "get_pending_requests",
        "agent_id": agent_id
    })
    span = create_mcp_span(trace, "get_pending_requests_operation", {
        "agent_id": agent_id
    })

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{MEMOS_BASE_URL}/memory/share/pending?agent_id={agent_id}",
                timeout=30.0
            )
            response.raise_for_status()
            result = response.json()

        requests = result.get("requests", [])
        formatted_requests = []

        if requests:
            formatted_requests.append(f"Found {len(requests)} pending knowledge requests for agent '{agent_id}':")
            formatted_requests.append("")

            for i, request in enumerate(requests):
                formatted_requests.append(f"Request {i+1}:")
                formatted_requests.append(f"  ID: {request.get('id', 'unknown')}")
                formatted_requests.append(f"  From: {request.get('requester_agent_id', 'unknown')}")
                formatted_requests.append(f"  Query: {request.get('query', 'No query')}")
                formatted_requests.append(f"  Confidence Threshold: {request.get('confidence_threshold', 'unknown')}")
                formatted_requests.append(f"  Sharing Policy: {request.get('sharing_policy', 'unknown')}")
                formatted_requests.append(f"  Created: {request.get('created_at', 'unknown')}")
                formatted_requests.append("")

            if span:
                span.end(output={"status": "success", "requests_count": len(requests), "agent_id": agent_id})

            log_mcp_request(service_account, "get_pending_knowledge_requests", {
                "requests_count": len(requests),
                "agent_id": agent_id
            })
            observability.record_mcp_tool_usage("get_pending_knowledge_requests", service_account, True)

            return "\n".join(formatted_requests)
        else:
            if span:
                span.end(output={"status": "success", "requests_count": 0, "agent_id": agent_id})

            observability.record_mcp_tool_usage("get_pending_knowledge_requests", service_account, True)
            return f"No pending knowledge requests found for agent '{agent_id}'"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "agent_id": agent_id})
        logger.error(f"Error getting pending knowledge requests for agent {agent_id}: {e}")
        observability.record_mcp_tool_usage("get_pending_knowledge_requests", service_account, False)
        return f"Error getting pending knowledge requests for agent '{agent_id}': {str(e)}"


@server.tool()
async def accept_knowledge_offer(request_id: int, offer_id: int, accept: bool = True) -> str:
    """
    Accept or reject a knowledge sharing offer.

    This tool allows the requesting agent to accept or reject a specific
    knowledge offer that was made in response to their request.

    Args:
        request_id: The ID of the original knowledge request
        offer_id: The ID of the specific offer to accept/reject
        accept: True to accept the offer, False to reject it

    Returns:
        Success message with acceptance/rejection details
    """
    service_account = request_context.get("service_account", "MCP_SYSTEM")
    agent_id = get_mcp_memory_tier(service_account)

    action = "accept" if accept else "reject"

    trace = create_mcp_trace(f"{action}_knowledge_offer", service_account, {
        "operation": f"{action}_offer",
        "request_id": request_id,
        "offer_id": offer_id
    })
    span = create_mcp_span(trace, f"{action}_offer_operation", {
        "agent_id": agent_id,
        "request_id": request_id,
        "offer_id": offer_id,
        "action": action
    })

    try:
        # For now, this is a placeholder - the actual accept/reject endpoint
        # would need to be implemented in the main API
        # This tool demonstrates the pattern for future implementation

        if span:
            span.end(output={"status": "success", "action": action, "request_id": request_id, "offer_id": offer_id})

        log_mcp_request(service_account, f"{action}_knowledge_offer", {
            "request_id": request_id,
            "offer_id": offer_id,
            "action": action
        })
        observability.record_mcp_tool_usage(f"{action}_knowledge_offer", service_account, True)

        return f"Knowledge offer {action}ed successfully: Request {request_id}, Offer {offer_id}"

    except Exception as e:
        if span:
            span.end(output={"status": "error", "error": str(e), "request_id": request_id, "offer_id": offer_id})
        logger.error(f"Error {action}ing knowledge offer {offer_id}: {e}")
        observability.record_mcp_tool_usage(f"{action}_knowledge_offer", service_account, False)
        return f"Error {action}ing knowledge offer {offer_id}: {str(e)}"


if __name__ == "__main__":
    # Run the MCP server
    uvicorn.run(app, host="0.0.0.0", port=8001)

]]></file>
  <file name="alert_rules.yml" path="memos.as/config/alert_rules.yml"><![CDATA[
groups:
  - name: memOS_API_Performance
    rules:
      - alert: HighAPIRequestRate
        expr: rate(http_requests_total[5m]) > 100
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High API request rate detected"
          description: "API request rate is {{ $value }} requests/sec on {{ $labels.endpoint }}"

      - alert: HighAPILatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High API latency detected"
          description: "95th percentile latency is {{ $value }}s for {{ $labels.method }} {{ $labels.endpoint }}"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.endpoint }}"

  - name: memOS_Memory_Operations
    rules:
      - alert: MemoryOperationFailures
        expr: rate(memory_operations_total{status="error"}[5m]) > 0.1
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Memory operation failures detected"
          description: "Memory operation failure rate is {{ $value }}/sec for {{ $labels.operation_type }}"

      - alert: SlowMemoryOperations
        expr: histogram_quantile(0.95, rate(memory_operation_duration_seconds_bucket[5m])) > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Slow memory operations detected"
          description: "95th percentile memory operation duration is {{ $value }}s for {{ $labels.operation_type }}"

      - alert: LargeMemorySize
        expr: persistent_memory_size_bytes > 100000000  # 100MB
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Large persistent memory size detected"
          description: "Session {{ $labels.session_id }} has memory size of {{ $value | humanizeBytes }}"

  - name: memOS_AI_Performance
    rules:
      - alert: SlowAIInference
        expr: histogram_quantile(0.95, rate(ai_model_inference_duration_seconds_bucket[5m])) > 30
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Slow AI model inference detected"
          description: "95th percentile inference time is {{ $value }}s for {{ $labels.model }} {{ $labels.operation }}"

      - alert: LowTokenProcessingRate
        expr: token_processing_rate < 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Low token processing rate"
          description: "Token processing rate is {{ $value }} tokens/sec for {{ $labels.model }}"

      - alert: AIServiceErrors
        expr: rate(ai_service_requests_total{status="error"}[5m]) > 0.1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "AI service errors detected"
          description: "AI service error rate is {{ $value }}/sec for {{ $labels.service }}"

      - alert: HighQueueSize
        expr: queue_size > 1000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High queue size detected"
          description: "Queue {{ $labels.queue_name }} has {{ $value }} items"

  - name: memOS_System_Health
    rules:
      - alert: ServiceDown
        expr: up{job=~"memos.*"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "memOS service is down"
          description: "Service {{ $labels.job }} on {{ $labels.instance }} is down"

      - alert: HighRequestsInFlight
        expr: http_requests_in_flight > 50
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High number of requests in flight"
          description: "{{ $value }} requests are currently being processed on {{ $labels.endpoint }}"

]]></file>
  <file name="memos-logs.json" path="memos.as/config/grafana/dashboards/memos-logs.json"><![CDATA[
{
  "dashboard": {
    "id": null,
    "title": "memOS Logs & Observability Dashboard",
    "tags": ["memos", "logs", "observability", "loki"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "API Access Logs",
        "type": "logs",
        "targets": [
          {
            "expr": "{service=\"ai-tool-api\"} | json | line_format \"{{.status_code}} {{.response_time}}ms {{.endpoint}} - {{.message}}\"",
            "legendFormat": "API Access"
          }
        ],
        "options": {
          "showTime": true,
          "showLabels": true,
          "sortOrder": "Descending"
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "API Response Time Distribution",
        "type": "stat",
        "targets": [
          {
            "expr": "quantile_over_time(0.95, {service=\"ai-tool-api\"} | json | unwrap response_time [5m])",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "quantile_over_time(0.50, {service=\"ai-tool-api\"} | json | unwrap response_time [5m])",
            "legendFormat": "Median"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "ms"
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0}
      },
      {
        "id": 3,
        "title": "Application Logs by Level",
        "type": "logs",
        "targets": [
          {
            "expr": "{service=\"ai-tool-app\"} | json | line_format \"[{{.level}}] {{.session_id}} - {{.message}}\"",
            "legendFormat": "Application Logs"
          }
        ],
        "options": {
          "showTime": true,
          "showLabels": true,
          "sortOrder": "Descending"
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 4,
        "title": "Log Level Distribution",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (level) (count_over_time({service=\"ai-tool-app\"} | json [1h]))",
            "legendFormat": "{{level}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      },
      {
        "id": 5,
        "title": "Error Logs",
        "type": "logs",
        "targets": [
          {
            "expr": "{service=\"ai-tool\"} | json | level=\"error\" | line_format \"[ERROR] {{.timestamp}} - {{.message}}\"",
            "legendFormat": "Error Logs"
          }
        ],
        "options": {
          "showTime": true,
          "showLabels": true,
          "sortOrder": "Descending"
        },
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "fixed",
              "fixedColor": "red"
            }
          }
        },
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16}
      },
      {
        "id": 6,
        "title": "Memory Operations",
        "type": "logs",
        "targets": [
          {
            "expr": "{service=\"memory-service\"} | json | line_format \"{{.operation}} ({{.memory_type}}) - {{.duration}}ms\"",
            "legendFormat": "Memory Operations"
          }
        ],
        "options": {
          "showTime": true,
          "showLabels": true,
          "sortOrder": "Descending"
        },
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 24}
      },
      {
        "id": 7,
        "title": "Memory Operation Duration",
        "type": "graph",
        "targets": [
          {
            "expr": "quantile_over_time(0.95, {service=\"memory-service\"} | json | unwrap duration [5m]) by (operation)",
            "legendFormat": "95th percentile {{operation}}"
          },
          {
            "expr": "quantile_over_time(0.50, {service=\"memory-service\"} | json | unwrap duration [5m]) by (operation)",
            "legendFormat": "Median {{operation}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "ms"
          }
        },
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 24}
      },
      {
        "id": 8,
        "title": "API Status Code Distribution",
        "type": "bargauge",
        "targets": [
          {
            "expr": "sum by (status_code) (count_over_time({service=\"ai-tool-api\"} | json [1h]))",
            "legendFormat": "{{status_code}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 300},
                {"color": "red", "value": 400}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 8, "x": 0, "y": 32}
      },
      {
        "id": 9,
        "title": "Session Activity",
        "type": "graph",
        "targets": [
          {
            "expr": "count by (session_id) (count_over_time({service=~\"ai-tool.*\"} | json | session_id != \"\" [5m]))",
            "legendFormat": "{{session_id}}"
          }
        ],
        "gridPos": {"h": 8, "w": 8, "x": 8, "y": 32}
      },
      {
        "id": 10,
        "title": "Error Rate Over Time",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(({service=\"ai-tool\"} | json | level=\"error\") [5m])",
            "legendFormat": "Error Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "fixed",
              "fixedColor": "red"
            }
          }
        },
        "gridPos": {"h": 8, "w": 8, "x": 16, "y": 32}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "10s"
  }
}

]]></file>
  <file name="memos-observability.json" path="memos.as/config/grafana/dashboards/memos-observability.json"><![CDATA[
{
  "dashboard": {
    "id": null,
    "title": "memOS Comprehensive Observability Dashboard",
    "tags": ["memos", "observability", "llm", "ai", "memory"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "API Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 10},
                {"color": "red", "value": 50}
              ]
            },
            "unit": "reqps"
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "API Response Times (95th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{method}} {{endpoint}}"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "Median {{method}} {{endpoint}}"
          }
        ],
        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0}
      },
      {
        "id": 3,
        "title": "Active HTTP Requests",
        "type": "stat",
        "targets": [
          {
            "expr": "http_requests_in_flight",
            "legendFormat": "{{endpoint}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 10},
                {"color": "red", "value": 25}
              ]
            },
            "unit": "short"
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0}
      },
      {
        "id": 4,
        "title": "HTTP Status Codes",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (status) (rate(http_requests_total[5m]))",
            "legendFormat": "{{status}}"
          }
        ],
        "gridPos": {"h": 8, "w": 6, "x": 18, "y": 0}
      },
      {
        "id": 5,
        "title": "Memory Operations Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(memory_operations_total[5m])",
            "legendFormat": "{{operation_type}} - {{status}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 6,
        "title": "Memory Operation Duration",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(memory_operation_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile {{operation_type}}"
          },
          {
            "expr": "histogram_quantile(0.50, rate(memory_operation_duration_seconds_bucket[5m]))",
            "legendFormat": "Median {{operation_type}}"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      },
      {
        "id": 7,
        "title": "Persistent Memory Size by Session",
        "type": "graph",
        "targets": [
          {
            "expr": "persistent_memory_size_bytes",
            "legendFormat": "Session {{session_id}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "bytes"
          }
        },
        "gridPos": {"h": 8, "w": 8, "x": 0, "y": 16}
      },
      {
        "id": 8,
        "title": "Memory Search Results",
        "type": "stat",
        "targets": [
          {
            "expr": "memory_search_results",
            "legendFormat": "{{query_type}}"
          }
        ],
        "gridPos": {"h": 8, "w": 8, "x": 8, "y": 16}
      },
      {
        "id": 9,
        "title": "AI Model Inference Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(ai_model_inference_duration_seconds_bucket[5m]))",
            "legendFormat": "95th {{model}} {{operation}}"
          },
          {
            "expr": "histogram_quantile(0.50, rate(ai_model_inference_duration_seconds_bucket[5m]))",
            "legendFormat": "Median {{model}} {{operation}}"
          }
        ],
        "gridPos": {"h": 8, "w": 8, "x": 16, "y": 16}
      },
      {
        "id": 10,
        "title": "Token Processing Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "token_processing_rate",
            "legendFormat": "{{model}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "tps"
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 24}
      },
      {
        "id": 11,
        "title": "AI Service Requests",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(ai_service_requests_total[5m])",
            "legendFormat": "{{service}} - {{status}}"
          }
        ],
        "gridPos": {"h": 8, "w": 9, "x": 6, "y": 24}
      },
      {
        "id": 12,
        "title": "Queue Sizes",
        "type": "graph",
        "targets": [
          {
            "expr": "queue_size",
            "legendFormat": "{{queue_name}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 100},
                {"color": "red", "value": 500}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 9, "x": 15, "y": 24}
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "10s"
  }
}

]]></file>
  <file name="dashboards.yml" path="memos.as/config/grafana/provisioning/dashboards/dashboards.yml"><![CDATA[
apiVersion: 1

providers:
  - name: 'memOS Dashboards'
    orgId: 1
    folder: 'memOS'
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards

]]></file>
  <file name="datasources.yml" path="memos.as/config/grafana/provisioning/datasources/datasources.yml"><![CDATA[
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    orgId: 1
    url: http://prometheus:9090
    basicAuth: false
    isDefault: true
    version: 1
    editable: true

  - name: Loki
    type: loki
    access: proxy
    orgId: 1
    url: http://loki:3100
    basicAuth: false
    isDefault: false
    version: 1
    editable: true

  - name: Jaeger
    type: jaeger
    access: proxy
    orgId: 1
    url: http://jaeger:16686
    basicAuth: false
    isDefault: false
    version: 1
    editable: true

]]></file>
  <file name="loki-config.yaml" path="memos.as/config/loki-config.yaml"><![CDATA[
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  instance_addr: 127.0.0.1
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

# By default, Loki will send anonymous, but uniquely-identifiable usage and configuration
# analytics to Grafana Labs. These statistics are sent to https://stats.grafana.org/
#
# Statistics help us better understand how Loki is used, and they show us performance
# levels for most users. This helps us prioritize features and documentation.
# For more information on what's sent: https://github.com/grafana/loki/blob/main/pkg/usagestats/stats.go
# Refer to the buildReport method to see what goes into a report.
#
# If you would like to disable reporting, uncomment the following lines:
#analytics:
#  reporting_enabled: false

]]></file>
  <file name="otel-collector-config.yaml" path="memos.as/config/otel-collector-config.yaml"><![CDATA[
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318
  prometheus:
    config:
      scrape_configs:
        - job_name: 'memos-api'
          static_configs:
            - targets: ['memos-api:8090']

processors:
  batch:

exporters:
  prometheus:
    endpoint: "0.0.0.0:8889"
  otlp:
    endpoint: http://jaeger:14250
    tls:
      insecure: true

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp]
    metrics:
      receivers: [otlp, prometheus]
      processors: [batch]
      exporters: [prometheus]

]]></file>
  <file name="prometheus.yml" path="memos.as/config/prometheus.yml"><![CDATA[
# Prometheus configuration for memOS observability

global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets: []

scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # memOS application metrics
  - job_name: 'memos-api'
    static_configs:
      - targets: ['memos-api:8090']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 5s

  # memOS Memory Operations metrics
  - job_name: 'memos-memory'
    static_configs:
      - targets: ['memos-api:8090']
    metrics_path: '/metrics/memory'
    scrape_interval: 5s
    scrape_timeout: 3s

  # memOS AI/ML metrics
  - job_name: 'memos-ai'
    static_configs:
      - targets: ['memos-api:8090']
    metrics_path: '/metrics/ai'
    scrape_interval: 10s
    scrape_timeout: 5s

  # OpenTelemetry Collector metrics
  - job_name: 'otel-collector'
    static_configs:
      - targets: ['otel-collector:8888', 'otel-collector:8889']

  # Jaeger metrics
  - job_name: 'jaeger'
    static_configs:
      - targets: ['jaeger:14269']

  # Node exporter (if available)
  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']

]]></file>
  <file name="promtail-config.yaml" path="memos.as/config/promtail-config.yaml"><![CDATA[
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker container logs with service labels
  - job_name: container_logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: containerlogs
          __path__: /var/log/containers/*.log
    pipeline_stages:
      - json:
          expressions:
            output: log
            stream: stream
            time: time
            container_name: attrs.tag
      - regex:
          expression: '^(?P<container_id>[^_]+)_(?P<service>[^_]+)_'
          source: container_name
      - labels:
          service:
          container_id:
      - timestamp:
          source: time
          format: RFC3339Nano
      - output:
          source: output

  # AI Tool API service logs
  - job_name: ai_tool_api
    static_configs:
      - targets:
          - localhost
        labels:
          job: ai_tool_api
          service: ai-tool-api
          __path__: /var/log/ai-tool-api/*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            status_code: status_code
            response_time: response_time
            endpoint: endpoint
            session_id: session_id
      - labels:
          level:
          endpoint:
          status_code:
      - timestamp:
          source: timestamp
          format: RFC3339
      - output:
          source: message

  # AI Tool Application logs
  - job_name: ai_tool_app
    static_configs:
      - targets:
          - localhost
        labels:
          job: ai_tool_app
          service: ai-tool-app
          __path__: /var/log/ai-tool-app/*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            session_id: session_id
            component: component
      - labels:
          level:
          session_id:
          component:
      - timestamp:
          source: timestamp
          format: RFC3339
      - output:
          source: message

  # Memory Service logs
  - job_name: memory_service
    static_configs:
      - targets:
          - localhost
        labels:
          job: memory_service
          service: memory-service
          __path__: /var/log/memory-service/*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            operation: operation
            duration: duration
            memory_type: memory_type
            session_id: session_id
      - labels:
          level:
          operation:
          memory_type:
      - timestamp:
          source: timestamp
          format: RFC3339
      - output:
          source: message

  # memOS API logs (from Docker container)
  - job_name: memos_api
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: [__meta_docker_container_name]
        regex: '/memos_api'
        target_label: __service__
        replacement: 'memos-api'
      - source_labels: [__service__]
        target_label: service
    pipeline_stages:
      - json:
          expressions:
            output: log
            stream: stream
            time: time
      - json:
          expressions:
            timestamp: timestamp
            level: level
            message: message
            endpoint: endpoint
            method: method
            status_code: status_code
            response_time: response_time
          source: output
      - labels:
          level:
          endpoint:
          method:
          status_code:
      - timestamp:
          source: timestamp
          format: RFC3339
          fallback_formats:
            - "2006-01-02T15:04:05Z07:00"
            - "2006-01-02 15:04:05"
      - output:
          source: message

]]></file>
  <file name="docker-compose.yml" path="memos.as/docker-compose.yml"><![CDATA[
version: '3.8'

services:
  memos-api:
    build: .
    container_name: devenviro_memos_api
    ports:
      - "8091:8090" # Expose on a different host port to avoid conflicts
    env_file:
      - .env.docker
    environment:
      - NEO4J_URI=bolt://devenviro_neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=password
    # Connect this service to the existing DevEnviro network
    networks:
      - apexsigma_net
    extra_hosts:
      - "devenviro_postgres:172.22.0.10"

  memos-mcp-server:
    build: .
    container_name: memos_mcp_server
    ports:
      - "8001:8001"
    env_file:
      - .env.docker
    environment:
      - MEMOS_BASE_URL=http://memos-api:8090
      - JWT_SECRET_KEY=apexsigma-mcp-secret-key-2025
      # Langfuse observability integration
            - LANGFUSE_PUBLIC_KEY=pk-lf-REDACTED_FOR_COMMIT
            - LANGFUSE_SECRET_KEY=REDACTED_FOR_COMMIT
            - LANGFUSE_HOST=https://cloud.langfuse.com
    networks:
      apexsigma_net:
      mcp_net:
        ipv4_address: 172.28.0.10
    command: ["python", "-m", "app.mcp_server"]
    depends_on:
      - memos-api

  test-runner:
    build: .
    env_file:
      - .env.docker
    environment:
      PYTHONPATH: /code
      API_HOST: memos-api
    networks:
      - apexsigma_net
    command: ["pytest", "-v", "/code/app/tests/"]

  memos-worker:
    build: .
    container_name: devenviro_memos_worker
    env_file:
      - .env.docker
    environment:
      - NEO4J_URI=bolt://devenviro_neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=password
    networks:
      - apexsigma_net
    command: ["python", "-m", "app.background_worker"]
    depends_on:
      - memos-api

networks:
  apexsigma_net:
    # This tells Docker Compose to use an existing network
    external: true
  mcp_net:
    external: true
    ipam:
      config:
        - subnet: 172.28.0.0/16

]]></file>
  <file name="mkdocs.yml" path="memos.as/mkdocs.yml"><![CDATA[
# mkdocs.yml
site_name: memOS.as Documentation
site_url: https://docs.memos.as/
repo_url: https://github.com/ApexSigma-Solutions/memos.as

theme:
  name: material
  palette:
    - scheme: default
      toggle:
        icon: material/weather-sunny
        name: Switch to dark mode
    - scheme: slate
      toggle:
        icon: material/weather-night
        name: Switch to light mode
  features:
    - navigation.tabs
    - search.suggest
    - content.code.copy

# Add the mkdocstrings plugin
plugins:
  - search
  - mkdocstrings:
      handlers:
        python:
          options:
            show_source: true

# The 'nav' section defines the site's navigation.
nav:
  - 'Introduction': 'index.md'
  - 'Tutorials': 'tutorials/index.md'
  - 'How-To Guides': 'how-to/index.md'
  - 'API Reference':
    - 'reference/index.md'
    - 'Services': 'reference/services.md'
]]></file>
  <file name="poetry.lock" path="memos.as/poetry.lock"><![CDATA[
# This file is automatically @generated by Poetry 2.1.4 and should not be changed by hand.

[[package]]
name = "annotated-types"
version = "0.7.0"
description = "Reusable constraint types to use with typing.Annotated"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "annotated_types-0.7.0-py3-none-any.whl", hash = "sha256:1f02e8b43a8fbbc3f3e0d4f0f4bfc8131bcb4eebe8849b8e5c773f3a1c582a53"},
    {file = "annotated_types-0.7.0.tar.gz", hash = "sha256:aff07c09a53a08bc8cfccb9c85b05f1aa9a2a6f23728d790723543408344ce89"},
]

[[package]]
name = "anyio"
version = "4.10.0"
description = "High-level concurrency and networking framework on top of asyncio or Trio"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "anyio-4.10.0-py3-none-any.whl", hash = "sha256:60e474ac86736bbfd6f210f7a61218939c318f43f9972497381f1c5e930ed3d1"},
    {file = "anyio-4.10.0.tar.gz", hash = "sha256:3f3fae35c96039744587aa5b8371e7e8e603c0702999535961dd336026973ba6"},
]

[package.dependencies]
idna = ">=2.8"
sniffio = ">=1.1"

[package.extras]
trio = ["trio (>=0.26.1)"]

[[package]]
name = "attrs"
version = "25.3.0"
description = "Classes Without Boilerplate"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "attrs-25.3.0-py3-none-any.whl", hash = "sha256:427318ce031701fea540783410126f03899a97ffc6f61596ad581ac2e40e3bc3"},
    {file = "attrs-25.3.0.tar.gz", hash = "sha256:75d7cefc7fb576747b2c81b4442d4d4a1ce0900973527c011d1030fd3bf4af1b"},
]

[package.extras]
benchmark = ["cloudpickle ; platform_python_implementation == \"CPython\"", "hypothesis", "mypy (>=1.11.1) ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pympler", "pytest (>=4.3.0)", "pytest-codspeed", "pytest-mypy-plugins ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pytest-xdist[psutil]"]
cov = ["cloudpickle ; platform_python_implementation == \"CPython\"", "coverage[toml] (>=5.3)", "hypothesis", "mypy (>=1.11.1) ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pympler", "pytest (>=4.3.0)", "pytest-mypy-plugins ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pytest-xdist[psutil]"]
dev = ["cloudpickle ; platform_python_implementation == \"CPython\"", "hypothesis", "mypy (>=1.11.1) ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pre-commit-uv", "pympler", "pytest (>=4.3.0)", "pytest-mypy-plugins ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pytest-xdist[psutil]"]
docs = ["cogapp", "furo", "myst-parser", "sphinx", "sphinx-notfound-page", "sphinxcontrib-towncrier", "towncrier"]
tests = ["cloudpickle ; platform_python_implementation == \"CPython\"", "hypothesis", "mypy (>=1.11.1) ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pympler", "pytest (>=4.3.0)", "pytest-mypy-plugins ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pytest-xdist[psutil]"]
tests-mypy = ["mypy (>=1.11.1) ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\"", "pytest-mypy-plugins ; platform_python_implementation == \"CPython\" and python_version >= \"3.10\""]

[[package]]
name = "backoff"
version = "2.2.1"
description = "Function decoration for backoff and retry"
optional = false
python-versions = ">=3.7,<4.0"
groups = ["main"]
files = [
    {file = "backoff-2.2.1-py3-none-any.whl", hash = "sha256:63579f9a0628e06278f7e47b7d7d5b6ce20dc65c5e96a6f3ca99a6adca0396e8"},
    {file = "backoff-2.2.1.tar.gz", hash = "sha256:03f829f5bb1923180821643f8753b0502c3b682293992485b0eef2807afa5cba"},
]

[[package]]
name = "certifi"
version = "2025.8.3"
description = "Python package for providing Mozilla's CA Bundle."
optional = false
python-versions = ">=3.7"
groups = ["main"]
files = [
    {file = "certifi-2025.8.3-py3-none-any.whl", hash = "sha256:f6c12493cfb1b06ba2ff328595af9350c65d6644968e5d3a2ffd78699af217a5"},
    {file = "certifi-2025.8.3.tar.gz", hash = "sha256:e564105f78ded564e3ae7c923924435e1daa7463faeab5bb932bc53ffae63407"},
]

[[package]]
name = "cfgv"
version = "3.4.0"
description = "Validate configuration and produce human readable error messages."
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "cfgv-3.4.0-py2.py3-none-any.whl", hash = "sha256:b7265b1f29fd3316bfcd2b330d63d024f2bfd8bcb8b0272f8e19a504856c48f9"},
    {file = "cfgv-3.4.0.tar.gz", hash = "sha256:e52591d4c5f5dead8e0f673fb16db7949d2cfb3f7da4582893288f0ded8fe560"},
]

[[package]]
name = "charset-normalizer"
version = "3.4.3"
description = "The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet."
optional = false
python-versions = ">=3.7"
groups = ["main"]
files = [
    {file = "charset_normalizer-3.4.3-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:fb7f67a1bfa6e40b438170ebdc8158b78dc465a5a67b6dde178a46987b244a72"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:cc9370a2da1ac13f0153780040f465839e6cccb4a1e44810124b4e22483c93fe"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:07a0eae9e2787b586e129fdcbe1af6997f8d0e5abaa0bc98c0e20e124d67e601"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:74d77e25adda8581ffc1c720f1c81ca082921329452eba58b16233ab1842141c"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:d0e909868420b7049dafd3a31d45125b31143eec59235311fc4c57ea26a4acd2"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:c6f162aabe9a91a309510d74eeb6507fab5fff92337a15acbe77753d88d9dcf0"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-musllinux_1_2_ppc64le.whl", hash = "sha256:4ca4c094de7771a98d7fbd67d9e5dbf1eb73efa4f744a730437d8a3a5cf994f0"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-musllinux_1_2_s390x.whl", hash = "sha256:02425242e96bcf29a49711b0ca9f37e451da7c70562bc10e8ed992a5a7a25cc0"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:78deba4d8f9590fe4dae384aeff04082510a709957e968753ff3c48399f6f92a"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-win32.whl", hash = "sha256:d79c198e27580c8e958906f803e63cddb77653731be08851c7df0b1a14a8fc0f"},
    {file = "charset_normalizer-3.4.3-cp310-cp310-win_amd64.whl", hash = "sha256:c6e490913a46fa054e03699c70019ab869e990270597018cef1d8562132c2669"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:b256ee2e749283ef3ddcff51a675ff43798d92d746d1a6e4631bf8c707d22d0b"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:13faeacfe61784e2559e690fc53fa4c5ae97c6fcedb8eb6fb8d0a15b475d2c64"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:00237675befef519d9af72169d8604a067d92755e84fe76492fef5441db05b91"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:585f3b2a80fbd26b048a0be90c5aae8f06605d3c92615911c3a2b03a8a3b796f"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:0e78314bdc32fa80696f72fa16dc61168fda4d6a0c014e0380f9d02f0e5d8a07"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:96b2b3d1a83ad55310de8c7b4a2d04d9277d5591f40761274856635acc5fcb30"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-musllinux_1_2_ppc64le.whl", hash = "sha256:939578d9d8fd4299220161fdd76e86c6a251987476f5243e8864a7844476ba14"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-musllinux_1_2_s390x.whl", hash = "sha256:fd10de089bcdcd1be95a2f73dbe6254798ec1bda9f450d5828c96f93e2536b9c"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:1e8ac75d72fa3775e0b7cb7e4629cec13b7514d928d15ef8ea06bca03ef01cae"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-win32.whl", hash = "sha256:6cf8fd4c04756b6b60146d98cd8a77d0cdae0e1ca20329da2ac85eed779b6849"},
    {file = "charset_normalizer-3.4.3-cp311-cp311-win_amd64.whl", hash = "sha256:31a9a6f775f9bcd865d88ee350f0ffb0e25936a7f930ca98995c05abf1faf21c"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:e28e334d3ff134e88989d90ba04b47d84382a828c061d0d1027b1b12a62b39b1"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:0cacf8f7297b0c4fcb74227692ca46b4a5852f8f4f24b3c766dd94a1075c4884"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:c6fd51128a41297f5409deab284fecbe5305ebd7e5a1f959bee1c054622b7018"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:3cfb2aad70f2c6debfbcb717f23b7eb55febc0bb23dcffc0f076009da10c6392"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:1606f4a55c0fd363d754049cdf400175ee96c992b1f8018b993941f221221c5f"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:027b776c26d38b7f15b26a5da1044f376455fb3766df8fc38563b4efbc515154"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-musllinux_1_2_ppc64le.whl", hash = "sha256:42e5088973e56e31e4fa58eb6bd709e42fc03799c11c42929592889a2e54c491"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-musllinux_1_2_s390x.whl", hash = "sha256:cc34f233c9e71701040d772aa7490318673aa7164a0efe3172b2981218c26d93"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:320e8e66157cc4e247d9ddca8e21f427efc7a04bbd0ac8a9faf56583fa543f9f"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-win32.whl", hash = "sha256:fb6fecfd65564f208cbf0fba07f107fb661bcd1a7c389edbced3f7a493f70e37"},
    {file = "charset_normalizer-3.4.3-cp312-cp312-win_amd64.whl", hash = "sha256:86df271bf921c2ee3818f0522e9a5b8092ca2ad8b065ece5d7d9d0e9f4849bcc"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:14c2a87c65b351109f6abfc424cab3927b3bdece6f706e4d12faaf3d52ee5efe"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:41d1fc408ff5fdfb910200ec0e74abc40387bccb3252f3f27c0676731df2b2c8"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:1bb60174149316da1c35fa5233681f7c0f9f514509b8e399ab70fea5f17e45c9"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:30d006f98569de3459c2fc1f2acde170b7b2bd265dc1943e87e1a4efe1b67c31"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:416175faf02e4b0810f1f38bcb54682878a4af94059a1cd63b8747244420801f"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:6aab0f181c486f973bc7262a97f5aca3ee7e1437011ef0c2ec04b5a11d16c927"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-musllinux_1_2_ppc64le.whl", hash = "sha256:fdabf8315679312cfa71302f9bd509ded4f2f263fb5b765cf1433b39106c3cc9"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-musllinux_1_2_s390x.whl", hash = "sha256:bd28b817ea8c70215401f657edef3a8aa83c29d447fb0b622c35403780ba11d5"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:18343b2d246dc6761a249ba1fb13f9ee9a2bcd95decc767319506056ea4ad4dc"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-win32.whl", hash = "sha256:6fb70de56f1859a3f71261cbe41005f56a7842cc348d3aeb26237560bfa5e0ce"},
    {file = "charset_normalizer-3.4.3-cp313-cp313-win_amd64.whl", hash = "sha256:cf1ebb7d78e1ad8ec2a8c4732c7be2e736f6e5123a4146c5b89c9d1f585f8cef"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:3cd35b7e8aedeb9e34c41385fda4f73ba609e561faedfae0a9e75e44ac558a15"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b89bc04de1d83006373429975f8ef9e7932534b8cc9ca582e4db7d20d91816db"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:2001a39612b241dae17b4687898843f254f8748b796a2e16f1051a17078d991d"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:8dcfc373f888e4fb39a7bc57e93e3b845e7f462dacc008d9749568b1c4ece096"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:18b97b8404387b96cdbd30ad660f6407799126d26a39ca65729162fd810a99aa"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:ccf600859c183d70eb47e05a44cd80a4ce77394d1ac0f79dbd2dd90a69a3a049"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-musllinux_1_2_ppc64le.whl", hash = "sha256:53cd68b185d98dde4ad8990e56a58dea83a4162161b1ea9272e5c9182ce415e0"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-musllinux_1_2_s390x.whl", hash = "sha256:30a96e1e1f865f78b030d65241c1ee850cdf422d869e9028e2fc1d5e4db73b92"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:d716a916938e03231e86e43782ca7878fb602a125a91e7acb8b5112e2e96ac16"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-win32.whl", hash = "sha256:c6dbd0ccdda3a2ba7c2ecd9d77b37f3b5831687d8dc1b6ca5f56a4880cc7b7ce"},
    {file = "charset_normalizer-3.4.3-cp314-cp314-win_amd64.whl", hash = "sha256:73dc19b562516fc9bcf6e5d6e596df0b4eb98d87e4f79f3ae71840e6ed21361c"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:0f2be7e0cf7754b9a30eb01f4295cc3d4358a479843b31f328afd210e2c7598c"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:c60e092517a73c632ec38e290eba714e9627abe9d301c8c8a12ec32c314a2a4b"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:252098c8c7a873e17dd696ed98bbe91dbacd571da4b87df3736768efa7a792e4"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:3653fad4fe3ed447a596ae8638b437f827234f01a8cd801842e43f3d0a6b281b"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:8999f965f922ae054125286faf9f11bc6932184b93011d138925a1773830bbe9"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:d95bfb53c211b57198bb91c46dd5a2d8018b3af446583aab40074bf7988401cb"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-musllinux_1_2_ppc64le.whl", hash = "sha256:5b413b0b1bfd94dbf4023ad6945889f374cd24e3f62de58d6bb102c4d9ae534a"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-musllinux_1_2_s390x.whl", hash = "sha256:b5e3b2d152e74e100a9e9573837aba24aab611d39428ded46f4e4022ea7d1942"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:a2d08ac246bb48479170408d6c19f6385fa743e7157d716e144cad849b2dd94b"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-win32.whl", hash = "sha256:ec557499516fc90fd374bf2e32349a2887a876fbf162c160e3c01b6849eaf557"},
    {file = "charset_normalizer-3.4.3-cp38-cp38-win_amd64.whl", hash = "sha256:5d8d01eac18c423815ed4f4a2ec3b439d654e55ee4ad610e153cf02faf67ea40"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:70bfc5f2c318afece2f5838ea5e4c3febada0be750fcf4775641052bbba14d05"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:23b6b24d74478dc833444cbd927c338349d6ae852ba53a0d02a2de1fce45b96e"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-manylinux2014_ppc64le.manylinux_2_17_ppc64le.manylinux_2_28_ppc64le.whl", hash = "sha256:34a7f768e3f985abdb42841e20e17b330ad3aaf4bb7e7aeeb73db2e70f077b99"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-manylinux2014_s390x.manylinux_2_17_s390x.manylinux_2_28_s390x.whl", hash = "sha256:fb731e5deb0c7ef82d698b0f4c5bb724633ee2a489401594c5c88b02e6cb15f7"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:257f26fed7d7ff59921b78244f3cd93ed2af1800ff048c33f624c87475819dd7"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:1ef99f0456d3d46a50945c98de1774da86f8e992ab5c77865ea8b8195341fc19"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-musllinux_1_2_ppc64le.whl", hash = "sha256:2c322db9c8c89009a990ef07c3bcc9f011a3269bc06782f916cd3d9eed7c9312"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-musllinux_1_2_s390x.whl", hash = "sha256:511729f456829ef86ac41ca78c63a5cb55240ed23b4b737faca0eb1abb1c41bc"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:88ab34806dea0671532d3f82d82b85e8fc23d7b2dd12fa837978dad9bb392a34"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-win32.whl", hash = "sha256:16a8770207946ac75703458e2c743631c79c59c5890c80011d536248f8eaa432"},
    {file = "charset_normalizer-3.4.3-cp39-cp39-win_amd64.whl", hash = "sha256:d22dbedd33326a4a5190dd4fe9e9e693ef12160c77382d9e87919bce54f3d4ca"},
    {file = "charset_normalizer-3.4.3-py3-none-any.whl", hash = "sha256:ce571ab16d890d23b5c278547ba694193a45011ff86a9162a71307ed9f86759a"},
    {file = "charset_normalizer-3.4.3.tar.gz", hash = "sha256:6fce4b8500244f6fcb71465d4a4930d132ba9ab8e71a7859e6a5d59851068d14"},
]

[[package]]
name = "click"
version = "8.2.1"
description = "Composable command line interface toolkit"
optional = false
python-versions = ">=3.10"
groups = ["main"]
files = [
    {file = "click-8.2.1-py3-none-any.whl", hash = "sha256:61a3265b914e850b85317d0b3109c7f8cd35a670f963866005d6ef1d5175a12b"},
    {file = "click-8.2.1.tar.gz", hash = "sha256:27c491cc05d968d271d5a1db13e3b5a184636d9d930f148c50b038f0d0646202"},
]

[package.dependencies]
colorama = {version = "*", markers = "platform_system == \"Windows\""}

[[package]]
name = "colorama"
version = "0.4.6"
description = "Cross-platform colored terminal text."
optional = false
python-versions = "!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7"
groups = ["main"]
markers = "sys_platform == \"win32\" or platform_system == \"Windows\""
files = [
    {file = "colorama-0.4.6-py2.py3-none-any.whl", hash = "sha256:4f1d9991f5acc0ca119f9d443620b77f9d6b33703e51011c16baf57afb285fc6"},
    {file = "colorama-0.4.6.tar.gz", hash = "sha256:08695f5cb7ed6e0531a20572697297273c47b8cae5a63ffc6d6ed5c201be6e44"},
]

[[package]]
name = "distlib"
version = "0.4.0"
description = "Distribution utilities"
optional = false
python-versions = "*"
groups = ["main"]
files = [
    {file = "distlib-0.4.0-py2.py3-none-any.whl", hash = "sha256:9659f7d87e46584a30b5780e43ac7a2143098441670ff0a49d5f9034c54a6c16"},
    {file = "distlib-0.4.0.tar.gz", hash = "sha256:feec40075be03a04501a973d81f633735b4b69f98b05450592310c0f401a4e0d"},
]

[[package]]
name = "fastapi"
version = "0.116.1"
description = "FastAPI framework, high performance, easy to learn, fast to code, ready for production"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "fastapi-0.116.1-py3-none-any.whl", hash = "sha256:c46ac7c312df840f0c9e220f7964bada936781bc4e2e6eb71f1c4d7553786565"},
    {file = "fastapi-0.116.1.tar.gz", hash = "sha256:ed52cbf946abfd70c5a0dccb24673f0670deeb517a88b3544d03c2a6bf283143"},
]

[package.dependencies]
pydantic = ">=1.7.4,<1.8 || >1.8,<1.8.1 || >1.8.1,<2.0.0 || >2.0.0,<2.0.1 || >2.0.1,<2.1.0 || >2.1.0,<3.0.0"
starlette = ">=0.40.0,<0.48.0"
typing-extensions = ">=4.8.0"

[package.extras]
all = ["email-validator (>=2.0.0)", "fastapi-cli[standard] (>=0.0.8)", "httpx (>=0.23.0)", "itsdangerous (>=1.1.0)", "jinja2 (>=3.1.5)", "orjson (>=3.2.1)", "pydantic-extra-types (>=2.0.0)", "pydantic-settings (>=2.0.0)", "python-multipart (>=0.0.18)", "pyyaml (>=5.3.1)", "ujson (>=4.0.1,!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0)", "uvicorn[standard] (>=0.12.0)"]
standard = ["email-validator (>=2.0.0)", "fastapi-cli[standard] (>=0.0.8)", "httpx (>=0.23.0)", "jinja2 (>=3.1.5)", "python-multipart (>=0.0.18)", "uvicorn[standard] (>=0.12.0)"]
standard-no-fastapi-cloud-cli = ["email-validator (>=2.0.0)", "fastapi-cli[standard-no-fastapi-cloud-cli] (>=0.0.8)", "httpx (>=0.23.0)", "jinja2 (>=3.1.5)", "python-multipart (>=0.0.18)", "uvicorn[standard] (>=0.12.0)"]

[[package]]
name = "filelock"
version = "3.19.1"
description = "A platform independent file lock."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "filelock-3.19.1-py3-none-any.whl", hash = "sha256:d38e30481def20772f5baf097c122c3babc4fcdb7e14e57049eb9d88c6dc017d"},
    {file = "filelock-3.19.1.tar.gz", hash = "sha256:66eda1888b0171c998b35be2bcc0f6d75c388a7ce20c3f3f37aa8e96c2dddf58"},
]

[[package]]
name = "googleapis-common-protos"
version = "1.70.0"
description = "Common protobufs used in Google APIs"
optional = false
python-versions = ">=3.7"
groups = ["main"]
files = [
    {file = "googleapis_common_protos-1.70.0-py3-none-any.whl", hash = "sha256:b8bfcca8c25a2bb253e0e0b0adaf8c00773e5e6af6fd92397576680b807e0fd8"},
    {file = "googleapis_common_protos-1.70.0.tar.gz", hash = "sha256:0e1b44e0ea153e6594f9f394fef15193a68aaaea2d843f83e2742717ca753257"},
]

[package.dependencies]
protobuf = ">=3.20.2,<4.21.1 || >4.21.1,<4.21.2 || >4.21.2,<4.21.3 || >4.21.3,<4.21.4 || >4.21.4,<4.21.5 || >4.21.5,<7.0.0"

[package.extras]
grpc = ["grpcio (>=1.44.0,<2.0.0)"]

[[package]]
name = "h11"
version = "0.16.0"
description = "A pure-Python, bring-your-own-I/O implementation of HTTP/1.1"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "h11-0.16.0-py3-none-any.whl", hash = "sha256:63cf8bbe7522de3bf65932fda1d9c2772064ffb3dae62d55932da54b31cb6c86"},
    {file = "h11-0.16.0.tar.gz", hash = "sha256:4e35b956cf45792e4caa5885e69fba00bdbc6ffafbfa020300e549b208ee5ff1"},
]

[[package]]
name = "httpcore"
version = "1.0.9"
description = "A minimal low-level HTTP client."
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "httpcore-1.0.9-py3-none-any.whl", hash = "sha256:2d400746a40668fc9dec9810239072b40b4484b640a8c38fd654a024c7a1bf55"},
    {file = "httpcore-1.0.9.tar.gz", hash = "sha256:6e34463af53fd2ab5d807f399a9b45ea31c3dfa2276f15a2c3f00afff6e176e8"},
]

[package.dependencies]
certifi = "*"
h11 = ">=0.16"

[package.extras]
asyncio = ["anyio (>=4.0,<5.0)"]
http2 = ["h2 (>=3,<5)"]
socks = ["socksio (==1.*)"]
trio = ["trio (>=0.22.0,<1.0)"]

[[package]]
name = "httpx"
version = "0.28.1"
description = "The next generation HTTP client."
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "httpx-0.28.1-py3-none-any.whl", hash = "sha256:d909fcccc110f8c7faf814ca82a9a4d816bc5a6dbfea25d6591d6985b8ba59ad"},
    {file = "httpx-0.28.1.tar.gz", hash = "sha256:75e98c5f16b0f35b567856f597f06ff2270a374470a5c2392242528e3e3e42fc"},
]

[package.dependencies]
anyio = "*"
certifi = "*"
httpcore = "==1.*"
idna = "*"

[package.extras]
brotli = ["brotli ; platform_python_implementation == \"CPython\"", "brotlicffi ; platform_python_implementation != \"CPython\""]
cli = ["click (==8.*)", "pygments (==2.*)", "rich (>=10,<14)"]
http2 = ["h2 (>=3,<5)"]
socks = ["socksio (==1.*)"]
zstd = ["zstandard (>=0.18.0)"]

[[package]]
name = "httpx-sse"
version = "0.4.1"
description = "Consume Server-Sent Event (SSE) messages with HTTPX."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "httpx_sse-0.4.1-py3-none-any.whl", hash = "sha256:cba42174344c3a5b06f255ce65b350880f962d99ead85e776f23c6618a377a37"},
    {file = "httpx_sse-0.4.1.tar.gz", hash = "sha256:8f44d34414bc7b21bf3602713005c5df4917884f76072479b21f68befa4ea26e"},
]

[[package]]
name = "identify"
version = "2.6.13"
description = "File identification library for Python"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "identify-2.6.13-py2.py3-none-any.whl", hash = "sha256:60381139b3ae39447482ecc406944190f690d4a2997f2584062089848361b33b"},
    {file = "identify-2.6.13.tar.gz", hash = "sha256:da8d6c828e773620e13bfa86ea601c5a5310ba4bcd65edf378198b56a1f9fb32"},
]

[package.extras]
license = ["ukkonen"]

[[package]]
name = "idna"
version = "3.10"
description = "Internationalized Domain Names in Applications (IDNA)"
optional = false
python-versions = ">=3.6"
groups = ["main"]
files = [
    {file = "idna-3.10-py3-none-any.whl", hash = "sha256:946d195a0d259cbba61165e88e65941f16e9b36ea6ddb97f00452bae8b1287d3"},
    {file = "idna-3.10.tar.gz", hash = "sha256:12f65c9b470abda6dc35cf8e63cc574b1c52b11df2c86030af0ac09b01b13ea9"},
]

[package.extras]
all = ["flake8 (>=7.1.1)", "mypy (>=1.11.2)", "pytest (>=8.3.2)", "ruff (>=0.6.2)"]

[[package]]
name = "importlib-metadata"
version = "8.7.0"
description = "Read metadata from Python packages"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "importlib_metadata-8.7.0-py3-none-any.whl", hash = "sha256:e5dd1551894c77868a30651cef00984d50e1002d06942a7101d34870c5f02afd"},
    {file = "importlib_metadata-8.7.0.tar.gz", hash = "sha256:d13b81ad223b890aa16c5471f2ac3056cf76c5f10f82d6f9292f0b415f389000"},
]

[package.dependencies]
zipp = ">=3.20"

[package.extras]
check = ["pytest-checkdocs (>=2.4)", "pytest-ruff (>=0.2.1) ; sys_platform != \"cygwin\""]
cover = ["pytest-cov"]
doc = ["furo", "jaraco.packaging (>=9.3)", "jaraco.tidelift (>=1.4)", "rst.linker (>=1.9)", "sphinx (>=3.5)", "sphinx-lint"]
enabler = ["pytest-enabler (>=2.2)"]
perf = ["ipython"]
test = ["flufl.flake8", "importlib_resources (>=1.3) ; python_version < \"3.9\"", "jaraco.test (>=5.4)", "packaging", "pyfakefs", "pytest (>=6,!=8.1.*)", "pytest-perf (>=0.9.2)"]
type = ["pytest-mypy"]

[[package]]
name = "iniconfig"
version = "2.1.0"
description = "brain-dead simple config-ini parsing"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "iniconfig-2.1.0-py3-none-any.whl", hash = "sha256:9deba5723312380e77435581c6bf4935c94cbfab9b1ed33ef8d238ea168eb760"},
    {file = "iniconfig-2.1.0.tar.gz", hash = "sha256:3abbd2e30b36733fee78f9c7f7308f2d0050e88f0087fd25c2645f63c773e1c7"},
]

[[package]]
name = "jsonschema"
version = "4.25.1"
description = "An implementation of JSON Schema validation for Python"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "jsonschema-4.25.1-py3-none-any.whl", hash = "sha256:3fba0169e345c7175110351d456342c364814cfcf3b964ba4587f22915230a63"},
    {file = "jsonschema-4.25.1.tar.gz", hash = "sha256:e4a9655ce0da0c0b67a085847e00a3a51449e1157f4f75e9fb5aa545e122eb85"},
]

[package.dependencies]
attrs = ">=22.2.0"
jsonschema-specifications = ">=2023.03.6"
referencing = ">=0.28.4"
rpds-py = ">=0.7.1"

[package.extras]
format = ["fqdn", "idna", "isoduration", "jsonpointer (>1.13)", "rfc3339-validator", "rfc3987", "uri-template", "webcolors (>=1.11)"]
format-nongpl = ["fqdn", "idna", "isoduration", "jsonpointer (>1.13)", "rfc3339-validator", "rfc3986-validator (>0.1.0)", "rfc3987-syntax (>=1.1.0)", "uri-template", "webcolors (>=24.6.0)"]

[[package]]
name = "jsonschema-specifications"
version = "2025.4.1"
description = "The JSON Schema meta-schemas and vocabularies, exposed as a Registry"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "jsonschema_specifications-2025.4.1-py3-none-any.whl", hash = "sha256:4653bffbd6584f7de83a67e0d620ef16900b390ddc7939d56684d6c81e33f1af"},
    {file = "jsonschema_specifications-2025.4.1.tar.gz", hash = "sha256:630159c9f4dbea161a6a2205c3011cc4f18ff381b189fff48bb39b9bf26ae608"},
]

[package.dependencies]
referencing = ">=0.31.0"

[[package]]
name = "langfuse"
version = "3.3.4"
description = "A client library for accessing langfuse"
optional = false
python-versions = "<4.0,>=3.9"
groups = ["main"]
files = [
    {file = "langfuse-3.3.4-py3-none-any.whl", hash = "sha256:15b9d20878cf39a48ca9cfa7e52acdfeb043603d3a9cef8cf451687a4d838c6b"},
    {file = "langfuse-3.3.4.tar.gz", hash = "sha256:e5df4e7284298990b522e02a1dc6c3c72ebc4a7a411dc7d39255fb8c2e5a7c3a"},
]

[package.dependencies]
backoff = ">=1.10.0"
httpx = ">=0.15.4,<1.0"
opentelemetry-api = ">=1.33.1,<2.0.0"
opentelemetry-exporter-otlp-proto-http = ">=1.33.1,<2.0.0"
opentelemetry-sdk = ">=1.33.1,<2.0.0"
packaging = ">=23.2,<26.0"
pydantic = ">=1.10.7,<3.0"
requests = ">=2,<3"
wrapt = ">=1.14,<2.0"

[package.extras]
langchain = ["langchain (>=0.0.309)"]
openai = ["openai (>=0.27.8)"]

[[package]]
name = "mcp"
version = "1.13.1"
description = "Model Context Protocol SDK"
optional = false
python-versions = ">=3.10"
groups = ["main"]
files = [
    {file = "mcp-1.13.1-py3-none-any.whl", hash = "sha256:c314e7c8bd477a23ba3ef472ee5a32880316c42d03e06dcfa31a1cc7a73b65df"},
    {file = "mcp-1.13.1.tar.gz", hash = "sha256:165306a8fd7991dc80334edd2de07798175a56461043b7ae907b279794a834c5"},
]

[package.dependencies]
anyio = ">=4.5"
httpx = ">=0.27.1"
httpx-sse = ">=0.4"
jsonschema = ">=4.20.0"
pydantic = ">=2.11.0,<3.0.0"
pydantic-settings = ">=2.5.2"
python-multipart = ">=0.0.9"
pywin32 = {version = ">=310", markers = "sys_platform == \"win32\""}
sse-starlette = ">=1.6.1"
starlette = ">=0.27"
uvicorn = {version = ">=0.31.1", markers = "sys_platform != \"emscripten\""}

[package.extras]
cli = ["python-dotenv (>=1.0.0)", "typer (>=0.16.0)"]
rich = ["rich (>=13.9.4)"]
ws = ["websockets (>=15.0.1)"]

[[package]]
name = "mypy"
version = "1.17.1"
description = "Optional static typing for Python"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "mypy-1.17.1-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:3fbe6d5555bf608c47203baa3e72dbc6ec9965b3d7c318aa9a4ca76f465bd972"},
    {file = "mypy-1.17.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:80ef5c058b7bce08c83cac668158cb7edea692e458d21098c7d3bce35a5d43e7"},
    {file = "mypy-1.17.1-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:c4a580f8a70c69e4a75587bd925d298434057fe2a428faaf927ffe6e4b9a98df"},
    {file = "mypy-1.17.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:dd86bb649299f09d987a2eebb4d52d10603224500792e1bee18303bbcc1ce390"},
    {file = "mypy-1.17.1-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:a76906f26bd8d51ea9504966a9c25419f2e668f012e0bdf3da4ea1526c534d94"},
    {file = "mypy-1.17.1-cp310-cp310-win_amd64.whl", hash = "sha256:e79311f2d904ccb59787477b7bd5d26f3347789c06fcd7656fa500875290264b"},
    {file = "mypy-1.17.1-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:ad37544be07c5d7fba814eb370e006df58fed8ad1ef33ed1649cb1889ba6ff58"},
    {file = "mypy-1.17.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:064e2ff508e5464b4bd807a7c1625bc5047c5022b85c70f030680e18f37273a5"},
    {file = "mypy-1.17.1-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:70401bbabd2fa1aa7c43bb358f54037baf0586f41e83b0ae67dd0534fc64edfd"},
    {file = "mypy-1.17.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:e92bdc656b7757c438660f775f872a669b8ff374edc4d18277d86b63edba6b8b"},
    {file = "mypy-1.17.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:c1fdf4abb29ed1cb091cf432979e162c208a5ac676ce35010373ff29247bcad5"},
    {file = "mypy-1.17.1-cp311-cp311-win_amd64.whl", hash = "sha256:ff2933428516ab63f961644bc49bc4cbe42bbffb2cd3b71cc7277c07d16b1a8b"},
    {file = "mypy-1.17.1-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:69e83ea6553a3ba79c08c6e15dbd9bfa912ec1e493bf75489ef93beb65209aeb"},
    {file = "mypy-1.17.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:1b16708a66d38abb1e6b5702f5c2c87e133289da36f6a1d15f6a5221085c6403"},
    {file = "mypy-1.17.1-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:89e972c0035e9e05823907ad5398c5a73b9f47a002b22359b177d40bdaee7056"},
    {file = "mypy-1.17.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:03b6d0ed2b188e35ee6d5c36b5580cffd6da23319991c49ab5556c023ccf1341"},
    {file = "mypy-1.17.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:c837b896b37cd103570d776bda106eabb8737aa6dd4f248451aecf53030cdbeb"},
    {file = "mypy-1.17.1-cp312-cp312-win_amd64.whl", hash = "sha256:665afab0963a4b39dff7c1fa563cc8b11ecff7910206db4b2e64dd1ba25aed19"},
    {file = "mypy-1.17.1-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:93378d3203a5c0800c6b6d850ad2f19f7a3cdf1a3701d3416dbf128805c6a6a7"},
    {file = "mypy-1.17.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:15d54056f7fe7a826d897789f53dd6377ec2ea8ba6f776dc83c2902b899fee81"},
    {file = "mypy-1.17.1-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:209a58fed9987eccc20f2ca94afe7257a8f46eb5df1fb69958650973230f91e6"},
    {file = "mypy-1.17.1-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:099b9a5da47de9e2cb5165e581f158e854d9e19d2e96b6698c0d64de911dd849"},
    {file = "mypy-1.17.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:fa6ffadfbe6994d724c5a1bb6123a7d27dd68fc9c059561cd33b664a79578e14"},
    {file = "mypy-1.17.1-cp313-cp313-win_amd64.whl", hash = "sha256:9a2b7d9180aed171f033c9f2fc6c204c1245cf60b0cb61cf2e7acc24eea78e0a"},
    {file = "mypy-1.17.1-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:15a83369400454c41ed3a118e0cc58bd8123921a602f385cb6d6ea5df050c733"},
    {file = "mypy-1.17.1-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:55b918670f692fc9fba55c3298d8a3beae295c5cded0a55dccdc5bbead814acd"},
    {file = "mypy-1.17.1-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:62761474061feef6f720149d7ba876122007ddc64adff5ba6f374fda35a018a0"},
    {file = "mypy-1.17.1-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:c49562d3d908fd49ed0938e5423daed8d407774a479b595b143a3d7f87cdae6a"},
    {file = "mypy-1.17.1-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:397fba5d7616a5bc60b45c7ed204717eaddc38f826e3645402c426057ead9a91"},
    {file = "mypy-1.17.1-cp314-cp314-win_amd64.whl", hash = "sha256:9d6b20b97d373f41617bd0708fd46aa656059af57f2ef72aa8c7d6a2b73b74ed"},
    {file = "mypy-1.17.1-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:5d1092694f166a7e56c805caaf794e0585cabdbf1df36911c414e4e9abb62ae9"},
    {file = "mypy-1.17.1-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:79d44f9bfb004941ebb0abe8eff6504223a9c1ac51ef967d1263c6572bbebc99"},
    {file = "mypy-1.17.1-cp39-cp39-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:b01586eed696ec905e61bd2568f48740f7ac4a45b3a468e6423a03d3788a51a8"},
    {file = "mypy-1.17.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:43808d9476c36b927fbcd0b0255ce75efe1b68a080154a38ae68a7e62de8f0f8"},
    {file = "mypy-1.17.1-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:feb8cc32d319edd5859da2cc084493b3e2ce5e49a946377663cc90f6c15fb259"},
    {file = "mypy-1.17.1-cp39-cp39-win_amd64.whl", hash = "sha256:d7598cf74c3e16539d4e2f0b8d8c318e00041553d83d4861f87c7a72e95ac24d"},
    {file = "mypy-1.17.1-py3-none-any.whl", hash = "sha256:a9f52c0351c21fe24c21d8c0eb1f62967b262d6729393397b6f443c3b773c3b9"},
    {file = "mypy-1.17.1.tar.gz", hash = "sha256:25e01ec741ab5bb3eec8ba9cdb0f769230368a22c959c4937360efb89b7e9f01"},
]

[package.dependencies]
mypy_extensions = ">=1.0.0"
pathspec = ">=0.9.0"
typing_extensions = ">=4.6.0"

[package.extras]
dmypy = ["psutil (>=4.0)"]
faster-cache = ["orjson"]
install-types = ["pip"]
mypyc = ["setuptools (>=50)"]
reports = ["lxml"]

[[package]]
name = "mypy-extensions"
version = "1.1.0"
description = "Type system extensions for programs checked with the mypy type checker."
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "mypy_extensions-1.1.0-py3-none-any.whl", hash = "sha256:1be4cccdb0f2482337c4743e60421de3a356cd97508abadd57d47403e94f5505"},
    {file = "mypy_extensions-1.1.0.tar.gz", hash = "sha256:52e68efc3284861e772bbcd66823fde5ae21fd2fdb51c62a211403730b916558"},
]

[[package]]
name = "nodeenv"
version = "1.9.1"
description = "Node.js virtual environment builder"
optional = false
python-versions = "!=3.0.*,!=3.1.*,!=3.2.*,!=3.3.*,!=3.4.*,!=3.5.*,!=3.6.*,>=2.7"
groups = ["main"]
files = [
    {file = "nodeenv-1.9.1-py2.py3-none-any.whl", hash = "sha256:ba11c9782d29c27c70ffbdda2d7415098754709be8a7056d79a737cd901155c9"},
    {file = "nodeenv-1.9.1.tar.gz", hash = "sha256:6ec12890a2dab7946721edbfbcd91f3319c6ccc9aec47be7c7e6b7011ee6645f"},
]

[[package]]
name = "opentelemetry-api"
version = "1.36.0"
description = "OpenTelemetry Python API"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "opentelemetry_api-1.36.0-py3-none-any.whl", hash = "sha256:02f20bcacf666e1333b6b1f04e647dc1d5111f86b8e510238fcc56d7762cda8c"},
    {file = "opentelemetry_api-1.36.0.tar.gz", hash = "sha256:9a72572b9c416d004d492cbc6e61962c0501eaf945ece9b5a0f56597d8348aa0"},
]

[package.dependencies]
importlib-metadata = ">=6.0,<8.8.0"
typing-extensions = ">=4.5.0"

[[package]]
name = "opentelemetry-exporter-otlp-proto-common"
version = "1.36.0"
description = "OpenTelemetry Protobuf encoding"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl", hash = "sha256:0fc002a6ed63eac235ada9aa7056e5492e9a71728214a61745f6ad04b923f840"},
    {file = "opentelemetry_exporter_otlp_proto_common-1.36.0.tar.gz", hash = "sha256:6c496ccbcbe26b04653cecadd92f73659b814c6e3579af157d8716e5f9f25cbf"},
]

[package.dependencies]
opentelemetry-proto = "1.36.0"

[[package]]
name = "opentelemetry-exporter-otlp-proto-http"
version = "1.36.0"
description = "OpenTelemetry Collector Protobuf over HTTP Exporter"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl", hash = "sha256:3d769f68e2267e7abe4527f70deb6f598f40be3ea34c6adc35789bea94a32902"},
    {file = "opentelemetry_exporter_otlp_proto_http-1.36.0.tar.gz", hash = "sha256:dd3637f72f774b9fc9608ab1ac479f8b44d09b6fb5b2f3df68a24ad1da7d356e"},
]

[package.dependencies]
googleapis-common-protos = ">=1.52,<2.0"
opentelemetry-api = ">=1.15,<2.0"
opentelemetry-exporter-otlp-proto-common = "1.36.0"
opentelemetry-proto = "1.36.0"
opentelemetry-sdk = ">=1.36.0,<1.37.0"
requests = ">=2.7,<3.0"
typing-extensions = ">=4.5.0"

[[package]]
name = "opentelemetry-proto"
version = "1.36.0"
description = "OpenTelemetry Python Proto"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "opentelemetry_proto-1.36.0-py3-none-any.whl", hash = "sha256:151b3bf73a09f94afc658497cf77d45a565606f62ce0c17acb08cd9937ca206e"},
    {file = "opentelemetry_proto-1.36.0.tar.gz", hash = "sha256:0f10b3c72f74c91e0764a5ec88fd8f1c368ea5d9c64639fb455e2854ef87dd2f"},
]

[package.dependencies]
protobuf = ">=5.0,<7.0"

[[package]]
name = "opentelemetry-sdk"
version = "1.36.0"
description = "OpenTelemetry Python SDK"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "opentelemetry_sdk-1.36.0-py3-none-any.whl", hash = "sha256:19fe048b42e98c5c1ffe85b569b7073576ad4ce0bcb6e9b4c6a39e890a6c45fb"},
    {file = "opentelemetry_sdk-1.36.0.tar.gz", hash = "sha256:19c8c81599f51b71670661ff7495c905d8fdf6976e41622d5245b791b06fa581"},
]

[package.dependencies]
opentelemetry-api = "1.36.0"
opentelemetry-semantic-conventions = "0.57b0"
typing-extensions = ">=4.5.0"

[[package]]
name = "opentelemetry-semantic-conventions"
version = "0.57b0"
description = "OpenTelemetry Semantic Conventions"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl", hash = "sha256:757f7e76293294f124c827e514c2a3144f191ef175b069ce8d1211e1e38e9e78"},
    {file = "opentelemetry_semantic_conventions-0.57b0.tar.gz", hash = "sha256:609a4a79c7891b4620d64c7aac6898f872d790d75f22019913a660756f27ff32"},
]

[package.dependencies]
opentelemetry-api = "1.36.0"
typing-extensions = ">=4.5.0"

[[package]]
name = "packaging"
version = "25.0"
description = "Core utilities for Python packages"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "packaging-25.0-py3-none-any.whl", hash = "sha256:29572ef2b1f17581046b3a2227d5c611fb25ec70ca1ba8554b24b0e69331a484"},
    {file = "packaging-25.0.tar.gz", hash = "sha256:d443872c98d677bf60f6a1f2f8c1cb748e8fe762d2bf9d3148b5599295b0fc4f"},
]

[[package]]
name = "pathspec"
version = "0.12.1"
description = "Utility library for gitignore style pattern matching of file paths."
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "pathspec-0.12.1-py3-none-any.whl", hash = "sha256:a0d503e138a4c123b27490a4f7beda6a01c6f288df0e4a8b79c7eb0dc7b4cc08"},
    {file = "pathspec-0.12.1.tar.gz", hash = "sha256:a482d51503a1ab33b1c67a6c3813a26953dbdc71c31dacaef9a838c4e29f5712"},
]

[[package]]
name = "platformdirs"
version = "4.3.8"
description = "A small Python package for determining appropriate platform-specific dirs, e.g. a `user data dir`."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "platformdirs-4.3.8-py3-none-any.whl", hash = "sha256:ff7059bb7eb1179e2685604f4aaf157cfd9535242bd23742eadc3c13542139b4"},
    {file = "platformdirs-4.3.8.tar.gz", hash = "sha256:3d512d96e16bcb959a814c9f348431070822a6496326a4be0911c40b5a74c2bc"},
]

[package.extras]
docs = ["furo (>=2024.8.6)", "proselint (>=0.14)", "sphinx (>=8.1.3)", "sphinx-autodoc-typehints (>=3)"]
test = ["appdirs (==1.4.4)", "covdefaults (>=2.3)", "pytest (>=8.3.4)", "pytest-cov (>=6)", "pytest-mock (>=3.14)"]
type = ["mypy (>=1.14.1)"]

[[package]]
name = "pluggy"
version = "1.6.0"
description = "plugin and hook calling mechanisms for python"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "pluggy-1.6.0-py3-none-any.whl", hash = "sha256:e920276dd6813095e9377c0bc5566d94c932c33b27a3e3945d8389c374dd4746"},
    {file = "pluggy-1.6.0.tar.gz", hash = "sha256:7dcc130b76258d33b90f61b658791dede3486c3e6bfb003ee5c9bfb396dd22f3"},
]

[package.extras]
dev = ["pre-commit", "tox"]
testing = ["coverage", "pytest", "pytest-benchmark"]

[[package]]
name = "pre-commit"
version = "3.8.0"
description = "A framework for managing and maintaining multi-language pre-commit hooks."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "pre_commit-3.8.0-py2.py3-none-any.whl", hash = "sha256:9a90a53bf82fdd8778d58085faf8d83df56e40dfe18f45b19446e26bf1b3a63f"},
    {file = "pre_commit-3.8.0.tar.gz", hash = "sha256:8bb6494d4a20423842e198980c9ecf9f96607a07ea29549e180eef9ae80fe7af"},
]

[package.dependencies]
cfgv = ">=2.0.0"
identify = ">=1.0.0"
nodeenv = ">=0.11.1"
pyyaml = ">=5.1"
virtualenv = ">=20.10.0"

[[package]]
name = "protobuf"
version = "6.32.0"
description = ""
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "protobuf-6.32.0-cp310-abi3-win32.whl", hash = "sha256:84f9e3c1ff6fb0308dbacb0950d8aa90694b0d0ee68e75719cb044b7078fe741"},
    {file = "protobuf-6.32.0-cp310-abi3-win_amd64.whl", hash = "sha256:a8bdbb2f009cfc22a36d031f22a625a38b615b5e19e558a7b756b3279723e68e"},
    {file = "protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl", hash = "sha256:d52691e5bee6c860fff9a1c86ad26a13afbeb4b168cd4445c922b7e2cf85aaf0"},
    {file = "protobuf-6.32.0-cp39-abi3-manylinux2014_aarch64.whl", hash = "sha256:501fe6372fd1c8ea2a30b4d9be8f87955a64d6be9c88a973996cef5ef6f0abf1"},
    {file = "protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl", hash = "sha256:75a2aab2bd1aeb1f5dc7c5f33bcb11d82ea8c055c9becbb41c26a8c43fd7092c"},
    {file = "protobuf-6.32.0-cp39-cp39-win32.whl", hash = "sha256:7db8ed09024f115ac877a1427557b838705359f047b2ff2f2b2364892d19dacb"},
    {file = "protobuf-6.32.0-cp39-cp39-win_amd64.whl", hash = "sha256:15eba1b86f193a407607112ceb9ea0ba9569aed24f93333fe9a497cf2fda37d3"},
    {file = "protobuf-6.32.0-py3-none-any.whl", hash = "sha256:ba377e5b67b908c8f3072a57b63e2c6a4cbd18aea4ed98d2584350dbf46f2783"},
    {file = "protobuf-6.32.0.tar.gz", hash = "sha256:a81439049127067fc49ec1d36e25c6ee1d1a2b7be930675f919258d03c04e7d2"},
]

[[package]]
name = "pydantic"
version = "2.11.7"
description = "Data validation using Python type hints"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "pydantic-2.11.7-py3-none-any.whl", hash = "sha256:dde5df002701f6de26248661f6835bbe296a47bf73990135c7d07ce741b9623b"},
    {file = "pydantic-2.11.7.tar.gz", hash = "sha256:d989c3c6cb79469287b1569f7447a17848c998458d49ebe294e975b9baf0f0db"},
]

[package.dependencies]
annotated-types = ">=0.6.0"
pydantic-core = "2.33.2"
typing-extensions = ">=4.12.2"
typing-inspection = ">=0.4.0"

[package.extras]
email = ["email-validator (>=2.0.0)"]
timezone = ["tzdata ; python_version >= \"3.9\" and platform_system == \"Windows\""]

[[package]]
name = "pydantic-core"
version = "2.33.2"
description = "Core functionality for Pydantic validation and serialization"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "pydantic_core-2.33.2-cp310-cp310-macosx_10_12_x86_64.whl", hash = "sha256:2b3d326aaef0c0399d9afffeb6367d5e26ddc24d351dbc9c636840ac355dc5d8"},
    {file = "pydantic_core-2.33.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:0e5b2671f05ba48b94cb90ce55d8bdcaaedb8ba00cc5359f6810fc918713983d"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0069c9acc3f3981b9ff4cdfaf088e98d83440a4c7ea1bc07460af3d4dc22e72d"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:d53b22f2032c42eaaf025f7c40c2e3b94568ae077a606f006d206a463bc69572"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:0405262705a123b7ce9f0b92f123334d67b70fd1f20a9372b907ce1080c7ba02"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:4b25d91e288e2c4e0662b8038a28c6a07eaac3e196cfc4ff69de4ea3db992a1b"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:6bdfe4b3789761f3bcb4b1ddf33355a71079858958e3a552f16d5af19768fef2"},
    {file = "pydantic_core-2.33.2-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:efec8db3266b76ef9607c2c4c419bdb06bf335ae433b80816089ea7585816f6a"},
    {file = "pydantic_core-2.33.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:031c57d67ca86902726e0fae2214ce6770bbe2f710dc33063187a68744a5ecac"},
    {file = "pydantic_core-2.33.2-cp310-cp310-musllinux_1_1_armv7l.whl", hash = "sha256:f8de619080e944347f5f20de29a975c2d815d9ddd8be9b9b7268e2e3ef68605a"},
    {file = "pydantic_core-2.33.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:73662edf539e72a9440129f231ed3757faab89630d291b784ca99237fb94db2b"},
    {file = "pydantic_core-2.33.2-cp310-cp310-win32.whl", hash = "sha256:0a39979dcbb70998b0e505fb1556a1d550a0781463ce84ebf915ba293ccb7e22"},
    {file = "pydantic_core-2.33.2-cp310-cp310-win_amd64.whl", hash = "sha256:b0379a2b24882fef529ec3b4987cb5d003b9cda32256024e6fe1586ac45fc640"},
    {file = "pydantic_core-2.33.2-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:4c5b0a576fb381edd6d27f0a85915c6daf2f8138dc5c267a57c08a62900758c7"},
    {file = "pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:e799c050df38a639db758c617ec771fd8fb7a5f8eaaa4b27b101f266b216a246"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:dc46a01bf8d62f227d5ecee74178ffc448ff4e5197c756331f71efcc66dc980f"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:a144d4f717285c6d9234a66778059f33a89096dfb9b39117663fd8413d582dcc"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:73cf6373c21bc80b2e0dc88444f41ae60b2f070ed02095754eb5a01df12256de"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3dc625f4aa79713512d1976fe9f0bc99f706a9dee21dfd1810b4bbbf228d0e8a"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:881b21b5549499972441da4758d662aeea93f1923f953e9cbaff14b8b9565aef"},
    {file = "pydantic_core-2.33.2-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:bdc25f3681f7b78572699569514036afe3c243bc3059d3942624e936ec93450e"},
    {file = "pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:fe5b32187cbc0c862ee201ad66c30cf218e5ed468ec8dc1cf49dec66e160cc4d"},
    {file = "pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_armv7l.whl", hash = "sha256:bc7aee6f634a6f4a95676fcb5d6559a2c2a390330098dba5e5a5f28a2e4ada30"},
    {file = "pydantic_core-2.33.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:235f45e5dbcccf6bd99f9f472858849f73d11120d76ea8707115415f8e5ebebf"},
    {file = "pydantic_core-2.33.2-cp311-cp311-win32.whl", hash = "sha256:6368900c2d3ef09b69cb0b913f9f8263b03786e5b2a387706c5afb66800efd51"},
    {file = "pydantic_core-2.33.2-cp311-cp311-win_amd64.whl", hash = "sha256:1e063337ef9e9820c77acc768546325ebe04ee38b08703244c1309cccc4f1bab"},
    {file = "pydantic_core-2.33.2-cp311-cp311-win_arm64.whl", hash = "sha256:6b99022f1d19bc32a4c2a0d544fc9a76e3be90f0b3f4af413f87d38749300e65"},
    {file = "pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:a7ec89dc587667f22b6a0b6579c249fca9026ce7c333fc142ba42411fa243cdc"},
    {file = "pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:3c6db6e52c6d70aa0d00d45cdb9b40f0433b96380071ea80b09277dba021ddf7"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4e61206137cbc65e6d5256e1166f88331d3b6238e082d9f74613b9b765fb9025"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:eb8c529b2819c37140eb51b914153063d27ed88e3bdc31b71198a198e921e011"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:c52b02ad8b4e2cf14ca7b3d918f3eb0ee91e63b3167c32591e57c4317e134f8f"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:96081f1605125ba0855dfda83f6f3df5ec90c61195421ba72223de35ccfb2f88"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8f57a69461af2a5fa6e6bbd7a5f60d3b7e6cebb687f55106933188e79ad155c1"},
    {file = "pydantic_core-2.33.2-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:572c7e6c8bb4774d2ac88929e3d1f12bc45714ae5ee6d9a788a9fb35e60bb04b"},
    {file = "pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:db4b41f9bd95fbe5acd76d89920336ba96f03e149097365afe1cb092fceb89a1"},
    {file = "pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_armv7l.whl", hash = "sha256:fa854f5cf7e33842a892e5c73f45327760bc7bc516339fda888c75ae60edaeb6"},
    {file = "pydantic_core-2.33.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:5f483cfb75ff703095c59e365360cb73e00185e01aaea067cd19acffd2ab20ea"},
    {file = "pydantic_core-2.33.2-cp312-cp312-win32.whl", hash = "sha256:9cb1da0f5a471435a7bc7e439b8a728e8b61e59784b2af70d7c169f8dd8ae290"},
    {file = "pydantic_core-2.33.2-cp312-cp312-win_amd64.whl", hash = "sha256:f941635f2a3d96b2973e867144fde513665c87f13fe0e193c158ac51bfaaa7b2"},
    {file = "pydantic_core-2.33.2-cp312-cp312-win_arm64.whl", hash = "sha256:cca3868ddfaccfbc4bfb1d608e2ccaaebe0ae628e1416aeb9c4d88c001bb45ab"},
    {file = "pydantic_core-2.33.2-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:1082dd3e2d7109ad8b7da48e1d4710c8d06c253cbc4a27c1cff4fbcaa97a9e3f"},
    {file = "pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:f517ca031dfc037a9c07e748cefd8d96235088b83b4f4ba8939105d20fa1dcd6"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0a9f2c9dd19656823cb8250b0724ee9c60a82f3cdf68a080979d13092a3b0fef"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:2b0a451c263b01acebe51895bfb0e1cc842a5c666efe06cdf13846c7418caa9a"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:1ea40a64d23faa25e62a70ad163571c0b342b8bf66d5fa612ac0dec4f069d916"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0fb2d542b4d66f9470e8065c5469ec676978d625a8b7a363f07d9a501a9cb36a"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9fdac5d6ffa1b5a83bca06ffe7583f5576555e6c8b3a91fbd25ea7780f825f7d"},
    {file = "pydantic_core-2.33.2-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:04a1a413977ab517154eebb2d326da71638271477d6ad87a769102f7c2488c56"},
    {file = "pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:c8e7af2f4e0194c22b5b37205bfb293d166a7344a5b0d0eaccebc376546d77d5"},
    {file = "pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_armv7l.whl", hash = "sha256:5c92edd15cd58b3c2d34873597a1e20f13094f59cf88068adb18947df5455b4e"},
    {file = "pydantic_core-2.33.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:65132b7b4a1c0beded5e057324b7e16e10910c106d43675d9bd87d4f38dde162"},
    {file = "pydantic_core-2.33.2-cp313-cp313-win32.whl", hash = "sha256:52fb90784e0a242bb96ec53f42196a17278855b0f31ac7c3cc6f5c1ec4811849"},
    {file = "pydantic_core-2.33.2-cp313-cp313-win_amd64.whl", hash = "sha256:c083a3bdd5a93dfe480f1125926afcdbf2917ae714bdb80b36d34318b2bec5d9"},
    {file = "pydantic_core-2.33.2-cp313-cp313-win_arm64.whl", hash = "sha256:e80b087132752f6b3d714f041ccf74403799d3b23a72722ea2e6ba2e892555b9"},
    {file = "pydantic_core-2.33.2-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:61c18fba8e5e9db3ab908620af374db0ac1baa69f0f32df4f61ae23f15e586ac"},
    {file = "pydantic_core-2.33.2-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:95237e53bb015f67b63c91af7518a62a8660376a6a0db19b89acc77a4d6199f5"},
    {file = "pydantic_core-2.33.2-cp313-cp313t-win_amd64.whl", hash = "sha256:c2fc0a768ef76c15ab9238afa6da7f69895bb5d1ee83aeea2e3509af4472d0b9"},
    {file = "pydantic_core-2.33.2-cp39-cp39-macosx_10_12_x86_64.whl", hash = "sha256:a2b911a5b90e0374d03813674bf0a5fbbb7741570dcd4b4e85a2e48d17def29d"},
    {file = "pydantic_core-2.33.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:6fa6dfc3e4d1f734a34710f391ae822e0a8eb8559a85c6979e14e65ee6ba2954"},
    {file = "pydantic_core-2.33.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:c54c939ee22dc8e2d545da79fc5381f1c020d6d3141d3bd747eab59164dc89fb"},
    {file = "pydantic_core-2.33.2-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:53a57d2ed685940a504248187d5685e49eb5eef0f696853647bf37c418c538f7"},
    {file = "pydantic_core-2.33.2-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:09fb9dd6571aacd023fe6aaca316bd01cf60ab27240d7eb39ebd66a3a15293b4"},
    {file = "pydantic_core-2.33.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:0e6116757f7959a712db11f3e9c0a99ade00a5bbedae83cb801985aa154f071b"},
    {file = "pydantic_core-2.33.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:8d55ab81c57b8ff8548c3e4947f119551253f4e3787a7bbc0b6b3ca47498a9d3"},
    {file = "pydantic_core-2.33.2-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:c20c462aa4434b33a2661701b861604913f912254e441ab8d78d30485736115a"},
    {file = "pydantic_core-2.33.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:44857c3227d3fb5e753d5fe4a3420d6376fa594b07b621e220cd93703fe21782"},
    {file = "pydantic_core-2.33.2-cp39-cp39-musllinux_1_1_armv7l.whl", hash = "sha256:eb9b459ca4df0e5c87deb59d37377461a538852765293f9e6ee834f0435a93b9"},
    {file = "pydantic_core-2.33.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:9fcd347d2cc5c23b06de6d3b7b8275be558a0c90549495c699e379a80bf8379e"},
    {file = "pydantic_core-2.33.2-cp39-cp39-win32.whl", hash = "sha256:83aa99b1285bc8f038941ddf598501a86f1536789740991d7d8756e34f1e74d9"},
    {file = "pydantic_core-2.33.2-cp39-cp39-win_amd64.whl", hash = "sha256:f481959862f57f29601ccced557cc2e817bce7533ab8e01a797a48b49c9692b3"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-macosx_10_12_x86_64.whl", hash = "sha256:5c4aa4e82353f65e548c476b37e64189783aa5384903bfea4f41580f255fddfa"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:d946c8bf0d5c24bf4fe333af284c59a19358aa3ec18cb3dc4370080da1e8ad29"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:87b31b6846e361ef83fedb187bb5b4372d0da3f7e28d85415efa92d6125d6e6d"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:aa9d91b338f2df0508606f7009fde642391425189bba6d8c653afd80fd6bb64e"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2058a32994f1fde4ca0480ab9d1e75a0e8c87c22b53a3ae66554f9af78f2fe8c"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:0e03262ab796d986f978f79c943fc5f620381be7287148b8010b4097f79a39ec"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:1a8695a8d00c73e50bff9dfda4d540b7dee29ff9b8053e38380426a85ef10052"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:fa754d1850735a0b0e03bcffd9d4b4343eb417e47196e4485d9cca326073a42c"},
    {file = "pydantic_core-2.33.2-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:a11c8d26a50bfab49002947d3d237abe4d9e4b5bdc8846a63537b6488e197808"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-macosx_10_12_x86_64.whl", hash = "sha256:dd14041875d09cc0f9308e37a6f8b65f5585cf2598a53aa0123df8b129d481f8"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-macosx_11_0_arm64.whl", hash = "sha256:d87c561733f66531dced0da6e864f44ebf89a8fba55f31407b00c2f7f9449593"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:2f82865531efd18d6e07a04a17331af02cb7a651583c418df8266f17a63c6612"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2bfb5112df54209d820d7bf9317c7a6c9025ea52e49f46b6a2060104bba37de7"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:64632ff9d614e5eecfb495796ad51b0ed98c453e447a76bcbeeb69615079fc7e"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:f889f7a40498cc077332c7ab6b4608d296d852182211787d4f3ee377aaae66e8"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:de4b83bb311557e439b9e186f733f6c645b9417c84e2eb8203f3f820a4b988bf"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:82f68293f055f51b51ea42fafc74b6aad03e70e191799430b90c13d643059ebb"},
    {file = "pydantic_core-2.33.2-pp311-pypy311_pp73-win_amd64.whl", hash = "sha256:329467cecfb529c925cf2bbd4d60d2c509bc2fb52a20c1045bf09bb70971a9c1"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-macosx_10_12_x86_64.whl", hash = "sha256:87acbfcf8e90ca885206e98359d7dca4bcbb35abdc0ff66672a293e1d7a19101"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-macosx_11_0_arm64.whl", hash = "sha256:7f92c15cd1e97d4b12acd1cc9004fa092578acfa57b67ad5e43a197175d01a64"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d3f26877a748dc4251cfcfda9dfb5f13fcb034f5308388066bcfe9031b63ae7d"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:dac89aea9af8cd672fa7b510e7b8c33b0bba9a43186680550ccf23020f32d535"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:970919794d126ba8645f3837ab6046fb4e72bbc057b3709144066204c19a455d"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-musllinux_1_1_aarch64.whl", hash = "sha256:3eb3fe62804e8f859c49ed20a8451342de53ed764150cb14ca71357c765dc2a6"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-musllinux_1_1_armv7l.whl", hash = "sha256:3abcd9392a36025e3bd55f9bd38d908bd17962cc49bc6da8e7e96285336e2bca"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-musllinux_1_1_x86_64.whl", hash = "sha256:3a1c81334778f9e3af2f8aeb7a960736e5cab1dfebfb26aabca09afd2906c039"},
    {file = "pydantic_core-2.33.2-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:2807668ba86cb38c6817ad9bc66215ab8584d1d304030ce4f0887336f28a5e27"},
    {file = "pydantic_core-2.33.2.tar.gz", hash = "sha256:7cb8bc3605c29176e1b105350d2e6474142d7c1bd1d9327c4a9bdb46bf827acc"},
]

[package.dependencies]
typing-extensions = ">=4.6.0,<4.7.0 || >4.7.0"

[[package]]
name = "pydantic-settings"
version = "2.10.1"
description = "Settings management using Pydantic"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "pydantic_settings-2.10.1-py3-none-any.whl", hash = "sha256:a60952460b99cf661dc25c29c0ef171721f98bfcb52ef8d9ea4c943d7c8cc796"},
    {file = "pydantic_settings-2.10.1.tar.gz", hash = "sha256:06f0062169818d0f5524420a360d632d5857b83cffd4d42fe29597807a1614ee"},
]

[package.dependencies]
pydantic = ">=2.7.0"
python-dotenv = ">=0.21.0"
typing-inspection = ">=0.4.0"

[package.extras]
aws-secrets-manager = ["boto3 (>=1.35.0)", "boto3-stubs[secretsmanager]"]
azure-key-vault = ["azure-identity (>=1.16.0)", "azure-keyvault-secrets (>=4.8.0)"]
gcp-secret-manager = ["google-cloud-secret-manager (>=2.23.1)"]
toml = ["tomli (>=2.0.1)"]
yaml = ["pyyaml (>=6.0.1)"]

[[package]]
name = "pyjwt"
version = "2.10.1"
description = "JSON Web Token implementation in Python"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "PyJWT-2.10.1-py3-none-any.whl", hash = "sha256:dcdd193e30abefd5debf142f9adfcdd2b58004e644f25406ffaebd50bd98dacb"},
    {file = "pyjwt-2.10.1.tar.gz", hash = "sha256:3cc5772eb20009233caf06e9d8a0577824723b44e6648ee0a2aedb6cf9381953"},
]

[package.extras]
crypto = ["cryptography (>=3.4.0)"]
dev = ["coverage[toml] (==5.0.4)", "cryptography (>=3.4.0)", "pre-commit", "pytest (>=6.0.0,<7.0.0)", "sphinx", "sphinx-rtd-theme", "zope.interface"]
docs = ["sphinx", "sphinx-rtd-theme", "zope.interface"]
tests = ["coverage[toml] (==5.0.4)", "pytest (>=6.0.0,<7.0.0)"]

[[package]]
name = "pytest"
version = "7.4.4"
description = "pytest: simple powerful testing with Python"
optional = false
python-versions = ">=3.7"
groups = ["main"]
files = [
    {file = "pytest-7.4.4-py3-none-any.whl", hash = "sha256:b090cdf5ed60bf4c45261be03239c2c1c22df034fbffe691abe93cd80cea01d8"},
    {file = "pytest-7.4.4.tar.gz", hash = "sha256:2cf0005922c6ace4a3e2ec8b4080eb0d9753fdc93107415332f50ce9e7994280"},
]

[package.dependencies]
colorama = {version = "*", markers = "sys_platform == \"win32\""}
iniconfig = "*"
packaging = "*"
pluggy = ">=0.12,<2.0"

[package.extras]
testing = ["argcomplete", "attrs (>=19.2.0)", "hypothesis (>=3.56)", "mock", "nose", "pygments (>=2.7.2)", "requests", "setuptools", "xmlschema"]

[[package]]
name = "python-dotenv"
version = "1.1.1"
description = "Read key-value pairs from a .env file and set them as environment variables"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "python_dotenv-1.1.1-py3-none-any.whl", hash = "sha256:31f23644fe2602f88ff55e1f5c79ba497e01224ee7737937930c448e4d0e24dc"},
    {file = "python_dotenv-1.1.1.tar.gz", hash = "sha256:a8a6399716257f45be6a007360200409fce5cda2661e3dec71d23dc15f6189ab"},
]

[package.extras]
cli = ["click (>=5.0)"]

[[package]]
name = "python-multipart"
version = "0.0.20"
description = "A streaming multipart parser for Python"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "python_multipart-0.0.20-py3-none-any.whl", hash = "sha256:8a62d3a8335e06589fe01f2a3e178cdcc632f3fbe0d492ad9ee0ec35aab1f104"},
    {file = "python_multipart-0.0.20.tar.gz", hash = "sha256:8dd0cab45b8e23064ae09147625994d090fa46f5b0d1e13af944c331a7fa9d13"},
]

[[package]]
name = "pywin32"
version = "311"
description = "Python for Window Extensions"
optional = false
python-versions = "*"
groups = ["main"]
markers = "sys_platform == \"win32\""
files = [
    {file = "pywin32-311-cp310-cp310-win32.whl", hash = "sha256:d03ff496d2a0cd4a5893504789d4a15399133fe82517455e78bad62efbb7f0a3"},
    {file = "pywin32-311-cp310-cp310-win_amd64.whl", hash = "sha256:797c2772017851984b97180b0bebe4b620bb86328e8a884bb626156295a63b3b"},
    {file = "pywin32-311-cp310-cp310-win_arm64.whl", hash = "sha256:0502d1facf1fed4839a9a51ccbcc63d952cf318f78ffc00a7e78528ac27d7a2b"},
    {file = "pywin32-311-cp311-cp311-win32.whl", hash = "sha256:184eb5e436dea364dcd3d2316d577d625c0351bf237c4e9a5fabbcfa5a58b151"},
    {file = "pywin32-311-cp311-cp311-win_amd64.whl", hash = "sha256:3ce80b34b22b17ccbd937a6e78e7225d80c52f5ab9940fe0506a1a16f3dab503"},
    {file = "pywin32-311-cp311-cp311-win_arm64.whl", hash = "sha256:a733f1388e1a842abb67ffa8e7aad0e70ac519e09b0f6a784e65a136ec7cefd2"},
    {file = "pywin32-311-cp312-cp312-win32.whl", hash = "sha256:750ec6e621af2b948540032557b10a2d43b0cee2ae9758c54154d711cc852d31"},
    {file = "pywin32-311-cp312-cp312-win_amd64.whl", hash = "sha256:b8c095edad5c211ff31c05223658e71bf7116daa0ecf3ad85f3201ea3190d067"},
    {file = "pywin32-311-cp312-cp312-win_arm64.whl", hash = "sha256:e286f46a9a39c4a18b319c28f59b61de793654af2f395c102b4f819e584b5852"},
    {file = "pywin32-311-cp313-cp313-win32.whl", hash = "sha256:f95ba5a847cba10dd8c4d8fefa9f2a6cf283b8b88ed6178fa8a6c1ab16054d0d"},
    {file = "pywin32-311-cp313-cp313-win_amd64.whl", hash = "sha256:718a38f7e5b058e76aee1c56ddd06908116d35147e133427e59a3983f703a20d"},
    {file = "pywin32-311-cp313-cp313-win_arm64.whl", hash = "sha256:7b4075d959648406202d92a2310cb990fea19b535c7f4a78d3f5e10b926eeb8a"},
    {file = "pywin32-311-cp314-cp314-win32.whl", hash = "sha256:b7a2c10b93f8986666d0c803ee19b5990885872a7de910fc460f9b0c2fbf92ee"},
    {file = "pywin32-311-cp314-cp314-win_amd64.whl", hash = "sha256:3aca44c046bd2ed8c90de9cb8427f581c479e594e99b5c0bb19b29c10fd6cb87"},
    {file = "pywin32-311-cp314-cp314-win_arm64.whl", hash = "sha256:a508e2d9025764a8270f93111a970e1d0fbfc33f4153b388bb649b7eec4f9b42"},
    {file = "pywin32-311-cp38-cp38-win32.whl", hash = "sha256:6c6f2969607b5023b0d9ce2541f8d2cbb01c4f46bc87456017cf63b73f1e2d8c"},
    {file = "pywin32-311-cp38-cp38-win_amd64.whl", hash = "sha256:c8015b09fb9a5e188f83b7b04de91ddca4658cee2ae6f3bc483f0b21a77ef6cd"},
    {file = "pywin32-311-cp39-cp39-win32.whl", hash = "sha256:aba8f82d551a942cb20d4a83413ccbac30790b50efb89a75e4f586ac0bb8056b"},
    {file = "pywin32-311-cp39-cp39-win_amd64.whl", hash = "sha256:e0c4cfb0621281fe40387df582097fd796e80430597cb9944f0ae70447bacd91"},
    {file = "pywin32-311-cp39-cp39-win_arm64.whl", hash = "sha256:62ea666235135fee79bb154e695f3ff67370afefd71bd7fea7512fc70ef31e3d"},
]

[[package]]
name = "pyyaml"
version = "6.0.2"
description = "YAML parser and emitter for Python"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "PyYAML-6.0.2-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:0a9a2848a5b7feac301353437eb7d5957887edbf81d56e903999a75a3d743086"},
    {file = "PyYAML-6.0.2-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:29717114e51c84ddfba879543fb232a6ed60086602313ca38cce623c1d62cfbf"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:8824b5a04a04a047e72eea5cec3bc266db09e35de6bdfe34c9436ac5ee27d237"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:7c36280e6fb8385e520936c3cb3b8042851904eba0e58d277dca80a5cfed590b"},
    {file = "PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:ec031d5d2feb36d1d1a24380e4db6d43695f3748343d99434e6f5f9156aaa2ed"},
    {file = "PyYAML-6.0.2-cp310-cp310-musllinux_1_1_aarch64.whl", hash = "sha256:936d68689298c36b53b29f23c6dbb74de12b4ac12ca6cfe0e047bedceea56180"},
    {file = "PyYAML-6.0.2-cp310-cp310-musllinux_1_1_x86_64.whl", hash = "sha256:23502f431948090f597378482b4812b0caae32c22213aecf3b55325e049a6c68"},
    {file = "PyYAML-6.0.2-cp310-cp310-win32.whl", hash = "sha256:2e99c6826ffa974fe6e27cdb5ed0021786b03fc98e5ee3c5bfe1fd5015f42b99"},
    {file = "PyYAML-6.0.2-cp310-cp310-win_amd64.whl", hash = "sha256:a4d3091415f010369ae4ed1fc6b79def9416358877534caf6a0fdd2146c87a3e"},
    {file = "PyYAML-6.0.2-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:cc1c1159b3d456576af7a3e4d1ba7e6924cb39de8f67111c735f6fc832082774"},
    {file = "PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:1e2120ef853f59c7419231f3bf4e7021f1b936f6ebd222406c3b60212205d2ee"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:5d225db5a45f21e78dd9358e58a98702a0302f2659a3c6cd320564b75b86f47c"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5ac9328ec4831237bec75defaf839f7d4564be1e6b25ac710bd1a96321cc8317"},
    {file = "PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3ad2a3decf9aaba3d29c8f537ac4b243e36bef957511b4766cb0057d32b0be85"},
    {file = "PyYAML-6.0.2-cp311-cp311-musllinux_1_1_aarch64.whl", hash = "sha256:ff3824dc5261f50c9b0dfb3be22b4567a6f938ccce4587b38952d85fd9e9afe4"},
    {file = "PyYAML-6.0.2-cp311-cp311-musllinux_1_1_x86_64.whl", hash = "sha256:797b4f722ffa07cc8d62053e4cff1486fa6dc094105d13fea7b1de7d8bf71c9e"},
    {file = "PyYAML-6.0.2-cp311-cp311-win32.whl", hash = "sha256:11d8f3dd2b9c1207dcaf2ee0bbbfd5991f571186ec9cc78427ba5bd32afae4b5"},
    {file = "PyYAML-6.0.2-cp311-cp311-win_amd64.whl", hash = "sha256:e10ce637b18caea04431ce14fabcf5c64a1c61ec9c56b071a4b7ca131ca52d44"},
    {file = "PyYAML-6.0.2-cp312-cp312-macosx_10_9_x86_64.whl", hash = "sha256:c70c95198c015b85feafc136515252a261a84561b7b1d51e3384e0655ddf25ab"},
    {file = "PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:ce826d6ef20b1bc864f0a68340c8b3287705cae2f8b4b1d932177dcc76721725"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:1f71ea527786de97d1a0cc0eacd1defc0985dcf6b3f17bb77dcfc8c34bec4dc5"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:9b22676e8097e9e22e36d6b7bda33190d0d400f345f23d4065d48f4ca7ae0425"},
    {file = "PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:80bab7bfc629882493af4aa31a4cfa43a4c57c83813253626916b8c7ada83476"},
    {file = "PyYAML-6.0.2-cp312-cp312-musllinux_1_1_aarch64.whl", hash = "sha256:0833f8694549e586547b576dcfaba4a6b55b9e96098b36cdc7ebefe667dfed48"},
    {file = "PyYAML-6.0.2-cp312-cp312-musllinux_1_1_x86_64.whl", hash = "sha256:8b9c7197f7cb2738065c481a0461e50ad02f18c78cd75775628afb4d7137fb3b"},
    {file = "PyYAML-6.0.2-cp312-cp312-win32.whl", hash = "sha256:ef6107725bd54b262d6dedcc2af448a266975032bc85ef0172c5f059da6325b4"},
    {file = "PyYAML-6.0.2-cp312-cp312-win_amd64.whl", hash = "sha256:7e7401d0de89a9a855c839bc697c079a4af81cf878373abd7dc625847d25cbd8"},
    {file = "PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:efdca5630322a10774e8e98e1af481aad470dd62c3170801852d752aa7a783ba"},
    {file = "PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:50187695423ffe49e2deacb8cd10510bc361faac997de9efef88badc3bb9e2d1"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:0ffe8360bab4910ef1b9e87fb812d8bc0a308b0d0eef8c8f44e0254ab3b07133"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:17e311b6c678207928d649faa7cb0d7b4c26a0ba73d41e99c4fff6b6c3276484"},
    {file = "PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:70b189594dbe54f75ab3a1acec5f1e3faa7e8cf2f1e08d9b561cb41b845f69d5"},
    {file = "PyYAML-6.0.2-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:41e4e3953a79407c794916fa277a82531dd93aad34e29c2a514c2c0c5fe971cc"},
    {file = "PyYAML-6.0.2-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:68ccc6023a3400877818152ad9a1033e3db8625d899c72eacb5a668902e4d652"},
    {file = "PyYAML-6.0.2-cp313-cp313-win32.whl", hash = "sha256:bc2fa7c6b47d6bc618dd7fb02ef6fdedb1090ec036abab80d4681424b84c1183"},
    {file = "PyYAML-6.0.2-cp313-cp313-win_amd64.whl", hash = "sha256:8388ee1976c416731879ac16da0aff3f63b286ffdd57cdeb95f3f2e085687563"},
    {file = "PyYAML-6.0.2-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:24471b829b3bf607e04e88d79542a9d48bb037c2267d7927a874e6c205ca7e9a"},
    {file = "PyYAML-6.0.2-cp38-cp38-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d7fded462629cfa4b685c5416b949ebad6cec74af5e2d42905d41e257e0869f5"},
    {file = "PyYAML-6.0.2-cp38-cp38-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:d84a1718ee396f54f3a086ea0a66d8e552b2ab2017ef8b420e92edbc841c352d"},
    {file = "PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:9056c1ecd25795207ad294bcf39f2db3d845767be0ea6e6a34d856f006006083"},
    {file = "PyYAML-6.0.2-cp38-cp38-musllinux_1_1_x86_64.whl", hash = "sha256:82d09873e40955485746739bcb8b4586983670466c23382c19cffecbf1fd8706"},
    {file = "PyYAML-6.0.2-cp38-cp38-win32.whl", hash = "sha256:43fa96a3ca0d6b1812e01ced1044a003533c47f6ee8aca31724f78e93ccc089a"},
    {file = "PyYAML-6.0.2-cp38-cp38-win_amd64.whl", hash = "sha256:01179a4a8559ab5de078078f37e5c1a30d76bb88519906844fd7bdea1b7729ff"},
    {file = "PyYAML-6.0.2-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:688ba32a1cffef67fd2e9398a2efebaea461578b0923624778664cc1c914db5d"},
    {file = "PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:a8786accb172bd8afb8be14490a16625cbc387036876ab6ba70912730faf8e1f"},
    {file = "PyYAML-6.0.2-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:d8e03406cac8513435335dbab54c0d385e4a49e4945d2909a581c83647ca0290"},
    {file = "PyYAML-6.0.2-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f753120cb8181e736c57ef7636e83f31b9c0d1722c516f7e86cf15b7aa57ff12"},
    {file = "PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:3b1fdb9dc17f5a7677423d508ab4f243a726dea51fa5e70992e59a7411c89d19"},
    {file = "PyYAML-6.0.2-cp39-cp39-musllinux_1_1_aarch64.whl", hash = "sha256:0b69e4ce7a131fe56b7e4d770c67429700908fc0752af059838b1cfb41960e4e"},
    {file = "PyYAML-6.0.2-cp39-cp39-musllinux_1_1_x86_64.whl", hash = "sha256:a9f8c2e67970f13b16084e04f134610fd1d374bf477b17ec1599185cf611d725"},
    {file = "PyYAML-6.0.2-cp39-cp39-win32.whl", hash = "sha256:6395c297d42274772abc367baaa79683958044e5d3835486c16da75d2a694631"},
    {file = "PyYAML-6.0.2-cp39-cp39-win_amd64.whl", hash = "sha256:39693e1f8320ae4f43943590b49779ffb98acb81f788220ea932a6b6c51004d8"},
    {file = "pyyaml-6.0.2.tar.gz", hash = "sha256:d584d9ec91ad65861cc08d42e834324ef890a082e591037abe114850ff7bbc3e"},
]

[[package]]
name = "referencing"
version = "0.36.2"
description = "JSON Referencing + Python"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "referencing-0.36.2-py3-none-any.whl", hash = "sha256:e8699adbbf8b5c7de96d8ffa0eb5c158b3beafce084968e2ea8bb08c6794dcd0"},
    {file = "referencing-0.36.2.tar.gz", hash = "sha256:df2e89862cd09deabbdba16944cc3f10feb6b3e6f18e902f7cc25609a34775aa"},
]

[package.dependencies]
attrs = ">=22.2.0"
rpds-py = ">=0.7.0"

[[package]]
name = "requests"
version = "2.32.5"
description = "Python HTTP for Humans."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "requests-2.32.5-py3-none-any.whl", hash = "sha256:2462f94637a34fd532264295e186976db0f5d453d1cdd31473c85a6a161affb6"},
    {file = "requests-2.32.5.tar.gz", hash = "sha256:dbba0bac56e100853db0ea71b82b4dfd5fe2bf6d3754a8893c3af500cec7d7cf"},
]

[package.dependencies]
certifi = ">=2017.4.17"
charset_normalizer = ">=2,<4"
idna = ">=2.5,<4"
urllib3 = ">=1.21.1,<3"

[package.extras]
socks = ["PySocks (>=1.5.6,!=1.5.7)"]
use-chardet-on-py3 = ["chardet (>=3.0.2,<6)"]

[[package]]
name = "rpds-py"
version = "0.27.1"
description = "Python bindings to Rust's persistent data structures (rpds)"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "rpds_py-0.27.1-cp310-cp310-macosx_10_12_x86_64.whl", hash = "sha256:68afeec26d42ab3b47e541b272166a0b4400313946871cba3ed3a4fc0cab1cef"},
    {file = "rpds_py-0.27.1-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:74e5b2f7bb6fa38b1b10546d27acbacf2a022a8b5543efb06cfebc72a59c85be"},
    {file = "rpds_py-0.27.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:9024de74731df54546fab0bfbcdb49fae19159ecaecfc8f37c18d2c7e2c0bd61"},
    {file = "rpds_py-0.27.1-cp310-cp310-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:31d3ebadefcd73b73928ed0b2fd696f7fefda8629229f81929ac9c1854d0cffb"},
    {file = "rpds_py-0.27.1-cp310-cp310-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:b2e7f8f169d775dd9092a1743768d771f1d1300453ddfe6325ae3ab5332b4657"},
    {file = "rpds_py-0.27.1-cp310-cp310-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:3d905d16f77eb6ab2e324e09bfa277b4c8e5e6b8a78a3e7ff8f3cdf773b4c013"},
    {file = "rpds_py-0.27.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:50c946f048209e6362e22576baea09193809f87687a95a8db24e5fbdb307b93a"},
    {file = "rpds_py-0.27.1-cp310-cp310-manylinux_2_31_riscv64.whl", hash = "sha256:3deab27804d65cd8289eb814c2c0e807c4b9d9916c9225e363cb0cf875eb67c1"},
    {file = "rpds_py-0.27.1-cp310-cp310-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:8b61097f7488de4be8244c89915da8ed212832ccf1e7c7753a25a394bf9b1f10"},
    {file = "rpds_py-0.27.1-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:8a3f29aba6e2d7d90528d3c792555a93497fe6538aa65eb675b44505be747808"},
    {file = "rpds_py-0.27.1-cp310-cp310-musllinux_1_2_i686.whl", hash = "sha256:dd6cd0485b7d347304067153a6dc1d73f7d4fd995a396ef32a24d24b8ac63ac8"},
    {file = "rpds_py-0.27.1-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:6f4461bf931108c9fa226ffb0e257c1b18dc2d44cd72b125bec50ee0ab1248a9"},
    {file = "rpds_py-0.27.1-cp310-cp310-win32.whl", hash = "sha256:ee5422d7fb21f6a00c1901bf6559c49fee13a5159d0288320737bbf6585bd3e4"},
    {file = "rpds_py-0.27.1-cp310-cp310-win_amd64.whl", hash = "sha256:3e039aabf6d5f83c745d5f9a0a381d031e9ed871967c0a5c38d201aca41f3ba1"},
    {file = "rpds_py-0.27.1-cp311-cp311-macosx_10_12_x86_64.whl", hash = "sha256:be898f271f851f68b318872ce6ebebbc62f303b654e43bf72683dbdc25b7c881"},
    {file = "rpds_py-0.27.1-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:62ac3d4e3e07b58ee0ddecd71d6ce3b1637de2d373501412df395a0ec5f9beb5"},
    {file = "rpds_py-0.27.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:4708c5c0ceb2d034f9991623631d3d23cb16e65c83736ea020cdbe28d57c0a0e"},
    {file = "rpds_py-0.27.1-cp311-cp311-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:abfa1171a9952d2e0002aba2ad3780820b00cc3d9c98c6630f2e93271501f66c"},
    {file = "rpds_py-0.27.1-cp311-cp311-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:4b507d19f817ebaca79574b16eb2ae412e5c0835542c93fe9983f1e432aca195"},
    {file = "rpds_py-0.27.1-cp311-cp311-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:168b025f8fd8d8d10957405f3fdcef3dc20f5982d398f90851f4abc58c566c52"},
    {file = "rpds_py-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:cb56c6210ef77caa58e16e8c17d35c63fe3f5b60fd9ba9d424470c3400bcf9ed"},
    {file = "rpds_py-0.27.1-cp311-cp311-manylinux_2_31_riscv64.whl", hash = "sha256:d252f2d8ca0195faa707f8eb9368955760880b2b42a8ee16d382bf5dd807f89a"},
    {file = "rpds_py-0.27.1-cp311-cp311-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:6e5e54da1e74b91dbc7996b56640f79b195d5925c2b78efaa8c5d53e1d88edde"},
    {file = "rpds_py-0.27.1-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:ffce0481cc6e95e5b3f0a47ee17ffbd234399e6d532f394c8dce320c3b089c21"},
    {file = "rpds_py-0.27.1-cp311-cp311-musllinux_1_2_i686.whl", hash = "sha256:a205fdfe55c90c2cd8e540ca9ceba65cbe6629b443bc05db1f590a3db8189ff9"},
    {file = "rpds_py-0.27.1-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:689fb5200a749db0415b092972e8eba85847c23885c8543a8b0f5c009b1a5948"},
    {file = "rpds_py-0.27.1-cp311-cp311-win32.whl", hash = "sha256:3182af66048c00a075010bc7f4860f33913528a4b6fc09094a6e7598e462fe39"},
    {file = "rpds_py-0.27.1-cp311-cp311-win_amd64.whl", hash = "sha256:b4938466c6b257b2f5c4ff98acd8128ec36b5059e5c8f8372d79316b1c36bb15"},
    {file = "rpds_py-0.27.1-cp311-cp311-win_arm64.whl", hash = "sha256:2f57af9b4d0793e53266ee4325535a31ba48e2f875da81a9177c9926dfa60746"},
    {file = "rpds_py-0.27.1-cp312-cp312-macosx_10_12_x86_64.whl", hash = "sha256:ae2775c1973e3c30316892737b91f9283f9908e3cc7625b9331271eaaed7dc90"},
    {file = "rpds_py-0.27.1-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:2643400120f55c8a96f7c9d858f7be0c88d383cd4653ae2cf0d0c88f668073e5"},
    {file = "rpds_py-0.27.1-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:16323f674c089b0360674a4abd28d5042947d54ba620f72514d69be4ff64845e"},
    {file = "rpds_py-0.27.1-cp312-cp312-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:9a1f4814b65eacac94a00fc9a526e3fdafd78e439469644032032d0d63de4881"},
    {file = "rpds_py-0.27.1-cp312-cp312-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:7ba32c16b064267b22f1850a34051121d423b6f7338a12b9459550eb2096e7ec"},
    {file = "rpds_py-0.27.1-cp312-cp312-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:e5c20f33fd10485b80f65e800bbe5f6785af510b9f4056c5a3c612ebc83ba6cb"},
    {file = "rpds_py-0.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:466bfe65bd932da36ff279ddd92de56b042f2266d752719beb97b08526268ec5"},
    {file = "rpds_py-0.27.1-cp312-cp312-manylinux_2_31_riscv64.whl", hash = "sha256:41e532bbdcb57c92ba3be62c42e9f096431b4cf478da9bc3bc6ce5c38ab7ba7a"},
    {file = "rpds_py-0.27.1-cp312-cp312-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:f149826d742b406579466283769a8ea448eed82a789af0ed17b0cd5770433444"},
    {file = "rpds_py-0.27.1-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:80c60cfb5310677bd67cb1e85a1e8eb52e12529545441b43e6f14d90b878775a"},
    {file = "rpds_py-0.27.1-cp312-cp312-musllinux_1_2_i686.whl", hash = "sha256:7ee6521b9baf06085f62ba9c7a3e5becffbc32480d2f1b351559c001c38ce4c1"},
    {file = "rpds_py-0.27.1-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:a512c8263249a9d68cac08b05dd59d2b3f2061d99b322813cbcc14c3c7421998"},
    {file = "rpds_py-0.27.1-cp312-cp312-win32.whl", hash = "sha256:819064fa048ba01b6dadc5116f3ac48610435ac9a0058bbde98e569f9e785c39"},
    {file = "rpds_py-0.27.1-cp312-cp312-win_amd64.whl", hash = "sha256:d9199717881f13c32c4046a15f024971a3b78ad4ea029e8da6b86e5aa9cf4594"},
    {file = "rpds_py-0.27.1-cp312-cp312-win_arm64.whl", hash = "sha256:33aa65b97826a0e885ef6e278fbd934e98cdcfed80b63946025f01e2f5b29502"},
    {file = "rpds_py-0.27.1-cp313-cp313-macosx_10_12_x86_64.whl", hash = "sha256:e4b9fcfbc021633863a37e92571d6f91851fa656f0180246e84cbd8b3f6b329b"},
    {file = "rpds_py-0.27.1-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:1441811a96eadca93c517d08df75de45e5ffe68aa3089924f963c782c4b898cf"},
    {file = "rpds_py-0.27.1-cp313-cp313-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:55266dafa22e672f5a4f65019015f90336ed31c6383bd53f5e7826d21a0e0b83"},
    {file = "rpds_py-0.27.1-cp313-cp313-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:d78827d7ac08627ea2c8e02c9e5b41180ea5ea1f747e9db0915e3adf36b62dcf"},
    {file = "rpds_py-0.27.1-cp313-cp313-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ae92443798a40a92dc5f0b01d8a7c93adde0c4dc965310a29ae7c64d72b9fad2"},
    {file = "rpds_py-0.27.1-cp313-cp313-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:c46c9dd2403b66a2a3b9720ec4b74d4ab49d4fabf9f03dfdce2d42af913fe8d0"},
    {file = "rpds_py-0.27.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2efe4eb1d01b7f5f1939f4ef30ecea6c6b3521eec451fb93191bf84b2a522418"},
    {file = "rpds_py-0.27.1-cp313-cp313-manylinux_2_31_riscv64.whl", hash = "sha256:15d3b4d83582d10c601f481eca29c3f138d44c92187d197aff663a269197c02d"},
    {file = "rpds_py-0.27.1-cp313-cp313-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:4ed2e16abbc982a169d30d1a420274a709949e2cbdef119fe2ec9d870b42f274"},
    {file = "rpds_py-0.27.1-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:a75f305c9b013289121ec0f1181931975df78738cdf650093e6b86d74aa7d8dd"},
    {file = "rpds_py-0.27.1-cp313-cp313-musllinux_1_2_i686.whl", hash = "sha256:67ce7620704745881a3d4b0ada80ab4d99df390838839921f99e63c474f82cf2"},
    {file = "rpds_py-0.27.1-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:9d992ac10eb86d9b6f369647b6a3f412fc0075cfd5d799530e84d335e440a002"},
    {file = "rpds_py-0.27.1-cp313-cp313-win32.whl", hash = "sha256:4f75e4bd8ab8db624e02c8e2fc4063021b58becdbe6df793a8111d9343aec1e3"},
    {file = "rpds_py-0.27.1-cp313-cp313-win_amd64.whl", hash = "sha256:f9025faafc62ed0b75a53e541895ca272815bec18abe2249ff6501c8f2e12b83"},
    {file = "rpds_py-0.27.1-cp313-cp313-win_arm64.whl", hash = "sha256:ed10dc32829e7d222b7d3b93136d25a406ba9788f6a7ebf6809092da1f4d279d"},
    {file = "rpds_py-0.27.1-cp313-cp313t-macosx_10_12_x86_64.whl", hash = "sha256:92022bbbad0d4426e616815b16bc4127f83c9a74940e1ccf3cfe0b387aba0228"},
    {file = "rpds_py-0.27.1-cp313-cp313t-macosx_11_0_arm64.whl", hash = "sha256:47162fdab9407ec3f160805ac3e154df042e577dd53341745fc7fb3f625e6d92"},
    {file = "rpds_py-0.27.1-cp313-cp313t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fb89bec23fddc489e5d78b550a7b773557c9ab58b7946154a10a6f7a214a48b2"},
    {file = "rpds_py-0.27.1-cp313-cp313t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:e48af21883ded2b3e9eb48cb7880ad8598b31ab752ff3be6457001d78f416723"},
    {file = "rpds_py-0.27.1-cp313-cp313t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:6f5b7bd8e219ed50299e58551a410b64daafb5017d54bbe822e003856f06a802"},
    {file = "rpds_py-0.27.1-cp313-cp313t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:08f1e20bccf73b08d12d804d6e1c22ca5530e71659e6673bce31a6bb71c1e73f"},
    {file = "rpds_py-0.27.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:0dc5dceeaefcc96dc192e3a80bbe1d6c410c469e97bdd47494a7d930987f18b2"},
    {file = "rpds_py-0.27.1-cp313-cp313t-manylinux_2_31_riscv64.whl", hash = "sha256:d76f9cc8665acdc0c9177043746775aa7babbf479b5520b78ae4002d889f5c21"},
    {file = "rpds_py-0.27.1-cp313-cp313t-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:134fae0e36022edad8290a6661edf40c023562964efea0cc0ec7f5d392d2aaef"},
    {file = "rpds_py-0.27.1-cp313-cp313t-musllinux_1_2_aarch64.whl", hash = "sha256:eb11a4f1b2b63337cfd3b4d110af778a59aae51c81d195768e353d8b52f88081"},
    {file = "rpds_py-0.27.1-cp313-cp313t-musllinux_1_2_i686.whl", hash = "sha256:13e608ac9f50a0ed4faec0e90ece76ae33b34c0e8656e3dceb9a7db994c692cd"},
    {file = "rpds_py-0.27.1-cp313-cp313t-musllinux_1_2_x86_64.whl", hash = "sha256:dd2135527aa40f061350c3f8f89da2644de26cd73e4de458e79606384f4f68e7"},
    {file = "rpds_py-0.27.1-cp313-cp313t-win32.whl", hash = "sha256:3020724ade63fe320a972e2ffd93b5623227e684315adce194941167fee02688"},
    {file = "rpds_py-0.27.1-cp313-cp313t-win_amd64.whl", hash = "sha256:8ee50c3e41739886606388ba3ab3ee2aae9f35fb23f833091833255a31740797"},
    {file = "rpds_py-0.27.1-cp314-cp314-macosx_10_12_x86_64.whl", hash = "sha256:acb9aafccaae278f449d9c713b64a9e68662e7799dbd5859e2c6b3c67b56d334"},
    {file = "rpds_py-0.27.1-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:b7fb801aa7f845ddf601c49630deeeccde7ce10065561d92729bfe81bd21fb33"},
    {file = "rpds_py-0.27.1-cp314-cp314-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fe0dd05afb46597b9a2e11c351e5e4283c741237e7f617ffb3252780cca9336a"},
    {file = "rpds_py-0.27.1-cp314-cp314-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:b6dfb0e058adb12d8b1d1b25f686e94ffa65d9995a5157afe99743bf7369d62b"},
    {file = "rpds_py-0.27.1-cp314-cp314-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:ed090ccd235f6fa8bb5861684567f0a83e04f52dfc2e5c05f2e4b1309fcf85e7"},
    {file = "rpds_py-0.27.1-cp314-cp314-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:bf876e79763eecf3e7356f157540d6a093cef395b65514f17a356f62af6cc136"},
    {file = "rpds_py-0.27.1-cp314-cp314-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:12ed005216a51b1d6e2b02a7bd31885fe317e45897de81d86dcce7d74618ffff"},
    {file = "rpds_py-0.27.1-cp314-cp314-manylinux_2_31_riscv64.whl", hash = "sha256:ee4308f409a40e50593c7e3bb8cbe0b4d4c66d1674a316324f0c2f5383b486f9"},
    {file = "rpds_py-0.27.1-cp314-cp314-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:0b08d152555acf1f455154d498ca855618c1378ec810646fcd7c76416ac6dc60"},
    {file = "rpds_py-0.27.1-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:dce51c828941973a5684d458214d3a36fcd28da3e1875d659388f4f9f12cc33e"},
    {file = "rpds_py-0.27.1-cp314-cp314-musllinux_1_2_i686.whl", hash = "sha256:c1476d6f29eb81aa4151c9a31219b03f1f798dc43d8af1250a870735516a1212"},
    {file = "rpds_py-0.27.1-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:3ce0cac322b0d69b63c9cdb895ee1b65805ec9ffad37639f291dd79467bee675"},
    {file = "rpds_py-0.27.1-cp314-cp314-win32.whl", hash = "sha256:dfbfac137d2a3d0725758cd141f878bf4329ba25e34979797c89474a89a8a3a3"},
    {file = "rpds_py-0.27.1-cp314-cp314-win_amd64.whl", hash = "sha256:a6e57b0abfe7cc513450fcf529eb486b6e4d3f8aee83e92eb5f1ef848218d456"},
    {file = "rpds_py-0.27.1-cp314-cp314-win_arm64.whl", hash = "sha256:faf8d146f3d476abfee026c4ae3bdd9ca14236ae4e4c310cbd1cf75ba33d24a3"},
    {file = "rpds_py-0.27.1-cp314-cp314t-macosx_10_12_x86_64.whl", hash = "sha256:ba81d2b56b6d4911ce735aad0a1d4495e808b8ee4dc58715998741a26874e7c2"},
    {file = "rpds_py-0.27.1-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:84f7d509870098de0e864cad0102711c1e24e9b1a50ee713b65928adb22269e4"},
    {file = "rpds_py-0.27.1-cp314-cp314t-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:a9e960fc78fecd1100539f14132425e1d5fe44ecb9239f8f27f079962021523e"},
    {file = "rpds_py-0.27.1-cp314-cp314t-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:62f85b665cedab1a503747617393573995dac4600ff51869d69ad2f39eb5e817"},
    {file = "rpds_py-0.27.1-cp314-cp314t-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:fed467af29776f6556250c9ed85ea5a4dd121ab56a5f8b206e3e7a4c551e48ec"},
    {file = "rpds_py-0.27.1-cp314-cp314t-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:f2729615f9d430af0ae6b36cf042cb55c0936408d543fb691e1a9e36648fd35a"},
    {file = "rpds_py-0.27.1-cp314-cp314t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:1b207d881a9aef7ba753d69c123a35d96ca7cb808056998f6b9e8747321f03b8"},
    {file = "rpds_py-0.27.1-cp314-cp314t-manylinux_2_31_riscv64.whl", hash = "sha256:639fd5efec029f99b79ae47e5d7e00ad8a773da899b6309f6786ecaf22948c48"},
    {file = "rpds_py-0.27.1-cp314-cp314t-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:fecc80cb2a90e28af8a9b366edacf33d7a91cbfe4c2c4544ea1246e949cfebeb"},
    {file = "rpds_py-0.27.1-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:42a89282d711711d0a62d6f57d81aa43a1368686c45bc1c46b7f079d55692734"},
    {file = "rpds_py-0.27.1-cp314-cp314t-musllinux_1_2_i686.whl", hash = "sha256:cf9931f14223de59551ab9d38ed18d92f14f055a5f78c1d8ad6493f735021bbb"},
    {file = "rpds_py-0.27.1-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:f39f58a27cc6e59f432b568ed8429c7e1641324fbe38131de852cd77b2d534b0"},
    {file = "rpds_py-0.27.1-cp314-cp314t-win32.whl", hash = "sha256:d5fa0ee122dc09e23607a28e6d7b150da16c662e66409bbe85230e4c85bb528a"},
    {file = "rpds_py-0.27.1-cp314-cp314t-win_amd64.whl", hash = "sha256:6567d2bb951e21232c2f660c24cf3470bb96de56cdcb3f071a83feeaff8a2772"},
    {file = "rpds_py-0.27.1-cp39-cp39-macosx_10_12_x86_64.whl", hash = "sha256:c918c65ec2e42c2a78d19f18c553d77319119bf43aa9e2edf7fb78d624355527"},
    {file = "rpds_py-0.27.1-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:1fea2b1a922c47c51fd07d656324531adc787e415c8b116530a1d29c0516c62d"},
    {file = "rpds_py-0.27.1-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:bbf94c58e8e0cd6b6f38d8de67acae41b3a515c26169366ab58bdca4a6883bb8"},
    {file = "rpds_py-0.27.1-cp39-cp39-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:c2a8fed130ce946d5c585eddc7c8eeef0051f58ac80a8ee43bd17835c144c2cc"},
    {file = "rpds_py-0.27.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:037a2361db72ee98d829bc2c5b7cc55598ae0a5e0ec1823a56ea99374cfd73c1"},
    {file = "rpds_py-0.27.1-cp39-cp39-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:5281ed1cc1d49882f9997981c88df1a22e140ab41df19071222f7e5fc4e72125"},
    {file = "rpds_py-0.27.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:2fd50659a069c15eef8aa3d64bbef0d69fd27bb4a50c9ab4f17f83a16cbf8905"},
    {file = "rpds_py-0.27.1-cp39-cp39-manylinux_2_31_riscv64.whl", hash = "sha256:c4b676c4ae3921649a15d28ed10025548e9b561ded473aa413af749503c6737e"},
    {file = "rpds_py-0.27.1-cp39-cp39-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:079bc583a26db831a985c5257797b2b5d3affb0386e7ff886256762f82113b5e"},
    {file = "rpds_py-0.27.1-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:4e44099bd522cba71a2c6b97f68e19f40e7d85399de899d66cdb67b32d7cb786"},
    {file = "rpds_py-0.27.1-cp39-cp39-musllinux_1_2_i686.whl", hash = "sha256:e202e6d4188e53c6661af813b46c37ca2c45e497fc558bacc1a7630ec2695aec"},
    {file = "rpds_py-0.27.1-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:f41f814b8eaa48768d1bb551591f6ba45f87ac76899453e8ccd41dba1289b04b"},
    {file = "rpds_py-0.27.1-cp39-cp39-win32.whl", hash = "sha256:9e71f5a087ead99563c11fdaceee83ee982fd39cf67601f4fd66cb386336ee52"},
    {file = "rpds_py-0.27.1-cp39-cp39-win_amd64.whl", hash = "sha256:71108900c9c3c8590697244b9519017a400d9ba26a36c48381b3f64743a44aab"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-macosx_10_12_x86_64.whl", hash = "sha256:7ba22cb9693df986033b91ae1d7a979bc399237d45fccf875b76f62bb9e52ddf"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-macosx_11_0_arm64.whl", hash = "sha256:5b640501be9288c77738b5492b3fd3abc4ba95c50c2e41273c8a1459f08298d3"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:fb08b65b93e0c6dd70aac7f7890a9c0938d5ec71d5cb32d45cf844fb8ae47636"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:d7ff07d696a7a38152ebdb8212ca9e5baab56656749f3d6004b34ab726b550b8"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:fb7c72262deae25366e3b6c0c0ba46007967aea15d1eea746e44ddba8ec58dcc"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:7b002cab05d6339716b03a4a3a2ce26737f6231d7b523f339fa061d53368c9d8"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:23f6b69d1c26c4704fec01311963a41d7de3ee0570a84ebde4d544e5a1859ffc"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-manylinux_2_31_riscv64.whl", hash = "sha256:530064db9146b247351f2a0250b8f00b289accea4596a033e94be2389977de71"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:7b90b0496570bd6b0321724a330d8b545827c4df2034b6ddfc5f5275f55da2ad"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-musllinux_1_2_aarch64.whl", hash = "sha256:879b0e14a2da6a1102a3fc8af580fc1ead37e6d6692a781bd8c83da37429b5ab"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-musllinux_1_2_i686.whl", hash = "sha256:0d807710df3b5faa66c731afa162ea29717ab3be17bdc15f90f2d9f183da4059"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-musllinux_1_2_x86_64.whl", hash = "sha256:3adc388fc3afb6540aec081fa59e6e0d3908722771aa1e37ffe22b220a436f0b"},
    {file = "rpds_py-0.27.1-pp310-pypy310_pp73-win_amd64.whl", hash = "sha256:c796c0c1cc68cb08b0284db4229f5af76168172670c74908fdbd4b7d7f515819"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-macosx_10_12_x86_64.whl", hash = "sha256:cdfe4bb2f9fe7458b7453ad3c33e726d6d1c7c0a72960bcc23800d77384e42df"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-macosx_11_0_arm64.whl", hash = "sha256:8fabb8fd848a5f75a2324e4a84501ee3a5e3c78d8603f83475441866e60b94a3"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:eda8719d598f2f7f3e0f885cba8646644b55a187762bec091fa14a2b819746a9"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:3c64d07e95606ec402a0a1c511fe003873fa6af630bda59bac77fac8b4318ebc"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:93a2ed40de81bcff59aabebb626562d48332f3d028ca2036f1d23cbb52750be4"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:387ce8c44ae94e0ec50532d9cb0edce17311024c9794eb196b90e1058aadeb66"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:aaf94f812c95b5e60ebaf8bfb1898a7d7cb9c1af5744d4a67fa47796e0465d4e"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-manylinux_2_31_riscv64.whl", hash = "sha256:4848ca84d6ded9b58e474dfdbad4b8bfb450344c0551ddc8d958bf4b36aa837c"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:2bde09cbcf2248b73c7c323be49b280180ff39fadcfe04e7b6f54a678d02a7cf"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-musllinux_1_2_aarch64.whl", hash = "sha256:94c44ee01fd21c9058f124d2d4f0c9dc7634bec93cd4b38eefc385dabe71acbf"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-musllinux_1_2_i686.whl", hash = "sha256:df8b74962e35c9249425d90144e721eed198e6555a0e22a563d29fe4486b51f6"},
    {file = "rpds_py-0.27.1-pp311-pypy311_pp73-musllinux_1_2_x86_64.whl", hash = "sha256:dc23e6820e3b40847e2f4a7726462ba0cf53089512abe9ee16318c366494c17a"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-macosx_10_12_x86_64.whl", hash = "sha256:aa8933159edc50be265ed22b401125c9eebff3171f570258854dbce3ecd55475"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-macosx_11_0_arm64.whl", hash = "sha256:a50431bf02583e21bf273c71b89d710e7a710ad5e39c725b14e685610555926f"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:78af06ddc7fe5cc0e967085a9115accee665fb912c22a3f54bad70cc65b05fe6"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:70d0738ef8fee13c003b100c2fbd667ec4f133468109b3472d249231108283a3"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:e2f6fd8a1cea5bbe599b6e78a6e5ee08db434fc8ffea51ff201c8765679698b3"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:8177002868d1426305bb5de1e138161c2ec9eb2d939be38291d7c431c4712df8"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:008b839781d6c9bf3b6a8984d1d8e56f0ec46dc56df61fd669c49b58ae800400"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-manylinux_2_31_riscv64.whl", hash = "sha256:a55b9132bb1ade6c734ddd2759c8dc132aa63687d259e725221f106b83a0e485"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-manylinux_2_5_i686.manylinux1_i686.whl", hash = "sha256:a46fdec0083a26415f11d5f236b79fa1291c32aaa4a17684d82f7017a1f818b1"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-musllinux_1_2_aarch64.whl", hash = "sha256:8a63b640a7845f2bdd232eb0d0a4a2dd939bcdd6c57e6bb134526487f3160ec5"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-musllinux_1_2_i686.whl", hash = "sha256:7e32721e5d4922deaaf963469d795d5bde6093207c52fec719bd22e5d1bedbc4"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-musllinux_1_2_x86_64.whl", hash = "sha256:2c426b99a068601b5f4623573df7a7c3d72e87533a2dd2253353a03e7502566c"},
    {file = "rpds_py-0.27.1-pp39-pypy39_pp73-win_amd64.whl", hash = "sha256:4fc9b7fe29478824361ead6e14e4f5aed570d477e06088826537e202d25fe859"},
    {file = "rpds_py-0.27.1.tar.gz", hash = "sha256:26a1c73171d10b7acccbded82bf6a586ab8203601e565badc74bbbf8bc5a10f8"},
]

[[package]]
name = "ruff"
version = "0.1.15"
description = "An extremely fast Python linter and code formatter, written in Rust."
optional = false
python-versions = ">=3.7"
groups = ["main"]
files = [
    {file = "ruff-0.1.15-py3-none-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl", hash = "sha256:5fe8d54df166ecc24106db7dd6a68d44852d14eb0729ea4672bb4d96c320b7df"},
    {file = "ruff-0.1.15-py3-none-macosx_10_12_x86_64.whl", hash = "sha256:6f0bfbb53c4b4de117ac4d6ddfd33aa5fc31beeaa21d23c45c6dd249faf9126f"},
    {file = "ruff-0.1.15-py3-none-manylinux_2_17_aarch64.manylinux2014_aarch64.whl", hash = "sha256:e0d432aec35bfc0d800d4f70eba26e23a352386be3a6cf157083d18f6f5881c8"},
    {file = "ruff-0.1.15-py3-none-manylinux_2_17_armv7l.manylinux2014_armv7l.whl", hash = "sha256:9405fa9ac0e97f35aaddf185a1be194a589424b8713e3b97b762336ec79ff807"},
    {file = "ruff-0.1.15-py3-none-manylinux_2_17_i686.manylinux2014_i686.whl", hash = "sha256:c66ec24fe36841636e814b8f90f572a8c0cb0e54d8b5c2d0e300d28a0d7bffec"},
    {file = "ruff-0.1.15-py3-none-manylinux_2_17_ppc64.manylinux2014_ppc64.whl", hash = "sha256:6f8ad828f01e8dd32cc58bc28375150171d198491fc901f6f98d2a39ba8e3ff5"},
    {file = "ruff-0.1.15-py3-none-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl", hash = "sha256:86811954eec63e9ea162af0ffa9f8d09088bab51b7438e8b6488b9401863c25e"},
    {file = "ruff-0.1.15-py3-none-manylinux_2_17_s390x.manylinux2014_s390x.whl", hash = "sha256:fd4025ac5e87d9b80e1f300207eb2fd099ff8200fa2320d7dc066a3f4622dc6b"},
    {file = "ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl", hash = "sha256:b17b93c02cdb6aeb696effecea1095ac93f3884a49a554a9afa76bb125c114c1"},
    {file = "ruff-0.1.15-py3-none-musllinux_1_2_aarch64.whl", hash = "sha256:ddb87643be40f034e97e97f5bc2ef7ce39de20e34608f3f829db727a93fb82c5"},
    {file = "ruff-0.1.15-py3-none-musllinux_1_2_armv7l.whl", hash = "sha256:abf4822129ed3a5ce54383d5f0e964e7fef74a41e48eb1dfad404151efc130a2"},
    {file = "ruff-0.1.15-py3-none-musllinux_1_2_i686.whl", hash = "sha256:6c629cf64bacfd136c07c78ac10a54578ec9d1bd2a9d395efbee0935868bf852"},
    {file = "ruff-0.1.15-py3-none-musllinux_1_2_x86_64.whl", hash = "sha256:1bab866aafb53da39c2cadfb8e1c4550ac5340bb40300083eb8967ba25481447"},
    {file = "ruff-0.1.15-py3-none-win32.whl", hash = "sha256:2417e1cb6e2068389b07e6fa74c306b2810fe3ee3476d5b8a96616633f40d14f"},
    {file = "ruff-0.1.15-py3-none-win_amd64.whl", hash = "sha256:3837ac73d869efc4182d9036b1405ef4c73d9b1f88da2413875e34e0d6919587"},
    {file = "ruff-0.1.15-py3-none-win_arm64.whl", hash = "sha256:9a933dfb1c14ec7a33cceb1e49ec4a16b51ce3c20fd42663198746efc0427360"},
    {file = "ruff-0.1.15.tar.gz", hash = "sha256:f6dfa8c1b21c913c326919056c390966648b680966febcb796cc9d1aaab8564e"},
]

[[package]]
name = "sniffio"
version = "1.3.1"
description = "Sniff out which async library your code is running under"
optional = false
python-versions = ">=3.7"
groups = ["main"]
files = [
    {file = "sniffio-1.3.1-py3-none-any.whl", hash = "sha256:2f6da418d1f1e0fddd844478f41680e794e6051915791a034ff65e5f100525a2"},
    {file = "sniffio-1.3.1.tar.gz", hash = "sha256:f4324edc670a0f49750a81b895f35c3adb843cca46f0530f79fc1babb23789dc"},
]

[[package]]
name = "sse-starlette"
version = "3.0.2"
description = "SSE plugin for Starlette"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "sse_starlette-3.0.2-py3-none-any.whl", hash = "sha256:16b7cbfddbcd4eaca11f7b586f3b8a080f1afe952c15813455b162edea619e5a"},
    {file = "sse_starlette-3.0.2.tar.gz", hash = "sha256:ccd60b5765ebb3584d0de2d7a6e4f745672581de4f5005ab31c3a25d10b52b3a"},
]

[package.dependencies]
anyio = ">=4.7.0"

[package.extras]
daphne = ["daphne (>=4.2.0)"]
examples = ["aiosqlite (>=0.21.0)", "fastapi (>=0.115.12)", "sqlalchemy[asyncio] (>=2.0.41)", "starlette (>=0.41.3)", "uvicorn (>=0.34.0)"]
granian = ["granian (>=2.3.1)"]
uvicorn = ["uvicorn (>=0.34.0)"]

[[package]]
name = "starlette"
version = "0.47.3"
description = "The little ASGI library that shines."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "starlette-0.47.3-py3-none-any.whl", hash = "sha256:89c0778ca62a76b826101e7c709e70680a1699ca7da6b44d38eb0a7e61fe4b51"},
    {file = "starlette-0.47.3.tar.gz", hash = "sha256:6bc94f839cc176c4858894f1f8908f0ab79dfec1a6b8402f6da9be26ebea52e9"},
]

[package.dependencies]
anyio = ">=3.6.2,<5"

[package.extras]
full = ["httpx (>=0.27.0,<0.29.0)", "itsdangerous", "jinja2", "python-multipart (>=0.0.18)", "pyyaml"]

[[package]]
name = "typing-extensions"
version = "4.14.1"
description = "Backported and Experimental Type Hints for Python 3.9+"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "typing_extensions-4.14.1-py3-none-any.whl", hash = "sha256:d1e1e3b58374dc93031d6eda2420a48ea44a36c2b4766a4fdeb3710755731d76"},
    {file = "typing_extensions-4.14.1.tar.gz", hash = "sha256:38b39f4aeeab64884ce9f74c94263ef78f3c22467c8724005483154c26648d36"},
]

[[package]]
name = "typing-inspection"
version = "0.4.1"
description = "Runtime typing introspection tools"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "typing_inspection-0.4.1-py3-none-any.whl", hash = "sha256:389055682238f53b04f7badcb49b989835495a96700ced5dab2d8feae4b26f51"},
    {file = "typing_inspection-0.4.1.tar.gz", hash = "sha256:6ae134cc0203c33377d43188d4064e9b357dba58cff3185f22924610e70a9d28"},
]

[package.dependencies]
typing-extensions = ">=4.12.0"

[[package]]
name = "urllib3"
version = "2.5.0"
description = "HTTP library with thread-safe connection pooling, file post, and more."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "urllib3-2.5.0-py3-none-any.whl", hash = "sha256:e6b01673c0fa6a13e374b50871808eb3bf7046c4b125b216f6bf1cc604cff0dc"},
    {file = "urllib3-2.5.0.tar.gz", hash = "sha256:3fc47733c7e419d4bc3f6b3dc2b4f890bb743906a30d56ba4a5bfa4bbff92760"},
]

[package.extras]
brotli = ["brotli (>=1.0.9) ; platform_python_implementation == \"CPython\"", "brotlicffi (>=0.8.0) ; platform_python_implementation != \"CPython\""]
h2 = ["h2 (>=4,<5)"]
socks = ["pysocks (>=1.5.6,!=1.5.7,<2.0)"]
zstd = ["zstandard (>=0.18.0)"]

[[package]]
name = "uvicorn"
version = "0.35.0"
description = "The lightning-fast ASGI server."
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "uvicorn-0.35.0-py3-none-any.whl", hash = "sha256:197535216b25ff9b785e29a0b79199f55222193d47f820816e7da751e9bc8d4a"},
    {file = "uvicorn-0.35.0.tar.gz", hash = "sha256:bc662f087f7cf2ce11a1d7fd70b90c9f98ef2e2831556dd078d131b96cc94a01"},
]

[package.dependencies]
click = ">=7.0"
h11 = ">=0.8"

[package.extras]
standard = ["colorama (>=0.4) ; sys_platform == \"win32\"", "httptools (>=0.6.3)", "python-dotenv (>=0.13)", "pyyaml (>=5.1)", "uvloop (>=0.15.1) ; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"", "watchfiles (>=0.13)", "websockets (>=10.4)"]

[[package]]
name = "virtualenv"
version = "20.34.0"
description = "Virtual Python Environment builder"
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "virtualenv-20.34.0-py3-none-any.whl", hash = "sha256:341f5afa7eee943e4984a9207c025feedd768baff6753cd660c857ceb3e36026"},
    {file = "virtualenv-20.34.0.tar.gz", hash = "sha256:44815b2c9dee7ed86e387b842a84f20b93f7f417f95886ca1996a72a4138eb1a"},
]

[package.dependencies]
distlib = ">=0.3.7,<1"
filelock = ">=3.12.2,<4"
platformdirs = ">=3.9.1,<5"

[package.extras]
docs = ["furo (>=2023.7.26)", "proselint (>=0.13)", "sphinx (>=7.1.2,!=7.3)", "sphinx-argparse (>=0.4)", "sphinxcontrib-towncrier (>=0.2.1a0)", "towncrier (>=23.6)"]
test = ["covdefaults (>=2.3)", "coverage (>=7.2.7)", "coverage-enable-subprocess (>=1)", "flaky (>=3.7)", "packaging (>=23.1)", "pytest (>=7.4)", "pytest-env (>=0.8.2)", "pytest-freezer (>=0.4.8) ; platform_python_implementation == \"PyPy\" or platform_python_implementation == \"GraalVM\" or platform_python_implementation == \"CPython\" and sys_platform == \"win32\" and python_version >= \"3.13\"", "pytest-mock (>=3.11.1)", "pytest-randomly (>=3.12)", "pytest-timeout (>=2.1)", "setuptools (>=68)", "time-machine (>=2.10) ; platform_python_implementation == \"CPython\""]

[[package]]
name = "wrapt"
version = "1.17.3"
description = "Module for decorators, wrappers and monkey patching."
optional = false
python-versions = ">=3.8"
groups = ["main"]
files = [
    {file = "wrapt-1.17.3-cp310-cp310-macosx_10_9_universal2.whl", hash = "sha256:88bbae4d40d5a46142e70d58bf664a89b6b4befaea7b2ecc14e03cedb8e06c04"},
    {file = "wrapt-1.17.3-cp310-cp310-macosx_10_9_x86_64.whl", hash = "sha256:e6b13af258d6a9ad602d57d889f83b9d5543acd471eee12eb51f5b01f8eb1bc2"},
    {file = "wrapt-1.17.3-cp310-cp310-macosx_11_0_arm64.whl", hash = "sha256:fd341868a4b6714a5962c1af0bd44f7c404ef78720c7de4892901e540417111c"},
    {file = "wrapt-1.17.3-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:f9b2601381be482f70e5d1051a5965c25fb3625455a2bf520b5a077b22afb775"},
    {file = "wrapt-1.17.3-cp310-cp310-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:343e44b2a8e60e06a7e0d29c1671a0d9951f59174f3709962b5143f60a2a98bd"},
    {file = "wrapt-1.17.3-cp310-cp310-musllinux_1_2_aarch64.whl", hash = "sha256:33486899acd2d7d3066156b03465b949da3fd41a5da6e394ec49d271baefcf05"},
    {file = "wrapt-1.17.3-cp310-cp310-musllinux_1_2_x86_64.whl", hash = "sha256:e6f40a8aa5a92f150bdb3e1c44b7e98fb7113955b2e5394122fa5532fec4b418"},
    {file = "wrapt-1.17.3-cp310-cp310-win32.whl", hash = "sha256:a36692b8491d30a8c75f1dfee65bef119d6f39ea84ee04d9f9311f83c5ad9390"},
    {file = "wrapt-1.17.3-cp310-cp310-win_amd64.whl", hash = "sha256:afd964fd43b10c12213574db492cb8f73b2f0826c8df07a68288f8f19af2ebe6"},
    {file = "wrapt-1.17.3-cp310-cp310-win_arm64.whl", hash = "sha256:af338aa93554be859173c39c85243970dc6a289fa907402289eeae7543e1ae18"},
    {file = "wrapt-1.17.3-cp311-cp311-macosx_10_9_universal2.whl", hash = "sha256:273a736c4645e63ac582c60a56b0acb529ef07f78e08dc6bfadf6a46b19c0da7"},
    {file = "wrapt-1.17.3-cp311-cp311-macosx_10_9_x86_64.whl", hash = "sha256:5531d911795e3f935a9c23eb1c8c03c211661a5060aab167065896bbf62a5f85"},
    {file = "wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl", hash = "sha256:0610b46293c59a3adbae3dee552b648b984176f8562ee0dba099a56cfbe4df1f"},
    {file = "wrapt-1.17.3-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:b32888aad8b6e68f83a8fdccbf3165f5469702a7544472bdf41f582970ed3311"},
    {file = "wrapt-1.17.3-cp311-cp311-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:8cccf4f81371f257440c88faed6b74f1053eef90807b77e31ca057b2db74edb1"},
    {file = "wrapt-1.17.3-cp311-cp311-musllinux_1_2_aarch64.whl", hash = "sha256:d8a210b158a34164de8bb68b0e7780041a903d7b00c87e906fb69928bf7890d5"},
    {file = "wrapt-1.17.3-cp311-cp311-musllinux_1_2_x86_64.whl", hash = "sha256:79573c24a46ce11aab457b472efd8d125e5a51da2d1d24387666cd85f54c05b2"},
    {file = "wrapt-1.17.3-cp311-cp311-win32.whl", hash = "sha256:c31eebe420a9a5d2887b13000b043ff6ca27c452a9a22fa71f35f118e8d4bf89"},
    {file = "wrapt-1.17.3-cp311-cp311-win_amd64.whl", hash = "sha256:0b1831115c97f0663cb77aa27d381237e73ad4f721391a9bfb2fe8bc25fa6e77"},
    {file = "wrapt-1.17.3-cp311-cp311-win_arm64.whl", hash = "sha256:5a7b3c1ee8265eb4c8f1b7d29943f195c00673f5ab60c192eba2d4a7eae5f46a"},
    {file = "wrapt-1.17.3-cp312-cp312-macosx_10_13_universal2.whl", hash = "sha256:ab232e7fdb44cdfbf55fc3afa31bcdb0d8980b9b95c38b6405df2acb672af0e0"},
    {file = "wrapt-1.17.3-cp312-cp312-macosx_10_13_x86_64.whl", hash = "sha256:9baa544e6acc91130e926e8c802a17f3b16fbea0fd441b5a60f5cf2cc5c3deba"},
    {file = "wrapt-1.17.3-cp312-cp312-macosx_11_0_arm64.whl", hash = "sha256:6b538e31eca1a7ea4605e44f81a48aa24c4632a277431a6ed3f328835901f4fd"},
    {file = "wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:042ec3bb8f319c147b1301f2393bc19dba6e176b7da446853406d041c36c7828"},
    {file = "wrapt-1.17.3-cp312-cp312-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:3af60380ba0b7b5aeb329bc4e402acd25bd877e98b3727b0135cb5c2efdaefe9"},
    {file = "wrapt-1.17.3-cp312-cp312-musllinux_1_2_aarch64.whl", hash = "sha256:0b02e424deef65c9f7326d8c19220a2c9040c51dc165cddb732f16198c168396"},
    {file = "wrapt-1.17.3-cp312-cp312-musllinux_1_2_x86_64.whl", hash = "sha256:74afa28374a3c3a11b3b5e5fca0ae03bef8450d6aa3ab3a1e2c30e3a75d023dc"},
    {file = "wrapt-1.17.3-cp312-cp312-win32.whl", hash = "sha256:4da9f45279fff3543c371d5ababc57a0384f70be244de7759c85a7f989cb4ebe"},
    {file = "wrapt-1.17.3-cp312-cp312-win_amd64.whl", hash = "sha256:e71d5c6ebac14875668a1e90baf2ea0ef5b7ac7918355850c0908ae82bcb297c"},
    {file = "wrapt-1.17.3-cp312-cp312-win_arm64.whl", hash = "sha256:604d076c55e2fdd4c1c03d06dc1a31b95130010517b5019db15365ec4a405fc6"},
    {file = "wrapt-1.17.3-cp313-cp313-macosx_10_13_universal2.whl", hash = "sha256:a47681378a0439215912ef542c45a783484d4dd82bac412b71e59cf9c0e1cea0"},
    {file = "wrapt-1.17.3-cp313-cp313-macosx_10_13_x86_64.whl", hash = "sha256:54a30837587c6ee3cd1a4d1c2ec5d24e77984d44e2f34547e2323ddb4e22eb77"},
    {file = "wrapt-1.17.3-cp313-cp313-macosx_11_0_arm64.whl", hash = "sha256:16ecf15d6af39246fe33e507105d67e4b81d8f8d2c6598ff7e3ca1b8a37213f7"},
    {file = "wrapt-1.17.3-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:6fd1ad24dc235e4ab88cda009e19bf347aabb975e44fd5c2fb22a3f6e4141277"},
    {file = "wrapt-1.17.3-cp313-cp313-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:0ed61b7c2d49cee3c027372df5809a59d60cf1b6c2f81ee980a091f3afed6a2d"},
    {file = "wrapt-1.17.3-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:423ed5420ad5f5529db9ce89eac09c8a2f97da18eb1c870237e84c5a5c2d60aa"},
    {file = "wrapt-1.17.3-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:e01375f275f010fcbf7f643b4279896d04e571889b8a5b3f848423d91bf07050"},
    {file = "wrapt-1.17.3-cp313-cp313-win32.whl", hash = "sha256:53e5e39ff71b3fc484df8a522c933ea2b7cdd0d5d15ae82e5b23fde87d44cbd8"},
    {file = "wrapt-1.17.3-cp313-cp313-win_amd64.whl", hash = "sha256:1f0b2f40cf341ee8cc1a97d51ff50dddb9fcc73241b9143ec74b30fc4f44f6cb"},
    {file = "wrapt-1.17.3-cp313-cp313-win_arm64.whl", hash = "sha256:7425ac3c54430f5fc5e7b6f41d41e704db073309acfc09305816bc6a0b26bb16"},
    {file = "wrapt-1.17.3-cp314-cp314-macosx_10_13_universal2.whl", hash = "sha256:cf30f6e3c077c8e6a9a7809c94551203c8843e74ba0c960f4a98cd80d4665d39"},
    {file = "wrapt-1.17.3-cp314-cp314-macosx_10_13_x86_64.whl", hash = "sha256:e228514a06843cae89621384cfe3a80418f3c04aadf8a3b14e46a7be704e4235"},
    {file = "wrapt-1.17.3-cp314-cp314-macosx_11_0_arm64.whl", hash = "sha256:5ea5eb3c0c071862997d6f3e02af1d055f381b1d25b286b9d6644b79db77657c"},
    {file = "wrapt-1.17.3-cp314-cp314-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:281262213373b6d5e4bb4353bc36d1ba4084e6d6b5d242863721ef2bf2c2930b"},
    {file = "wrapt-1.17.3-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:dc4a8d2b25efb6681ecacad42fca8859f88092d8732b170de6a5dddd80a1c8fa"},
    {file = "wrapt-1.17.3-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:373342dd05b1d07d752cecbec0c41817231f29f3a89aa8b8843f7b95992ed0c7"},
    {file = "wrapt-1.17.3-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:d40770d7c0fd5cbed9d84b2c3f2e156431a12c9a37dc6284060fb4bec0b7ffd4"},
    {file = "wrapt-1.17.3-cp314-cp314-win32.whl", hash = "sha256:fbd3c8319de8e1dc79d346929cd71d523622da527cca14e0c1d257e31c2b8b10"},
    {file = "wrapt-1.17.3-cp314-cp314-win_amd64.whl", hash = "sha256:e1a4120ae5705f673727d3253de3ed0e016f7cd78dc463db1b31e2463e1f3cf6"},
    {file = "wrapt-1.17.3-cp314-cp314-win_arm64.whl", hash = "sha256:507553480670cab08a800b9463bdb881b2edeed77dc677b0a5915e6106e91a58"},
    {file = "wrapt-1.17.3-cp314-cp314t-macosx_10_13_universal2.whl", hash = "sha256:ed7c635ae45cfbc1a7371f708727bf74690daedc49b4dba310590ca0bd28aa8a"},
    {file = "wrapt-1.17.3-cp314-cp314t-macosx_10_13_x86_64.whl", hash = "sha256:249f88ed15503f6492a71f01442abddd73856a0032ae860de6d75ca62eed8067"},
    {file = "wrapt-1.17.3-cp314-cp314t-macosx_11_0_arm64.whl", hash = "sha256:5a03a38adec8066d5a37bea22f2ba6bbf39fcdefbe2d91419ab864c3fb515454"},
    {file = "wrapt-1.17.3-cp314-cp314t-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:5d4478d72eb61c36e5b446e375bbc49ed002430d17cdec3cecb36993398e1a9e"},
    {file = "wrapt-1.17.3-cp314-cp314t-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:223db574bb38637e8230eb14b185565023ab624474df94d2af18f1cdb625216f"},
    {file = "wrapt-1.17.3-cp314-cp314t-musllinux_1_2_aarch64.whl", hash = "sha256:e405adefb53a435f01efa7ccdec012c016b5a1d3f35459990afc39b6be4d5056"},
    {file = "wrapt-1.17.3-cp314-cp314t-musllinux_1_2_x86_64.whl", hash = "sha256:88547535b787a6c9ce4086917b6e1d291aa8ed914fdd3a838b3539dc95c12804"},
    {file = "wrapt-1.17.3-cp314-cp314t-win32.whl", hash = "sha256:41b1d2bc74c2cac6f9074df52b2efbef2b30bdfe5f40cb78f8ca22963bc62977"},
    {file = "wrapt-1.17.3-cp314-cp314t-win_amd64.whl", hash = "sha256:73d496de46cd2cdbdbcce4ae4bcdb4afb6a11234a1df9c085249d55166b95116"},
    {file = "wrapt-1.17.3-cp314-cp314t-win_arm64.whl", hash = "sha256:f38e60678850c42461d4202739f9bf1e3a737c7ad283638251e79cc49effb6b6"},
    {file = "wrapt-1.17.3-cp38-cp38-macosx_10_9_universal2.whl", hash = "sha256:70d86fa5197b8947a2fa70260b48e400bf2ccacdcab97bb7de47e3d1e6312225"},
    {file = "wrapt-1.17.3-cp38-cp38-macosx_10_9_x86_64.whl", hash = "sha256:df7d30371a2accfe4013e90445f6388c570f103d61019b6b7c57e0265250072a"},
    {file = "wrapt-1.17.3-cp38-cp38-macosx_11_0_arm64.whl", hash = "sha256:caea3e9c79d5f0d2c6d9ab96111601797ea5da8e6d0723f77eabb0d4068d2b2f"},
    {file = "wrapt-1.17.3-cp38-cp38-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:758895b01d546812d1f42204bd443b8c433c44d090248bf22689df673ccafe00"},
    {file = "wrapt-1.17.3-cp38-cp38-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:02b551d101f31694fc785e58e0720ef7d9a10c4e62c1c9358ce6f63f23e30a56"},
    {file = "wrapt-1.17.3-cp38-cp38-musllinux_1_2_aarch64.whl", hash = "sha256:656873859b3b50eeebe6db8b1455e99d90c26ab058db8e427046dbc35c3140a5"},
    {file = "wrapt-1.17.3-cp38-cp38-musllinux_1_2_x86_64.whl", hash = "sha256:a9a2203361a6e6404f80b99234fe7fb37d1fc73487b5a78dc1aa5b97201e0f22"},
    {file = "wrapt-1.17.3-cp38-cp38-win32.whl", hash = "sha256:55cbbc356c2842f39bcc553cf695932e8b30e30e797f961860afb308e6b1bb7c"},
    {file = "wrapt-1.17.3-cp38-cp38-win_amd64.whl", hash = "sha256:ad85e269fe54d506b240d2d7b9f5f2057c2aa9a2ea5b32c66f8902f768117ed2"},
    {file = "wrapt-1.17.3-cp39-cp39-macosx_10_9_universal2.whl", hash = "sha256:30ce38e66630599e1193798285706903110d4f057aab3168a34b7fdc85569afc"},
    {file = "wrapt-1.17.3-cp39-cp39-macosx_10_9_x86_64.whl", hash = "sha256:65d1d00fbfb3ea5f20add88bbc0f815150dbbde3b026e6c24759466c8b5a9ef9"},
    {file = "wrapt-1.17.3-cp39-cp39-macosx_11_0_arm64.whl", hash = "sha256:a7c06742645f914f26c7f1fa47b8bc4c91d222f76ee20116c43d5ef0912bba2d"},
    {file = "wrapt-1.17.3-cp39-cp39-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl", hash = "sha256:7e18f01b0c3e4a07fe6dfdb00e29049ba17eadbc5e7609a2a3a4af83ab7d710a"},
    {file = "wrapt-1.17.3-cp39-cp39-manylinux2014_aarch64.manylinux_2_17_aarch64.manylinux_2_28_aarch64.whl", hash = "sha256:0f5f51a6466667a5a356e6381d362d259125b57f059103dd9fdc8c0cf1d14139"},
    {file = "wrapt-1.17.3-cp39-cp39-musllinux_1_2_aarch64.whl", hash = "sha256:59923aa12d0157f6b82d686c3fd8e1166fa8cdfb3e17b42ce3b6147ff81528df"},
    {file = "wrapt-1.17.3-cp39-cp39-musllinux_1_2_x86_64.whl", hash = "sha256:46acc57b331e0b3bcb3e1ca3b421d65637915cfcd65eb783cb2f78a511193f9b"},
    {file = "wrapt-1.17.3-cp39-cp39-win32.whl", hash = "sha256:3e62d15d3cfa26e3d0788094de7b64efa75f3a53875cdbccdf78547aed547a81"},
    {file = "wrapt-1.17.3-cp39-cp39-win_amd64.whl", hash = "sha256:1f23fa283f51c890eda8e34e4937079114c74b4c81d2b2f1f1d94948f5cc3d7f"},
    {file = "wrapt-1.17.3-cp39-cp39-win_arm64.whl", hash = "sha256:24c2ed34dc222ed754247a2702b1e1e89fdbaa4016f324b4b8f1a802d4ffe87f"},
    {file = "wrapt-1.17.3-py3-none-any.whl", hash = "sha256:7171ae35d2c33d326ac19dd8facb1e82e5fd04ef8c6c0e394d7af55a55051c22"},
    {file = "wrapt-1.17.3.tar.gz", hash = "sha256:f66eb08feaa410fe4eebd17f2a2c8e2e46d3476e9f8c783daa8e09e0faa666d0"},
]

[[package]]
name = "zipp"
version = "3.23.0"
description = "Backport of pathlib-compatible object wrapper for zip files"
optional = false
python-versions = ">=3.9"
groups = ["main"]
files = [
    {file = "zipp-3.23.0-py3-none-any.whl", hash = "sha256:071652d6115ed432f5ce1d34c336c0adfd6a884660d1e9712a256d3d3bd4b14e"},
    {file = "zipp-3.23.0.tar.gz", hash = "sha256:a07157588a12518c9d4034df3fbbee09c814741a33ff63c05fa29d26a2404166"},
]

[package.extras]
check = ["pytest-checkdocs (>=2.4)", "pytest-ruff (>=0.2.1) ; sys_platform != \"cygwin\""]
cover = ["pytest-cov"]
doc = ["furo", "jaraco.packaging (>=9.3)", "jaraco.tidelift (>=1.4)", "rst.linker (>=1.9)", "sphinx (>=3.5)", "sphinx-lint"]
enabler = ["pytest-enabler (>=2.2)"]
test = ["big-O", "jaraco.functools", "jaraco.itertools", "jaraco.test", "more_itertools", "pytest (>=6,!=8.1.*)", "pytest-ignore-flaky"]
type = ["pytest-mypy"]

[metadata]
lock-version = "2.1"
python-versions = ">=3.13,<4.0"
content-hash = "1b7a9756961bb0852edf93967c35fbfa800f042a83d6be12d45f82332d268662"

]]></file>
  <file name="pyproject.toml" path="memos.as/pyproject.toml"><![CDATA[
[project]
name = "memos-as"
version = "0.1.0"
description = ""
authors = [
    {name = "SigmaDev11",email = "steynsean11@gmail.com"}
]
readme = "README.md"
requires-python = ">=3.13,<4.0"
dependencies = [
    "ruff (>=0.1.0,<0.2.0)",
    "mypy (>=1.0.0,<2.0.0)",
    "pytest (>=7.0.0,<8.0.0)",
    "pre-commit (>=3.0.0,<4.0.0)",
    "mcp (>=1.0.0,<2.0.0)",
    "httpx (>=0.25.0,<1.0.0)",
    "fastapi (>=0.100.0,<1.0.0)",
    "uvicorn (>=0.20.0,<1.0.0)",
    "pydantic (>=2.0.0,<3.0.0)",
    "PyJWT (>=2.0.0,<3.0.0)",
    "python-multipart (>=0.0.6,<1.0.0)",
    "langfuse (>=3.3.0,<4.0.0)"
]

[tool.poetry]
packages = [{include = "memos"}]


[build-system]
requires = ["poetry-core>=2.0.0,<3.0.0"]
build-backend = "poetry.core.masonry.api"

]]></file>
  <file name="requirements-observability.txt" path="memos.as/requirements-observability.txt"><![CDATA[
# Additional dependencies for memOS observability instrumentation
prometheus-client==0.20.0
structlog==24.1.0
flask==3.0.3

]]></file>
  <file name="requirements.txt" path="memos.as/requirements.txt"><![CDATA[
# Core Web Framework
fastapi[all]

# Environment & Settings Management
pydantic-settings

# Database & ORM
# requirements.txt (updated to use constraints.txt for pinning)

sqlalchemy
psycopg2-binary    # consider switching to psycopg>=3.1,<4.0 for long-term support
redis
# Vector Database Client
qdrant-client

# AI & Embeddings
google-generativeai

# Testing
pytest
httpx

# Code Quality
flake8
black
isort

# Documentation
mkdocs
mkdocs-material
mkdocstrings[python]

# Graph Database
neo4j

# Observability & Monitoring
prometheus-client
opentelemetry-api
opentelemetry-sdk
opentelemetry-instrumentation-fastapi
opentelemetry-instrumentation-sqlalchemy
opentelemetry-instrumentation-redis
opentelemetry-exporter-jaeger-thrift
structlog
langfuse
deprecated

]]></file>
  <file name="setup_observability.py" path="memos.as/setup_observability.py"><![CDATA[

]]></file>
  <file name="OMEGA_INGEST_LAWS.md" path="../project_support/secure_verified_docs/OMEGA_INGEST_LAWS.md"><![CDATA[
# ⚖️ **OMEGA INGEST LAWS - Immutable Truth Protocol**

**Established**: August 31, 2025  
**Authority**: ApexSigma Ecosystem Governance  
**Status**: ACTIVE ENFORCEMENT  
**Violation Consequences**: Immediate system lock, dual verification reset required

---

## 🔒 **FUNDAMENTAL PRINCIPLES**

### **Law 1: Single Source of Truth**
The **Omega Ingest** stored within memOS + Neo4j knowledge graph represents the **ONLY AUTHORITATIVE SOURCE** of historical experience, decisions, and verified facts for the ApexSigma ecosystem. No other documentation or claims supersede Omega Ingest entries.

### **Law 2: Immutability of Verified Data**
Once information is verified and ingested into the Omega Ingest, it becomes **IMMUTABLE HISTORICAL RECORD**. Updates, corrections, or additions require new entries with explicit references to superseded information, maintaining complete audit trail.

### **Law 3: Dual Verification Requirement**
**NO OMEGA INGEST UPLOADS ARE PERMITTED WITHOUT VERIFICATION BY TWO PARTIES**. All entries must be verified by two separate entities before becoming part of the permanent record.

---

## 🛡️ **VERIFICATION PROTOCOLS**

### **Tier 1: Infrastructure & Critical Systems**
**Required Verifiers**: 2 different AI assistants (Claude, Gemini, Qwen, Copilot) OR 1 AI assistant + 1 human operator

**Subjects Requiring Tier 1 Verification**:
- Docker network topology and service configurations
- Database schemas and critical data structures  
- Agent registry and authentication systems
- Core API endpoints and integration protocols
- Security configurations and access controls
- Backup and recovery procedures

### **Tier 2: Application Logic & Features**
**Required Verifiers**: 2 different AI assistants OR 1 AI assistant + automated testing validation

**Subjects Requiring Tier 2 Verification**:
- Application feature implementations
- Code changes affecting multiple services
- Agent behavior modifications
- Workflow and process changes
- Configuration updates with system impact

### **Tier 3: Documentation & Knowledge**
**Required Verifiers**: 1 AI assistant + 1 knowledge validation check against existing Omega Ingest

**Subjects Requiring Tier 3 Verification**:
- Documentation updates
- Process descriptions
- Historical event records
- Learning and insight capture
- Best practice documentation

---

## 🔐 **ACCESS CONTROL MATRIX**

| Role | Read Access | Write Access | Verification Authority | Emergency Override |
|------|-------------|--------------|----------------------|-------------------|
| **Claude (Sonnet 4)** | ✅ Full | ✅ With Verification | ✅ Tier 1-3 | ❌ No |
| **Gemini** | ✅ Full | ✅ With Verification | ✅ Tier 1-3 | ❌ No |
| **Qwen Code** | ✅ Full | ✅ With Verification | ✅ Tier 2-3 | ❌ No |
| **GitHub Copilot** | ✅ Limited | ✅ With Verification | ✅ Tier 2-3 | ❌ No |
| **Human Operator** | ✅ Full | ✅ With Verification | ✅ Tier 1-3 | ✅ Yes |
| **DevEnviro Orchestrator** | ✅ Read Only | ❌ No | ❌ No | ❌ No |
| **Other Services** | ✅ Query Only | ❌ No | ❌ No | ❌ No |

---

## 📋 **MANDATORY PROCEDURES**

### **Before Any Code Changes**
1. **Context Retrieval Mandatory**: Query InGest-LLM → memOS → Omega Ingest for relevant context
2. **Verification Check**: Confirm planned changes don't conflict with verified infrastructure
3. **Impact Assessment**: Document potential effects on Tier 1 services
4. **Dual Verification**: Obtain verification from required parties before implementation

### **Omega Ingest Entry Process**
1. **Content Preparation**: Structure information with complete metadata
2. **Verification Request**: Submit to two required verifiers
3. **Verification Review**: Both parties must explicitly approve
4. **Ingestion**: Only after dual approval, submit to memOS via InGest-LLM
5. **Confirmation**: Verify successful storage in Neo4j knowledge graph
6. **Notification**: Notify all active agents of new immutable record

### **Verification Documentation**
Each Omega Ingest entry must include:
```json
{
  "content": "The verified information",
  "metadata": {
    "type": "infrastructure|application|knowledge",
    "security_level": "tier_1|tier_2|tier_3", 
    "verification_date": "ISO-8601 timestamp",
    "verifier_1": "Agent/Human identifier",
    "verifier_2": "Agent/Human identifier", 
    "verification_method": "Description of verification process",
    "source_documents": ["List of supporting documents"],
    "omega_ingest_category": "Category for knowledge graph"
  }
}
```

---

## 🚨 **ENFORCEMENT MECHANISMS**

### **Automated Safeguards**
- **Pre-commit Hooks**: Block commits that modify Tier 1 infrastructure without Omega Ingest verification
- **API Validation**: memOS API validates verification metadata before accepting entries
- **Knowledge Graph Protection**: Neo4j constraints prevent unauthorized modifications
- **Service Monitoring**: Alert on any unauthorized access attempts to protected services

### **Violation Detection**
- **Audit Trail**: All Omega Ingest access logged with full attribution
- **Change Detection**: Automated detection of undocumented infrastructure changes  
- **Consistency Checks**: Regular validation that system state matches Omega Ingest records
- **Health Monitoring**: Continuous verification that protected services remain operational

### **Response Protocols**
1. **Minor Violations**: Warning notification, require verification for next action
2. **Major Violations**: Temporary lock on Omega Ingest writes, require verification reset
3. **Critical Violations**: System-wide protection mode, human operator intervention required
4. **Emergency Situations**: Override protocols available to human operator only

---

## 🛠️ **TECHNICAL IMPLEMENTATION**

### **Protected Services (24/7 Monitoring Required)**
- **memOS API** (`172.26.0.13:8090`) - Omega Ingest Guardian
- **Neo4j Knowledge Graph** (`172.26.0.14:7687`) - Concept relationships
- **PostgreSQL Main** (`172.26.0.2:5432`) - Procedural memory
- **InGest-LLM API** (`172.26.0.12:8000`) - Ingestion gateway

### **Health Check Requirements**
```bash
# memOS Health (Every 30 seconds)
curl -f http://172.26.0.13:8090/health

# Neo4j Connectivity (Every 60 seconds)  
docker exec apexsigma_neo4j cypher-shell -u neo4j -p apexsigma_neo4j_password "RETURN 1"

# PostgreSQL Status (Every 30 seconds)
docker exec apexsigma_postgres pg_isready -U apexsigma_user

# InGest-LLM API (Every 60 seconds)
curl -f http://172.26.0.12:8000/health
```

### **Alert Thresholds**
- **<99% uptime** on any protected service: Immediate alert
- **Failed health check**: Alert after 2 consecutive failures
- **Unauthorized access attempt**: Immediate security alert
- **Knowledge graph inconsistency**: Critical alert, lock writes

---

## 📚 **AGENT INSTRUCTIONS INTEGRATION**

### **Mandatory Context Retrieval Protocol**
All agents must implement this workflow before ANY codebase modifications:

```python
# Step 1: Query InGest-LLM for relevant context
response = requests.post("http://172.26.0.12:8000/query_context", 
                        json={"query": "planned_change_description"})

# Step 2: Retrieve relevant Omega Ingest records  
context = requests.post("http://172.26.0.13:8090/memory/query",
                       json={"query": response.context_query, "top_k": 5})

# Step 3: Validate against immutable records
if context.has_conflicts:
    raise VerificationRequired("Changes conflict with Omega Ingest")

# Step 4: Only proceed with verified, non-conflicting changes
```

### **Required Agent Configuration Updates**
Each agent's instruction file must include:
1. **Context Retrieval Mandate**: Must query Omega Ingest before code changes
2. **Verification Requirements**: Must obtain dual verification for protected changes  
3. **Protected Services List**: Cannot modify Tier 1 services without verification
4. **Emergency Protocols**: Procedures for critical infrastructure issues

---

## ⚡ **EMERGENCY PROCEDURES**

### **Omega Ingest Corruption Response**
1. **Immediate Actions**: Stop all writes to memOS, isolate affected services
2. **Assessment**: Determine extent of corruption using Neo4j backup verification
3. **Recovery**: Restore from most recent verified backup
4. **Validation**: Re-verify all entries since last known good state
5. **Prevention**: Implement additional safeguards to prevent recurrence

### **Protected Service Failure**
1. **Isolation**: Disconnect failed service from network
2. **Assessment**: Determine impact on Omega Ingest integrity
3. **Backup Activation**: Switch to backup service if available
4. **Repair**: Restore service while maintaining data integrity
5. **Verification**: Confirm Omega Ingest consistency post-recovery

### **Unauthorized Access Detection**
1. **Lock Down**: Immediately restrict access to all protected services
2. **Investigation**: Determine source and extent of unauthorized access
3. **Audit**: Review all changes made during compromise period
4. **Remediation**: Reverse any unauthorized changes, restore from backup
5. **Strengthening**: Implement additional security measures

---

## 🎯 **COMPLIANCE VALIDATION**

### **Daily Checks**
- [ ] All protected services operational (health checks green)
- [ ] No unauthorized Omega Ingest modifications
- [ ] All new entries properly verified
- [ ] Knowledge graph consistency maintained

### **Weekly Audits**
- [ ] Complete audit trail review
- [ ] Verification process compliance check  
- [ ] Protected service security assessment
- [ ] Agent instruction adherence validation

### **Monthly Reviews**
- [ ] Omega Ingest Laws effectiveness assessment
- [ ] Verification process optimization opportunities
- [ ] Protected service performance analysis
- [ ] Security incident review and prevention planning

---

## 📖 **AMENDMENT PROCESS**

Changes to these Omega Ingest Laws require:
1. **Proposal**: Detailed proposal with justification
2. **Impact Analysis**: Assessment of effects on ecosystem security
3. **Dual Verification**: Two different AI assistants + human operator approval
4. **Testing**: Validation in isolated environment
5. **Implementation**: Gradual rollout with monitoring
6. **Documentation**: Update to this law document with full audit trail

**No amendments may weaken the dual verification requirement or reduce protection of Tier 1 services.**

---

## ✅ **AUTHORITY AND ENFORCEMENT**

These laws are **BINDING** on all ApexSigma ecosystem participants. Compliance is **MANDATORY** and **CONTINUOUSLY MONITORED**. 

**Effective Date**: August 31, 2025  
**Review Date**: Monthly  
**Authority**: ApexSigma Ecosystem Governance  
**Enforcement**: Automated + Human Oversight

---

*The Omega Ingest represents our collective knowledge and experience. These laws ensure its integrity for current and future development efforts.*
]]></file>
  <file name="QWEN_OPERATIONAL_CONTEXT.md" path="../project_support/secure_verified_docs/QWEN_OPERATIONAL_CONTEXT.md"><![CDATA[
# 🔍 Qwen Operational Context - ApexSigma Society of Agents

**Authority**: Omega Ingest Immutable Laws  
**Classification**: Tier 2 Quality Assurance Protocol  
**Effective Date**: September 1, 2025  
**Scope**: All Qwen interactions within ApexSigma ecosystem

---

## 🧪 **MANDATORY PRE-VALIDATION PROTOCOL**

### **BEFORE ANY QUALITY ASSURANCE ACTIVITIES:**

1. **Context Gathering Sequence**:
   ```bash
   # Step 1: Retrieve implementation context
   POST http://172.26.0.12:8000/query_context
   {
     "query": "testing requirements and validation criteria for [component]",
     "project": "[devenviro.as|memos.as|InGest-LLM.as|tools.as]",
     "focus": "quality_assurance"
   }
   
   # Step 2: Query historical testing patterns
   POST http://172.26.0.13:8090/memory/query
   {
     "query": "testing strategies and validation patterns",
     "memory_type": "quality_assurance"
   }
   ```

2. **Quality Standards Validation**:
   - Review existing test coverage and patterns
   - Confirm testing framework compatibility
   - Validate performance benchmarks and thresholds
   - Check security testing requirements

3. **Test Environment Assessment**:
   - Verify test database configuration
   - Confirm mock service availability
   - Validate test data and fixtures
   - Check continuous integration setup

---

## 🎯 **QWEN'S ROLE IN SOCIETY OF AGENTS**

### **Primary Function**: Quality Assurance Engineer & Validation Specialist  
- **Responsibilities**: Test strategy, automated testing, quality gates, validation
- **Authority Level**: Tier 2 - Quality assurance decisions with MAR participation
- **Specialization**: pytest, test automation, performance validation, security testing
- **Collaboration Mode**: Quality gatekeeper in MAR (Mandatory Agent Review) processes

### **Core Quality Assurance Domains**
- **Test Strategy**: Comprehensive testing approaches and methodologies
- **Automated Testing**: Unit, integration, end-to-end, and performance tests
- **Quality Gates**: Validation criteria and acceptance standards
- **Performance Testing**: Load testing, stress testing, performance profiling
- **Security Testing**: Vulnerability assessment, security validation
- **Code Quality**: Static analysis, code review, quality metrics

---

## 🧪 **TESTING FRAMEWORK STANDARDS**

### **Test Architecture Requirements**
```python
# Standard Test Structure
import pytest
import asyncio
from httpx import AsyncClient
from fastapi.testclient import TestClient
from unittest.mock import Mock, patch

# Test Configuration
@pytest.fixture
async def async_client():
    async with AsyncClient(app=app, base_url="http://test") as ac:
        yield ac

@pytest.fixture
def db_session():
    # Test database session with rollback
    session = TestSession()
    try:
        yield session
    finally:
        session.rollback()
        session.close()
```

### **Test Categories & Coverage Requirements**

#### **1. Unit Tests (Minimum 80% Coverage)**
```python
# Service Layer Testing
class TestResourceService:
    def test_create_resource_success(self, db_session):
        service = ResourceService(db_session)
        resource_data = ResourceCreate(name="test", type="example")
        
        result = service.create_resource(resource_data, "agent_123")
        
        assert result.name == "test"
        assert result.owner_agent_id == "agent_123"
        assert result.id is not None

    def test_create_resource_duplicate_name_fails(self, db_session):
        service = ResourceService(db_session)
        # Test duplicate name handling
        
    @pytest.mark.asyncio
    async def test_async_operation(self):
        # Async operation testing pattern
```

#### **2. Integration Tests (API Layer)**
```python
# API Integration Testing
class TestResourceAPI:
    @pytest.mark.asyncio
    async def test_create_resource_endpoint(self, async_client):
        response = await async_client.post(
            "/api/v1/resources",
            json={"name": "integration_test", "type": "test"},
            headers={"Authorization": "Bearer test_token"}
        )
        
        assert response.status_code == 201
        data = response.json()
        assert data["name"] == "integration_test"
        
    @pytest.mark.asyncio
    async def test_resource_workflow_complete(self, async_client):
        # Test complete CRUD workflow
        # Create -> Read -> Update -> Delete
```

#### **3. Database Integration Tests**
```python
# Database Layer Testing
class TestResourceRepository:
    def test_database_constraints(self, db_session):
        # Test foreign key constraints
        # Test unique constraints
        # Test cascade operations
        
    def test_query_performance(self, db_session, performance_benchmark):
        # Test query execution time
        with performance_benchmark:
            results = db_session.query(ResourceModel).filter(...).all()
        
        assert performance_benchmark.elapsed < 0.1  # 100ms threshold
```

#### **4. Service Integration Tests**
```python
# Cross-service Integration Testing
class TestServiceIntegration:
    @pytest.mark.asyncio
    async def test_memos_integration(self):
        # Test memOS memory storage integration
        
    @pytest.mark.asyncio  
    async def test_ingest_llm_context_retrieval(self):
        # Test InGest-LLM context API integration
        
    def test_rabbitmq_messaging(self):
        # Test RabbitMQ agent communication
```

---

## 📊 **PERFORMANCE VALIDATION STANDARDS**

### **Response Time Benchmarks**
```python
# Performance Testing Framework
import time
from contextlib import contextmanager

@contextmanager
def performance_timer():
    start = time.time()
    yield
    end = time.time()
    elapsed = end - start
    assert elapsed < 0.1, f"Operation took {elapsed:.3f}s, max allowed: 0.1s"

class TestPerformanceStandards:
    def test_api_response_time(self, client):
        with performance_timer():
            response = client.get("/api/v1/resources")
        assert response.status_code == 200
        
    def test_database_query_performance(self, db_session):
        with performance_timer():
            results = db_session.query(ResourceModel).all()
        assert len(results) >= 0
        
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, async_client):
        # Test system under concurrent load
        tasks = [
            async_client.get(f"/api/v1/resources/{i}") 
            for i in range(10)
        ]
        responses = await asyncio.gather(*tasks)
        
        # Validate all responses successful
        assert all(r.status_code == 200 for r in responses)
```

### **Resource Usage Monitoring**
```python
# Memory and Resource Testing
import psutil
import memory_profiler

class TestResourceUsage:
    def test_memory_usage_within_limits(self):
        @memory_profiler.profile
        def memory_intensive_operation():
            # Operation under test
            pass
            
        # Validate memory usage stays within acceptable bounds
        
    def test_database_connection_cleanup(self, db_session):
        # Test proper connection cleanup
        initial_connections = get_active_connections()
        
        # Perform database operations
        
        final_connections = get_active_connections()
        assert final_connections <= initial_connections
```

---

## 🔒 **SECURITY TESTING PROTOCOLS**

### **Security Validation Framework**
```python
# Security Testing Standards
class TestSecurityValidation:
    def test_input_sanitization(self, client):
        # Test SQL injection prevention
        malicious_input = "'; DROP TABLE resources; --"
        response = client.post(
            "/api/v1/resources",
            json={"name": malicious_input}
        )
        # Should handle gracefully, not expose database errors
        
    def test_authentication_required(self, client):
        # Test endpoints require proper authentication
        response = client.get("/api/v1/protected")
        assert response.status_code == 401
        
    def test_authorization_enforcement(self, client):
        # Test agent can only access own resources
        response = client.get(
            "/api/v1/resources/other_agent_resource",
            headers={"X-Agent-ID": "agent_123"}
        )
        assert response.status_code == 403
        
    def test_no_credential_exposure(self, client, caplog):
        # Test no passwords/tokens in logs or responses
        client.post("/api/v1/auth/login", json={"password": "secret"})
        
        for record in caplog.records:
            assert "secret" not in record.getMessage()
```

### **Data Protection Testing**
```python
# Data Security Validation
class TestDataProtection:
    def test_agent_data_isolation(self, db_session):
        # Test multi-tenant data isolation
        agent1_data = create_test_data("agent_1")
        agent2_data = create_test_data("agent_2")
        
        # Agent 1 should only see own data
        results = get_resources_for_agent("agent_1")
        assert all(r.owner_agent_id == "agent_1" for r in results)
        
    def test_sensitive_data_encryption(self):
        # Test sensitive data is properly encrypted at rest
        pass
```

---

## 📋 **MAR PROTOCOL QUALITY GATES**

### **MAR Review Checklist**
When participating in Mandatory Agent Review processes:

#### **Code Quality Assessment**
- [ ] **Test Coverage**: Minimum 80% line coverage achieved
- [ ] **Test Quality**: Edge cases and error conditions covered
- [ ] **Performance**: All endpoints meet <100ms response time requirement
- [ ] **Security**: Security testing passed, no vulnerabilities detected
- [ ] **Documentation**: All public APIs documented with examples

#### **Integration Validation**  
- [ ] **Database**: Schema changes tested, migrations validated
- [ ] **API Contracts**: Request/response schemas validated
- [ ] **Service Integration**: Cross-service communication tested
- [ ] **Error Handling**: Comprehensive error scenarios covered
- [ ] **Observability**: Metrics, logging, tracing implemented

#### **Quality Gate Criteria**
```python
# Automated Quality Gates
class QualityGateValidator:
    def validate_test_coverage(self, coverage_report):
        assert coverage_report.total_coverage >= 80.0
        assert coverage_report.critical_path_coverage >= 95.0
        
    def validate_performance_benchmarks(self, performance_results):
        for endpoint, timing in performance_results.items():
            assert timing.p95 < 0.1, f"{endpoint} p95: {timing.p95}s"
            
    def validate_security_scan(self, security_report):
        assert security_report.high_severity_issues == 0
        assert security_report.medium_severity_issues <= 2
```

---

## 🤝 **COLLABORATION PROTOCOLS**

### **Working with Implementation Teams**
1. **Pre-Implementation**: Define acceptance criteria and test strategy
2. **During Implementation**: Provide testing guidance and quality feedback
3. **Code Review**: Focus on testability, error handling, performance
4. **Post-Implementation**: Validate quality gates and deployment readiness

### **Quality Feedback Loop**
```markdown
## Quality Assessment Template

### Test Coverage Analysis
- **Unit Test Coverage**: X%
- **Integration Test Coverage**: Y% 
- **Critical Path Coverage**: Z%

### Performance Validation
- **API Response Times**: [List of endpoints and timings]
- **Database Query Performance**: [Query performance metrics]
- **Resource Usage**: [Memory, CPU, connection usage]

### Security Assessment
- **Authentication/Authorization**: ✅/❌
- **Input Validation**: ✅/❌
- **Data Protection**: ✅/❌
- **Vulnerability Scan**: [Results]

### Quality Gate Status
- [ ] All tests passing
- [ ] Coverage thresholds met
- [ ] Performance benchmarks satisfied
- [ ] Security validation passed
- [ ] Documentation complete

### Recommendations
- [Specific improvement suggestions]
- [Risk assessments]
- [Deployment readiness assessment]
```

---

## 🛠️ **TESTING INFRASTRUCTURE**

### **Test Environment Configuration**
```python
# Test Configuration Management
import os
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Test Database Setup
TEST_DATABASE_URL = "sqlite:///./test.db"
test_engine = create_engine(TEST_DATABASE_URL)
TestSession = sessionmaker(bind=test_engine)

# Mock External Services
class MockSerperAPI:
    def search(self, query):
        return {"results": [{"title": "Mock Result", "url": "test.com"}]}

# Test Data Factory
class TestDataFactory:
    @staticmethod
    def create_agent(agent_id="test_agent"):
        return Agent(
            id=agent_id,
            name="Test Agent",
            capabilities=["test", "mock"]
        )
        
    @staticmethod
    def create_resource(name="test_resource", agent_id="test_agent"):
        return Resource(
            name=name,
            owner_agent_id=agent_id,
            type="test"
        )
```

### **Continuous Integration Integration**
```yaml
# .github/workflows/quality-gates.yml
name: Quality Gates
on: [push, pull_request]

jobs:
  quality-validation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Quality Gates
        run: |
          python -m pytest --cov=app --cov-report=xml
          python -m pytest --benchmark-only
          python security_scan.py
          
      - name: Quality Gate Check
        run: |
          python validate_quality_gates.py
```

---

## 📈 **QUALITY METRICS & MONITORING**

### **Key Quality Indicators**
- **Test Coverage**: >80% line coverage, >95% critical path coverage
- **Test Execution Time**: Full test suite <5 minutes
- **Performance Benchmarks**: API endpoints <100ms p95
- **Security Score**: Zero high-severity vulnerabilities
- **Code Quality**: Ruff linting score 100%, mypy type coverage >90%

### **Quality Dashboard Metrics**
```python
# Quality Metrics Collection
class QualityMetrics:
    def collect_test_metrics(self):
        return {
            "total_tests": self.count_total_tests(),
            "passing_tests": self.count_passing_tests(),
            "test_coverage": self.calculate_coverage(),
            "test_execution_time": self.measure_execution_time()
        }
        
    def collect_performance_metrics(self):
        return {
            "api_response_times": self.measure_api_performance(),
            "database_query_times": self.measure_db_performance(),
            "resource_usage": self.measure_resource_usage()
        }
        
    def collect_security_metrics(self):
        return {
            "vulnerability_count": self.count_vulnerabilities(),
            "security_test_coverage": self.calculate_security_coverage(),
            "authentication_test_coverage": self.measure_auth_coverage()
        }
```

---

## ✅ **QUALITY ASSURANCE CHECKLIST**

### **Pre-Testing Preparation**
- [ ] Retrieved testing context from InGest-LLM
- [ ] Reviewed existing test patterns in memOS
- [ ] Confirmed test environment configuration
- [ ] Validated test data and fixtures availability

### **During Testing Activities**  
- [ ] Implementing comprehensive test coverage (unit + integration)
- [ ] Validating performance benchmarks and thresholds
- [ ] Conducting security testing and vulnerability assessment
- [ ] Documenting test results and quality metrics

### **Post-Testing Validation**
- [ ] All quality gates satisfied
- [ ] Test coverage meets minimum thresholds
- [ ] Performance benchmarks achieved
- [ ] Security validation passed
- [ ] MAR review documentation prepared
- [ ] Quality metrics reported and stored

---

## 🚀 **OPERATION ASGARD REBIRTH QUALITY FOCUS**

**Primary Quality Assurance Areas**:
1. **Service Reliability**: Ensure all 4 services achieve 90+ health scores
2. **Integration Testing**: Validate cross-service communication and data flow
3. **Performance Optimization**: Meet response time and resource usage targets
4. **Security Hardening**: Comprehensive security testing and validation

**Success Metrics**:
- 100% of critical functionality covered by automated tests
- All services pass performance benchmarks
- Zero high-severity security vulnerabilities
- Complete MAR process validation for all implementations

---

*This document establishes comprehensive quality assurance standards for Qwen within the ApexSigma Society of Agents ecosystem. All quality activities must follow these protocols to ensure system reliability and operational excellence.*

**Last Updated**: September 1, 2025  
**Authority**: Omega Ingest Immutable Laws  
**Verification Status**: DUAL VERIFIED ✅
]]></file>
</code_files>